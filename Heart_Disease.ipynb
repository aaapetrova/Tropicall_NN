{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaapetrova/Tropicall_NN/blob/main/Heart_Disease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNMoTiYpv_zn",
        "outputId": "2b2caf21-e7db-4f1a-8baa-56b8183de786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gccwn9ewwmKG"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/heart_2020_cleaned.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2HHc0WbxsAq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# For ML models\n",
        "from sklearn.linear_model import LinearRegression ,LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC ,SVR\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wAXIDM7CgOk",
        "outputId": "f59abe1b-0a5c-4e96-baf7-167e0afc3a09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DDFNGw4x6rq"
      },
      "source": [
        "# Загрузка Датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgNl5fBjyM0E",
        "outputId": "e641f612-b95a-42c8-f5bd-3e2e8f0d055d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/heart_2020_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/heart_2020_cleaned.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F92TLsjCz_l"
      },
      "source": [
        "## Column Descriptions\n",
        "\n",
        "- HeartDisease: Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI).\n",
        "- BMI: Body Mass Index (BMI).\n",
        "- Smoking: Have you smoked at least 100 cigarettes in your entire life?\n",
        "- AlcoholDrinking: Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week\n",
        "- Stroke(инсульт): (Ever told) (you had) a stroke?\n",
        "- PhysicalHealth: Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? (0-30 days).\n",
        "- MentalHealth: Thinking about your mental health, for how many days during the past 30 days was your mental health not good? (0-30 days).\n",
        "DiffWalking: Do you have serious difficulty walking or climbing stairs?\n",
        "- Sex: Are you male or female?\n",
        "- AgeCategory: Fourteen-level age category. (then calculated the mean)\n",
        "- Race: Imputed race/ethnicity value.\n",
        "- Diabetic: (Ever told) (you had) diabetes?\n",
        "- PhysicalActivity: Adults who reported doing physical activity or exercise during the past 30 days other than their regular job.\n",
        "- GenHealth: Would you say that in general your health is...\n",
        "- SleepTime: On average, how many hours of sleep do you get in a 24-hour period?\n",
        "- Asthma: (Ever told) (you had) asthma?\n",
        "- KidneyDisease: Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease?\n",
        "SkinCancer: (Ever told) (you had) skin cancer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdiShpiVBOfb"
      },
      "source": [
        "## Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5bh8DwN0x-bF",
        "outputId": "c136c88c-ae46-41a8-b6c9-02dd33ee2a1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  HeartDisease    BMI Smoking AlcoholDrinking Stroke  PhysicalHealth  \\\n",
              "0           No  16.60     Yes              No     No             3.0   \n",
              "1           No  20.34      No              No    Yes             0.0   \n",
              "2           No  26.58     Yes              No     No            20.0   \n",
              "3           No  24.21      No              No     No             0.0   \n",
              "4           No  23.71      No              No     No            28.0   \n",
              "\n",
              "   MentalHealth DiffWalking     Sex  AgeCategory   Race Diabetic  \\\n",
              "0          30.0          No  Female        55-59  White      Yes   \n",
              "1           0.0          No  Female  80 or older  White       No   \n",
              "2          30.0          No    Male        65-69  White      Yes   \n",
              "3           0.0          No  Female        75-79  White       No   \n",
              "4           0.0         Yes  Female        40-44  White       No   \n",
              "\n",
              "  PhysicalActivity  GenHealth  SleepTime Asthma KidneyDisease SkinCancer  \n",
              "0              Yes  Very good        5.0    Yes            No        Yes  \n",
              "1              Yes  Very good        7.0     No            No         No  \n",
              "2              Yes       Fair        8.0    Yes            No         No  \n",
              "3               No       Good        6.0     No            No        Yes  \n",
              "4              Yes  Very good        8.0     No            No         No  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf25259f-945f-4b79-af62-972f11131453\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeartDisease</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoking</th>\n",
              "      <th>AlcoholDrinking</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>PhysicalHealth</th>\n",
              "      <th>MentalHealth</th>\n",
              "      <th>DiffWalking</th>\n",
              "      <th>Sex</th>\n",
              "      <th>AgeCategory</th>\n",
              "      <th>Race</th>\n",
              "      <th>Diabetic</th>\n",
              "      <th>PhysicalActivity</th>\n",
              "      <th>GenHealth</th>\n",
              "      <th>SleepTime</th>\n",
              "      <th>Asthma</th>\n",
              "      <th>KidneyDisease</th>\n",
              "      <th>SkinCancer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No</td>\n",
              "      <td>16.60</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>55-59</td>\n",
              "      <td>White</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No</td>\n",
              "      <td>20.34</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>80 or older</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>7.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No</td>\n",
              "      <td>26.58</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>20.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Male</td>\n",
              "      <td>65-69</td>\n",
              "      <td>White</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Fair</td>\n",
              "      <td>8.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No</td>\n",
              "      <td>24.21</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>75-79</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Good</td>\n",
              "      <td>6.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No</td>\n",
              "      <td>23.71</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Female</td>\n",
              "      <td>40-44</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf25259f-945f-4b79-af62-972f11131453')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf25259f-945f-4b79-af62-972f11131453 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf25259f-945f-4b79-af62-972f11131453');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df = pd.read_csv(DATASET_PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4lZqRMyBQR7"
      },
      "source": [
        "## INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jbHvbM3BQBg",
        "outputId": "d9f79ee2-4fdb-4c91-fdf1-a78aae440db7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 319795 entries, 0 to 319794\n",
            "Data columns (total 18 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   HeartDisease      319795 non-null  object \n",
            " 1   BMI               319795 non-null  float64\n",
            " 2   Smoking           319795 non-null  object \n",
            " 3   AlcoholDrinking   319795 non-null  object \n",
            " 4   Stroke            319795 non-null  object \n",
            " 5   PhysicalHealth    319795 non-null  float64\n",
            " 6   MentalHealth      319795 non-null  float64\n",
            " 7   DiffWalking       319795 non-null  object \n",
            " 8   Sex               319795 non-null  object \n",
            " 9   AgeCategory       319795 non-null  object \n",
            " 10  Race              319795 non-null  object \n",
            " 11  Diabetic          319795 non-null  object \n",
            " 12  PhysicalActivity  319795 non-null  object \n",
            " 13  GenHealth         319795 non-null  object \n",
            " 14  SleepTime         319795 non-null  float64\n",
            " 15  Asthma            319795 non-null  object \n",
            " 16  KidneyDisease     319795 non-null  object \n",
            " 17  SkinCancer        319795 non-null  object \n",
            "dtypes: float64(4), object(14)\n",
            "memory usage: 43.9+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h0il8xT-CWw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Zkvc1B-HMq"
      },
      "outputs": [],
      "source": [
        "encode_AgeCategory = {'55-59':57, '80 or older':80, '65-69':67,\n",
        "                      '75-79':77,'40-44':42,'70-74':72,'60-64':62,\n",
        "                      '50-54':52,'45-49':47,'18-24':21,'35-39':37,\n",
        "                      '30-34':32,'25-29':27}\n",
        "df['AgeCategory'] = df['AgeCategory'].apply(lambda x: encode_AgeCategory[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAIHNeMLFKbc"
      },
      "source": [
        "Определение **категориальных** данных и **непрерывных**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOfoFs_NFRtn"
      },
      "outputs": [],
      "source": [
        "CategoricalFeatures = [\"HeartDisease\", \"Smoking\", \"AlcoholDrinking\", \"Stroke\", \"DiffWalking\", \"Sex\", \"Race\", \"Diabetic\", \"PhysicalActivity\", \"GenHealth\", \"Asthma\", \"KidneyDisease\", \"SkinCancer\"]\n",
        "ContinuesFeatues = [\"BMI\", \"PhysicalHealth\", \"MentalHealth\", \"AgeCategory\", \"SleepTime\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iUKOtcqkAygF",
        "outputId": "29e14597-7620-4712-d96c-76edca716736"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f4fa0556860>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_4936f_row0_col0 {\n",
              "  background-color: #6fb0d7;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4936f_row0_col1 {\n",
              "  background-color: #b5d4e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row0_col2, #T_4936f_row0_col3 {\n",
              "  background-color: #539ecd;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4936f_row0_col4 {\n",
              "  background-color: #74b3d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row0_col5 {\n",
              "  background-color: #7fb9da;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row0_col6, #T_4936f_row3_col0, #T_4936f_row3_col1, #T_4936f_row3_col2, #T_4936f_row3_col3, #T_4936f_row3_col4, #T_4936f_row3_col5 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4936f_row1_col0, #T_4936f_row1_col2, #T_4936f_row1_col3, #T_4936f_row1_col4, #T_4936f_row1_col5, #T_4936f_row2_col2, #T_4936f_row2_col3, #T_4936f_row2_col4, #T_4936f_row4_col1, #T_4936f_row4_col6 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row1_col1, #T_4936f_row2_col1 {\n",
              "  background-color: #94c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row1_col6, #T_4936f_row2_col6 {\n",
              "  background-color: #e7f0fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row2_col0 {\n",
              "  background-color: #f5fafe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row2_col5 {\n",
              "  background-color: #f5f9fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row3_col6 {\n",
              "  background-color: #1966ad;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_4936f_row4_col0 {\n",
              "  background-color: #e9f2fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row4_col2 {\n",
              "  background-color: #eef5fc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row4_col3 {\n",
              "  background-color: #dbe9f6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row4_col4 {\n",
              "  background-color: #dfebf7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_4936f_row4_col5 {\n",
              "  background-color: #e5eff9;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_4936f\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_4936f_level0_col0\" class=\"col_heading level0 col0\" >mean</th>\n",
              "      <th id=\"T_4936f_level0_col1\" class=\"col_heading level0 col1\" >std</th>\n",
              "      <th id=\"T_4936f_level0_col2\" class=\"col_heading level0 col2\" >min</th>\n",
              "      <th id=\"T_4936f_level0_col3\" class=\"col_heading level0 col3\" >25%</th>\n",
              "      <th id=\"T_4936f_level0_col4\" class=\"col_heading level0 col4\" >50%</th>\n",
              "      <th id=\"T_4936f_level0_col5\" class=\"col_heading level0 col5\" >75%</th>\n",
              "      <th id=\"T_4936f_level0_col6\" class=\"col_heading level0 col6\" >max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_4936f_level0_row0\" class=\"row_heading level0 row0\" >BMI</th>\n",
              "      <td id=\"T_4936f_row0_col0\" class=\"data row0 col0\" >28.325399</td>\n",
              "      <td id=\"T_4936f_row0_col1\" class=\"data row0 col1\" >6.356100</td>\n",
              "      <td id=\"T_4936f_row0_col2\" class=\"data row0 col2\" >12.020000</td>\n",
              "      <td id=\"T_4936f_row0_col3\" class=\"data row0 col3\" >24.030000</td>\n",
              "      <td id=\"T_4936f_row0_col4\" class=\"data row0 col4\" >27.340000</td>\n",
              "      <td id=\"T_4936f_row0_col5\" class=\"data row0 col5\" >31.420000</td>\n",
              "      <td id=\"T_4936f_row0_col6\" class=\"data row0 col6\" >94.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4936f_level0_row1\" class=\"row_heading level0 row1\" >PhysicalHealth</th>\n",
              "      <td id=\"T_4936f_row1_col0\" class=\"data row1 col0\" >3.371710</td>\n",
              "      <td id=\"T_4936f_row1_col1\" class=\"data row1 col1\" >7.950850</td>\n",
              "      <td id=\"T_4936f_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "      <td id=\"T_4936f_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
              "      <td id=\"T_4936f_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
              "      <td id=\"T_4936f_row1_col5\" class=\"data row1 col5\" >2.000000</td>\n",
              "      <td id=\"T_4936f_row1_col6\" class=\"data row1 col6\" >30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4936f_level0_row2\" class=\"row_heading level0 row2\" >MentalHealth</th>\n",
              "      <td id=\"T_4936f_row2_col0\" class=\"data row2 col0\" >3.898366</td>\n",
              "      <td id=\"T_4936f_row2_col1\" class=\"data row2 col1\" >7.955235</td>\n",
              "      <td id=\"T_4936f_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
              "      <td id=\"T_4936f_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
              "      <td id=\"T_4936f_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
              "      <td id=\"T_4936f_row2_col5\" class=\"data row2 col5\" >3.000000</td>\n",
              "      <td id=\"T_4936f_row2_col6\" class=\"data row2 col6\" >30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4936f_level0_row3\" class=\"row_heading level0 row3\" >AgeCategory</th>\n",
              "      <td id=\"T_4936f_row3_col0\" class=\"data row3 col0\" >54.355759</td>\n",
              "      <td id=\"T_4936f_row3_col1\" class=\"data row3 col1\" >17.720429</td>\n",
              "      <td id=\"T_4936f_row3_col2\" class=\"data row3 col2\" >21.000000</td>\n",
              "      <td id=\"T_4936f_row3_col3\" class=\"data row3 col3\" >42.000000</td>\n",
              "      <td id=\"T_4936f_row3_col4\" class=\"data row3 col4\" >57.000000</td>\n",
              "      <td id=\"T_4936f_row3_col5\" class=\"data row3 col5\" >67.000000</td>\n",
              "      <td id=\"T_4936f_row3_col6\" class=\"data row3 col6\" >80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_4936f_level0_row4\" class=\"row_heading level0 row4\" >SleepTime</th>\n",
              "      <td id=\"T_4936f_row4_col0\" class=\"data row4 col0\" >7.097075</td>\n",
              "      <td id=\"T_4936f_row4_col1\" class=\"data row4 col1\" >1.436007</td>\n",
              "      <td id=\"T_4936f_row4_col2\" class=\"data row4 col2\" >1.000000</td>\n",
              "      <td id=\"T_4936f_row4_col3\" class=\"data row4 col3\" >6.000000</td>\n",
              "      <td id=\"T_4936f_row4_col4\" class=\"data row4 col4\" >7.000000</td>\n",
              "      <td id=\"T_4936f_row4_col5\" class=\"data row4 col5\" >8.000000</td>\n",
              "      <td id=\"T_4936f_row4_col6\" class=\"data row4 col6\" >24.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.describe()[1:][ContinuesFeatues].T.style.background_gradient(cmap='Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gdr_kbvIBiAc",
        "outputId": "b6767fb7-ecfc-4279-81a6-5a70825bc6ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"757c17db-6391-4330-af05-dc553cbcb15d\" class=\"plotly-graph-div\" style=\"height:3200px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"757c17db-6391-4330-af05-dc553cbcb15d\")) {                    Plotly.newPlot(                        \"757c17db-6391-4330-af05-dc553cbcb15d\",                        [{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[292422,27373],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.9183673469387754,0.9999999999999999]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[187887,131908],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.9183673469387754,0.9999999999999999]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[298018,21777],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.7653061224489794,0.8469387755102039]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[307726,12069],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.7653061224489794,0.8469387755102039]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[275385,44410],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.6122448979591837,0.6938775510204082]}},{\"hole\":0.35,\"labels\":[\"Female\",\"Male\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[167805,151990],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.6122448979591837,0.6938775510204082]}},{\"hole\":0.35,\"labels\":[\"White\",\"Hispanic\",\"Black\",\"Other\",\"Asian\",\"American Indian/Alaskan Native\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[245212,27446,22939,10928,8068,5202],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.4591836734693877,0.5408163265306122]}},{\"hole\":0.35,\"labels\":[\"Yes\",\"No\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[247957,71838],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.4591836734693877,0.5408163265306122]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\",\"No, borderline diabetes\",\"Yes (during pregnancy)\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[269653,40802,6781,2559],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.30612244897959184,0.3877551020408163]}},{\"hole\":0.35,\"labels\":[\"Very good\",\"Good\",\"Excellent\",\"Fair\",\"Poor\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[113858,93129,66842,34677,11289],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.30612244897959184,0.3877551020408163]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[276923,42872],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.15306122448979592,0.2346938775510204]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[308016,11779],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.15306122448979592,0.2346938775510204]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[289976,29819],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,0.08163265306122448]}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"HeartDisease\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Smoking\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"AlcoholDrinking\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.8469387755102039,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Stroke\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.8469387755102039,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"DiffWalking\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6938775510204082,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Sex\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6938775510204082,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Race\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.5408163265306122,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Diabetic\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.5408163265306122,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"PhysicalActivity\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.3877551020408163,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"GenHealth\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.3877551020408163,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Asthma\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2346938775510204,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"KidneyDisease\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2346938775510204,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"SkinCancer\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.08163265306122448,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"font\":{\"size\":14},\"height\":3200,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('757c17db-6391-4330-af05-dc553cbcb15d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = make_subplots(\n",
        "    rows=7, cols=2, subplot_titles=(\"HeartDisease\", \"Smoking\",\n",
        "                                    \"AlcoholDrinking\",\"Stroke\",\n",
        "                                    \"DiffWalking\", \"Sex\",\n",
        "                                    'Race', 'Diabetic',\n",
        "                                    'PhysicalActivity','GenHealth',\n",
        "                                    'Asthma', 'KidneyDisease',\n",
        "                                    'SkinCancer'),\n",
        "    specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}]],\n",
        ")\n",
        "\n",
        "colours = ['#4285f4', '#ea4335', '#fbbc05', '#34a853']\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['HeartDisease'].value_counts().index),\n",
        "                     values=[x for x in df['HeartDisease'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Smoking'].value_counts().index),\n",
        "                     values=[x for x in df['Smoking'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['AlcoholDrinking'].value_counts().index),\n",
        "                     values=[x for x in df['AlcoholDrinking'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=2, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Stroke'].value_counts().index),\n",
        "                     values=[x for x in df['Stroke'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=2, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['DiffWalking'].value_counts().index),\n",
        "                     values=[x for x in df['DiffWalking'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=3, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Sex'].value_counts().index),\n",
        "                     values=[x for x in df['Sex'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=3, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Race'].value_counts().index),\n",
        "                     values=[x for x in df['Race'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=4, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['PhysicalActivity'].value_counts().index),\n",
        "                     values=[x for x in df['PhysicalActivity'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=4, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Diabetic'].value_counts().index),\n",
        "                     values=[x for x in df['Diabetic'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=5, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['GenHealth'].value_counts().index),\n",
        "                     values=[x for x in df['GenHealth'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=5, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Asthma'].value_counts().index),\n",
        "                     values=[x for x in df['Asthma'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=6, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['KidneyDisease'].value_counts().index),\n",
        "                     values=[x for x in df['KidneyDisease'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=6, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['SkinCancer'].value_counts().index),\n",
        "                     values=[x for x in df['SkinCancer'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=7, col=1)\n",
        "\n",
        "\n",
        "fig.update_layout(height=3200, font=dict(size=14), showlegend=False)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN80WRX0B9oF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw2J_z97E2tT"
      },
      "source": [
        "## Preprocessing datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "jGLMUNYVE7EZ",
        "outputId": "562fa750-970e-437d-dc37-3385bcd3d7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuous Columns\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f4f9e313700>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_cc855_row0_col0, #T_cc855_row0_col1, #T_cc855_row0_col2 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_cc855_row1_col0, #T_cc855_row1_col1, #T_cc855_row2_col0, #T_cc855_row3_col2 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_cc855_row1_col2, #T_cc855_row2_col2, #T_cc855_row3_col0 {\n",
              "  background-color: #e7f0fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_cc855_row2_col1 {\n",
              "  background-color: #f3f8fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_cc855_row3_col1 {\n",
              "  background-color: #d9e8f5;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_cc855\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_cc855_level0_col0\" class=\"col_heading level0 col0\" >min</th>\n",
              "      <th id=\"T_cc855_level0_col1\" class=\"col_heading level0 col1\" >mean</th>\n",
              "      <th id=\"T_cc855_level0_col2\" class=\"col_heading level0 col2\" >max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_cc855_level0_row0\" class=\"row_heading level0 row0\" >BMI</th>\n",
              "      <td id=\"T_cc855_row0_col0\" class=\"data row0 col0\" >12.020000</td>\n",
              "      <td id=\"T_cc855_row0_col1\" class=\"data row0 col1\" >28.325399</td>\n",
              "      <td id=\"T_cc855_row0_col2\" class=\"data row0 col2\" >94.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_cc855_level0_row1\" class=\"row_heading level0 row1\" >PhysicalHealth</th>\n",
              "      <td id=\"T_cc855_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
              "      <td id=\"T_cc855_row1_col1\" class=\"data row1 col1\" >3.371710</td>\n",
              "      <td id=\"T_cc855_row1_col2\" class=\"data row1 col2\" >30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_cc855_level0_row2\" class=\"row_heading level0 row2\" >MentalHealth</th>\n",
              "      <td id=\"T_cc855_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
              "      <td id=\"T_cc855_row2_col1\" class=\"data row2 col1\" >3.898366</td>\n",
              "      <td id=\"T_cc855_row2_col2\" class=\"data row2 col2\" >30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_cc855_level0_row3\" class=\"row_heading level0 row3\" >SleepTime</th>\n",
              "      <td id=\"T_cc855_row3_col0\" class=\"data row3 col0\" >1.000000</td>\n",
              "      <td id=\"T_cc855_row3_col1\" class=\"data row3 col1\" >7.097075</td>\n",
              "      <td id=\"T_cc855_row3_col2\" class=\"data row3 col2\" >24.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "print('Continuous Columns')\n",
        "df.select_dtypes(include=['float']).describe().T[['min', 'mean', 'max']].style.background_gradient(cmap='Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw_1jCBkFCTN"
      },
      "outputs": [],
      "source": [
        "for col in ContinuesFeatues:\n",
        "    df[col] = df[col]/df[col].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV5KQTcYFKPm",
        "outputId": "bd62bd83-1a83-4dea-b109-e93d8e22180b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Columns\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeartDisease        2\n",
              "Smoking             2\n",
              "AlcoholDrinking     2\n",
              "Stroke              2\n",
              "DiffWalking         2\n",
              "Sex                 2\n",
              "Race                6\n",
              "Diabetic            4\n",
              "PhysicalActivity    2\n",
              "GenHealth           5\n",
              "Asthma              2\n",
              "KidneyDisease       2\n",
              "SkinCancer          2\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "print('Categorical Columns\\n')\n",
        "df.select_dtypes(include=['O']).nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLcWgN4gFXXP"
      },
      "source": [
        "## Correlation Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHat_bJ8GHYc"
      },
      "outputs": [],
      "source": [
        "# Integer encode columns with 2 unique values\n",
        "for col in ['HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex', 'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer']:\n",
        "    if df[col].dtype == 'O':\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "# One-hot encode columns with more than 2 unique values\n",
        "df = pd.get_dummies(df, columns=['Race', 'Diabetic', 'GenHealth', ], prefix = ['Race', 'Diabetic', 'GenHealth'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "vmTNiD-YFYJA",
        "outputId": "4afc09fd-6087-4499-d7da-39c8b910634f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"2a90a07c-7502-4fa4-8749-5b3368fca1e7\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2a90a07c-7502-4fa4-8749-5b3368fca1e7\")) {                    Plotly.newPlot(                        \"2a90a07c-7502-4fa4-8749-5b3368fca1e7\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"HeartDisease\",\"Smoking\",\"AlcoholDrinking\",\"Stroke\",\"DiffWalking\",\"Sex\",\"PhysicalActivity\",\"Asthma\",\"KidneyDisease\",\"SkinCancer\",\"BMI\",\"PhysicalHealth\",\"MentalHealth\",\"AgeCategory\",\"SleepTime\"],\"y\":[\"HeartDisease\",\"Smoking\",\"AlcoholDrinking\",\"Stroke\",\"DiffWalking\",\"Sex\",\"PhysicalActivity\",\"Asthma\",\"KidneyDisease\",\"SkinCancer\",\"BMI\",\"PhysicalHealth\",\"MentalHealth\",\"AgeCategory\",\"SleepTime\"],\"z\":[[1.0,0.10776415602593577,-0.03207974262500417,0.19683529884287068,0.2012580485737773,0.07004047624891856,-0.10002993385404217,0.04144415110032234,0.14519709877256354,0.09331687769597886,0.05180319065574589,0.1707209719661753,0.028590714546905302,0.23158320851970648,0.008326646858286093],[0.10776415602593577,1.0,0.11176752021942088,0.06122603951647455,0.12007416384008394,0.08505248562950851,-0.0971737659539628,0.024148532018300563,0.03491968628492668,0.03397738727204867,0.02311811243270034,0.1153524139404747,0.08515728503775259,0.1306120821603677,-0.030335635088083682],[-0.03207974262500417,0.11176752021942088,1.0,-0.01985791423814558,-0.03532758282688487,0.004200142395935715,0.01748698277743856,-0.0022021002378207716,-0.028280091717843844,-0.005702370452955972,-0.03881622302230272,-0.01725428832282563,0.0512819728218972,-0.058587008626551716,-0.00506545083386641],[0.19683529884287068,0.06122603951647455,-0.01985791423814558,1.0,0.17414321368906785,-0.0030910549780765223,-0.0794551948440212,0.03886614032726421,0.09116684064825993,0.04811610449906195,0.019732982346395637,0.13701382711696813,0.046467061321632086,0.13673831258982225,0.01189998096058299],[0.2012580485737773,0.12007416384008394,-0.03532758282688487,0.17414321368906785,1.0,-0.06885955945150612,-0.2785239644744156,0.10322204883284222,0.15306375213121043,0.06484040178427677,0.18167826388865732,0.4283727985461525,0.15223466914599132,0.2413276153382443,-0.022216357334953533],[0.07004047624891856,0.08505248562950851,0.004200142395935715,-0.0030910549780765223,-0.06885955945150612,1.0,0.048246845809630415,-0.06919111638314385,-0.009083858389322482,0.01343379974373332,0.026939645259099483,-0.040903839313019365,-0.10005847280447244,-0.06700074478508969,-0.01570374828060869],[-0.10002993385404217,-0.0971737659539628,0.01748698277743856,-0.0794551948440212,-0.2785239644744156,0.048246845809630415,1.0,-0.0415258817742273,-0.0818273214414323,-0.0013278104683075525,-0.15061599390258318,-0.23228317707681606,-0.09580810490226797,-0.12036867337825854,0.003848841379442122],[0.04144415110032234,0.024148532018300563,-0.0022021002378207716,0.03886614032726421,0.10322204883284222,-0.06919111638314385,-0.0415258817742273,1.0,0.03970700033814702,-0.0003964769423400938,0.092345019006225,0.11790658022327909,0.1140081744247549,-0.05788658709501616,-0.04824528030804831],[0.14519709877256354,0.03491968628492668,-0.028280091717843844,0.09116684064825993,0.15306375213121043,-0.009083858389322482,-0.0818273214414323,0.03970700033814702,1.0,0.06181621651419233,0.050767532862528414,0.1421971848382409,0.03728112808809446,0.12249331295148701,0.006237934101449502],[0.09331687769597886,0.03397738727204867,-0.005702370452955972,0.04811610449906195,0.06484040178427677,0.01343379974373332,-0.0013278104683075525,-0.0003964769423400938,0.06181621651419233,1.0,-0.033643618515338786,0.0416996854817131,-0.033412190709220524,0.26139092150617554,0.041266167486529615],[0.05180319065574589,0.02311811243270034,-0.03881622302230272,0.019732982346395637,0.18167826388865732,0.026939645259099483,-0.15061599390258318,0.092345019006225,0.050767532862528414,-0.033643618515338786,1.0,0.1097875436096874,0.06413056947152597,-0.0004463909346209674,-0.05182225399892374],[0.1707209719661753,0.1153524139404747,-0.01725428832282563,0.13701382711696813,0.4283727985461525,-0.040903839313019365,-0.23228317707681606,0.11790658022327909,0.1421971848382409,0.0416996854817131,0.1097875436096874,1.0,0.2879866740873287,0.11100961741149926,-0.061386631929495],[0.028590714546905302,0.08515728503775259,0.0512819728218972,0.046467061321632086,0.15223466914599132,-0.10005847280447244,-0.09580810490226797,0.1140081744247549,0.03728112808809446,-0.033412190709220524,0.06413056947152597,0.2879866740873287,1.0,-0.1553270117678171,-0.11971678803142047],[0.23158320851970648,0.1306120821603677,-0.058587008626551716,0.13673831258982225,0.2413276153382443,-0.06700074478508969,-0.12036867337825854,-0.05788658709501616,0.12249331295148701,0.26139092150617554,-0.0004463909346209674,0.11100961741149926,-0.1553270117678171,1.0,0.1027001320622866],[0.008326646858286093,-0.030335635088083682,-0.00506545083386641,0.01189998096058299,-0.022216357334953533,-0.01570374828060869,0.003848841379442122,-0.04824528030804831,0.006237934101449502,0.041266167486529615,-0.05182225399892374,-0.061386631929495,-0.11971678803142047,0.1027001320622866,1.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"margin\":{\"t\":60},\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2a90a07c-7502-4fa4-8749-5b3368fca1e7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = px.imshow(df[['HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex',\n",
        "                    'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer','BMI', 'PhysicalHealth',\n",
        "                    'MentalHealth', 'AgeCategory', 'SleepTime']].corr(),\n",
        "                color_continuous_scale=\"Blues\")\n",
        "fig.update_layout(height=800)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayjGREK1GRIF",
        "outputId": "f3e4cc07-55fa-40cf-a298-eafa93d483c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    292422\n",
              "1     27373\n",
              "Name: HeartDisease, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df['HeartDisease'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiyNMxTwGgJM"
      },
      "source": [
        "# Проблема дисбалансировки\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPAWCNtEGfsk",
        "outputId": "64b992d8-1daa-4c16-82b1-f03ebd9e3e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    292422\n",
            "1    292422\n",
            "Name: HeartDisease, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "class_0 = df[df['HeartDisease'] == 0]\n",
        "class_1 = df[df['HeartDisease'] == 1]\n",
        "\n",
        "class_1 = class_1.sample(len(class_0),replace=True)\n",
        "df = pd.concat([class_0, class_1], axis=0)\n",
        "print(df['HeartDisease'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BAUc285G-17"
      },
      "source": [
        "# Features & Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG0EK8-aHB6r",
        "outputId": "9fe9d4c8-425a-4fd1-961f-ab5fd919ff4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((584844, 29), (584844,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "features = np.array(df[['BMI', 'Smoking', 'AlcoholDrinking', 'Stroke',\n",
        "       'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Sex', 'AgeCategory',\n",
        "       'PhysicalActivity', 'SleepTime', 'Asthma', 'KidneyDisease',\n",
        "       'SkinCancer', 'Race_American Indian/Alaskan Native', 'Race_Asian',\n",
        "       'Race_Black', 'Race_Hispanic', 'Race_Other', 'Race_White',\n",
        "       'Diabetic_No', 'Diabetic_No, borderline diabetes', 'Diabetic_Yes',\n",
        "       'Diabetic_Yes (during pregnancy)', 'GenHealth_Excellent',\n",
        "       'GenHealth_Fair', 'GenHealth_Good', 'GenHealth_Poor',\n",
        "       'GenHealth_Very good']])\n",
        "\n",
        "labels = np.array(df['HeartDisease'])\n",
        "\n",
        "features.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDAk2jldA3UC",
        "outputId": "720de8fa-d836-4e74-ebee-170de4f094cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.21444386, 0.        , 0.        , 1.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
              "       0.29166667, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
              "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "features[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlnF4H9tFLSD"
      },
      "source": [
        "# Model Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "firJhsq7i7bz",
        "outputId": "da80cd28-5c06-403b-f7d5-326291ab6d68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(467875, 116969)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "input_tensor = torch.from_numpy(features.astype(np.float32))\n",
        "label_tensor = torch.from_numpy(labels.astype(np.float32))\n",
        "\n",
        "dataset = TensorDataset(input_tensor, label_tensor)\n",
        "\n",
        "train_len = int(len(dataset) * 0.8)\n",
        "val_len = int(len(dataset) - train_len)\n",
        "\n",
        "(train_len, val_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKEhb-sTB5vD"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=29, num_workers=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=29, num_workers=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghyg_Zi6DRhx"
      },
      "source": [
        "### Test dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggcA9PSwDUET",
        "outputId": "abe2b633-772e-49cc-ba11-ef22c359c1d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element: 0\n",
            "X:\n",
            "tensor([[0.3228, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3382, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3218, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2383, 1.0000, 0.0000, 1.0000, 0.0333, 0.1333, 0.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3384, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.3375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2552, 1.0000, 0.0000, 0.0000, 0.6667, 1.0000, 1.0000, 0.0000, 0.3375,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3070, 1.0000, 0.0000, 0.0000, 0.2667, 0.0000, 1.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4304, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2574, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3358, 1.0000, 1.0000, 0.0000, 0.0667, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3146, 1.0000, 0.0000, 0.0000, 0.0667, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.4167, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2428, 1.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.4625,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2297, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2649, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3268, 0.0000, 1.0000, 0.0000, 0.0000, 0.1000, 0.0000, 1.0000, 0.4000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2477, 1.0000, 0.0000, 1.0000, 1.0000, 0.1667, 1.0000, 1.0000, 0.7125,\n",
            "         0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2757, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3174, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3308, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3684, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 1.0000, 0.0000, 0.5250,\n",
            "         0.0000, 0.1250, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2859, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3422, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 1.0000, 0.0000, 0.9000,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2485, 0.0000, 0.0000, 0.0000, 0.1667, 0.0667, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2966, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3026, 0.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2652, 0.0000, 0.0000, 0.0000, 0.0667, 0.0333, 0.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4248, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2147, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2625,\n",
            "         1.0000, 0.2917, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4550, 0.0000, 0.0000, 0.0000, 0.0667, 0.0000, 0.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.2500, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 1\n",
            "X:\n",
            "tensor([[0.3101, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         0.0000, 0.2500, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3403, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2794, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2206, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2958, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.7750,\n",
            "         0.0000, 0.6667, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2857, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3527, 1.0000, 0.0000, 0.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5875,\n",
            "         0.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3603, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.4167, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2055, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3082, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 1.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3456, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2269, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2646, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1603, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2912, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6500,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2203, 1.0000, 0.0000, 0.0000, 0.2333, 0.4667, 1.0000, 0.0000, 0.9000,\n",
            "         0.0000, 0.1667, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3076, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3438, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4625,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3228, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2036, 1.0000, 0.0000, 0.0000, 0.0000, 0.2333, 0.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.2917, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3206, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2571, 1.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.0000, 1.0000, 0.5250,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.3375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2172, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.0000, 0.0000, 0.3375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3658, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.2500, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2496, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1715, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6500,\n",
            "         0.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3220, 0.0000, 1.0000, 0.0000, 0.1333, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 2\n",
            "X:\n",
            "tensor([[0.2802, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 1.0000, 1.0000, 0.7750,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2802, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2859, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3736, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2102, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2585, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3113, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3308, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 1.0000, 0.4000,\n",
            "         1.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2544, 0.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 0.0000, 0.5875,\n",
            "         1.0000, 0.2917, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3676, 1.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2805, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3206, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2719, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2211, 0.0000, 0.0000, 0.0000, 0.1000, 1.0000, 0.0000, 1.0000, 0.3375,\n",
            "         1.0000, 0.2500, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2093, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3548, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2645, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.3333, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3238, 1.0000, 0.0000, 0.0000, 0.1333, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2794, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3077, 1.0000, 0.0000, 1.0000, 0.9667, 0.0667, 1.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2798, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2814, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5250,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2662, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3549, 0.0000, 0.0000, 0.0000, 0.0000, 0.4667, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2353, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5250,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3721, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000, 0.0000, 1.0000, 0.3375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2457, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5875,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2895, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3026, 1.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 3\n",
            "X:\n",
            "tensor([[0.3744, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.2625,\n",
            "         1.0000, 0.1667, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2008, 1.0000, 0.0000, 0.0000, 0.0667, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2216, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2958, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3161, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.1667, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3286, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.2917, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2646, 0.0000, 0.0000, 0.0000, 0.2333, 0.0000, 0.0000, 1.0000, 0.2625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2796, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2632, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.9625,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2642, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3146, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2950, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2972, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2501, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3174, 1.0000, 0.0000, 0.0000, 1.0000, 0.0333, 0.0000, 0.0000, 0.9000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2805, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3193, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         0.0000, 0.2917, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2715, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.0000, 0.0000, 0.5875,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5250,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3138, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2368, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.5250,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3850, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6500,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3234, 1.0000, 0.0000, 0.0000, 0.5000, 0.1667, 1.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2715, 0.0000, 0.0000, 0.0000, 0.5000, 0.0667, 0.0000, 0.0000, 0.5875,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.5036, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2723, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5875,\n",
            "         0.0000, 0.2083, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4129, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5875,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2383, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.0000, 0.0000, 0.3375,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 4\n",
            "X:\n",
            "tensor([[0.2526, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.2625,\n",
            "         1.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.1974, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3088, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.6500,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2084, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3317, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.3375,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3113, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7125,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3624, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3118, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2409, 1.0000, 0.0000, 0.0000, 0.2333, 0.2333, 1.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3403, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5250,\n",
            "         1.0000, 0.2917, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2775, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4491, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5250,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3289, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3063, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4625,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2997, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6500,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2895, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.0000, 1.0000, 0.4625,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3514, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 1.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2859, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2569, 1.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         0.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3912, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3593, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5875,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2206, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2632, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2886, 0.0000, 0.0000, 0.0000, 0.0667, 1.0000, 0.0000, 0.0000, 0.2625,\n",
            "         1.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3182, 0.0000, 0.0000, 0.0000, 0.1000, 0.0333, 0.0000, 0.0000, 0.7750,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4293, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5250,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2544, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.2917, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3258, 1.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
            "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 5\n",
            "X:\n",
            "tensor([[0.2801, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2615, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5250,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2889, 0.0000, 0.0000, 0.0000, 0.0000, 0.2667, 0.0000, 0.0000, 0.2625,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3167, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.5174, 0.0000, 0.0000, 0.0000, 0.5000, 0.1000, 1.0000, 0.0000, 0.9000,\n",
            "         0.0000, 0.4167, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2603, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 1.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.3750, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2533, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2642, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2932, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.9625,\n",
            "         0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2217, 0.0000, 0.0000, 0.0000, 0.0667, 0.0000, 0.0000, 0.0000, 0.3375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2449, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4295, 1.0000, 0.0000, 0.0000, 1.0000, 0.5000, 1.0000, 0.0000, 0.8375,\n",
            "         0.0000, 0.4167, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.5509, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2802, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3619, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.3333, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.1899, 0.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 0.0000, 0.2625,\n",
            "         1.0000, 0.2500, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2436, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3138, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.9625,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3158, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.3333, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3736, 1.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3847, 0.0000, 0.0000, 0.0000, 0.6000, 0.0667, 1.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3023, 1.0000, 0.0000, 0.0000, 0.1000, 0.5000, 1.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2886, 1.0000, 0.0000, 1.0000, 0.8333, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.3750, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2725, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3600, 1.0000, 0.0000, 0.0000, 1.0000, 0.3333, 1.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3076, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4625,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3599, 1.0000, 0.0000, 0.0000, 1.0000, 0.1000, 1.0000, 0.0000, 0.9000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2102, 1.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.0000, 1.0000, 0.2625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.])\n",
            "y.shape:\n",
            "torch.Size([29])\n"
          ]
        }
      ],
      "source": [
        "size = len(train_dataloader.dataset)\n",
        "for idx, (X, y) in enumerate(train_dataloader):\n",
        "    print(\"Element: \" + str(idx))\n",
        "    print(\"X:\")\n",
        "    print(X)\n",
        "    print(\"X.shape:\")\n",
        "    print(X.shape)\n",
        "    print(\"y:\")\n",
        "    print(y)\n",
        "    print(\"y.shape:\")\n",
        "    print(y.shape)\n",
        "    if idx == 5:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJRZj0TxCWxQ"
      },
      "source": [
        "## Torch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTowd4AQHOiH"
      },
      "outputs": [],
      "source": [
        "class TropicalPlusTorch:\n",
        "\n",
        "\n",
        "    def max_plus_sum(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        # max-plus sum\n",
        "        return torch.maximum(A, B).to(device)\n",
        "\n",
        "    def min_plus_sum(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        # min-plus sum\n",
        "        return torch.minimum(A, B).to(device)\n",
        "\n",
        "    def max_plus_mul(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        # max-plus mul\n",
        "        res_m = A.size()[0]\n",
        "        res_n = B.size()[1]\n",
        "        new_B = torch.transpose(B, 0, 1).repeat(res_m, 1).to(device)\n",
        "        new_A = torch.repeat_interleave(A, torch.full((1, A.size()[0]), res_n)[0].to(device), dim=0).to(device)\n",
        "        return torch.reshape(torch.amax(new_A + new_B, 1), (res_m, res_n)).to(device)\n",
        "\n",
        "    def min_plus_mul(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        # min-plus mul\n",
        "        res_m = A.size()[0]\n",
        "        res_n = B.size()[1]\n",
        "        new_B = torch.transpose(B, 0, 1).repeat(res_m, 1).to(device)\n",
        "        new_A = torch.repeat_interleave(A, torch.full((1, A.size()[0]), res_n)[0].to(device), dim=0).to(device)\n",
        "        return torch.reshape(torch.amin(new_A + new_B, 1), (res_m, res_n)).to(device)\n",
        "\n",
        "\n",
        "tropic = TropicalPlusTorch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL910nRO2bp8"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class TropicalLinearLayer_max(nn.Module):\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "    def __init__(self, size_in, size_out):\n",
        "        super().__init__()\n",
        "        self.size_in, self.size_out = size_in, size_out\n",
        "        weights = torch.Tensor(size_out, size_in)\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "        bias = torch.Tensor(size_out)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        # initialize weights and biases\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape, self.weights.t().shape)\n",
        "        w_time_x = tropic.max_plus_mul(x, self.weights.t()) #tropical_max_plus(x, self.weights.t())\n",
        "        #print(w_time_x.shape, self.bias.shape)\n",
        "        return tropic.max_plus_sum(w_time_x, self.bias)\n",
        "\n",
        "class TropicalLinearLayer_min(nn.Module):\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "    def __init__(self, size_in, size_out):\n",
        "        super().__init__()\n",
        "        self.size_in, self.size_out = size_in, size_out\n",
        "        weights = torch.Tensor(size_out, size_in)\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "        bias = torch.Tensor(size_out)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        # initialize weights and biases\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape, self.weights.t().shape)\n",
        "        w_time_x = tropic.min_plus_mul(x, self.weights.t()) #tropical_max_plus(x, self.weights.t())\n",
        "        #print(w_time_x.shape, self.bias.shape)\n",
        "        return tropic.min_plus_sum(w_time_x, self.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F3hpk0LCWIq",
        "outputId": "9a8e74cc-cbef-424b-e86e-7ea6fed05904"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (network): Sequential(\n",
              "    (0): Linear(in_features=29, out_features=58, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=58, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from torch import nn, Tensor\n",
        "import math \n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        linLayer = TropicalLinearLayer_max # nn.Linear\n",
        "        minLayer = TropicalLinearLayer_min\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(29, 58),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(58, 2)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.network(x)\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VmWb9vKCsUf"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEgNwTt2_ZXd"
      },
      "outputs": [],
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "train_acc = []\n",
        "val_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQhdsFszCtRx"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, epoch=0):\n",
        "    size = len(dataloader.dataset)\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.type(torch.LongTensor)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        epoch_loss += loss.item()\n",
        "        # Backpropagation\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    train_acc.append(correct / size)\n",
        "    train_loss.append(epoch_loss / size)\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, epoch):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.type(torch.LongTensor)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            i += 1\n",
        "    test_loss /= num_batches\n",
        "    val_loss.append(test_loss)\n",
        "    correct /= size\n",
        "    val_acc.append(correct)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Rye25al-6cjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du3NOCgxCzIa",
        "outputId": "e4aa12a6-f25c-432f-fec7-a8fcf5f4578c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "loss: 0.575877  [466900/467875]\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522874 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 6 s\n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.474357  [    0/467875]\n",
            "loss: 0.530758  [ 2900/467875]\n",
            "loss: 0.706346  [ 5800/467875]\n",
            "loss: 0.554850  [ 8700/467875]\n",
            "loss: 0.536961  [11600/467875]\n",
            "loss: 0.445264  [14500/467875]\n",
            "loss: 0.417384  [17400/467875]\n",
            "loss: 0.448017  [20300/467875]\n",
            "loss: 0.513633  [23200/467875]\n",
            "loss: 0.569376  [26100/467875]\n",
            "loss: 0.588111  [29000/467875]\n",
            "loss: 0.446918  [31900/467875]\n",
            "loss: 0.564052  [34800/467875]\n",
            "loss: 0.587081  [37700/467875]\n",
            "loss: 0.487339  [40600/467875]\n",
            "loss: 0.559687  [43500/467875]\n",
            "loss: 0.624158  [46400/467875]\n",
            "loss: 0.485490  [49300/467875]\n",
            "loss: 0.465672  [52200/467875]\n",
            "loss: 0.554284  [55100/467875]\n",
            "loss: 0.483858  [58000/467875]\n",
            "loss: 0.407054  [60900/467875]\n",
            "loss: 0.437020  [63800/467875]\n",
            "loss: 0.504843  [66700/467875]\n",
            "loss: 0.570190  [69600/467875]\n",
            "loss: 0.549772  [72500/467875]\n",
            "loss: 0.440240  [75400/467875]\n",
            "loss: 0.567278  [78300/467875]\n",
            "loss: 0.488465  [81200/467875]\n",
            "loss: 0.570715  [84100/467875]\n",
            "loss: 0.473085  [87000/467875]\n",
            "loss: 0.468717  [89900/467875]\n",
            "loss: 0.457283  [92800/467875]\n",
            "loss: 0.554897  [95700/467875]\n",
            "loss: 0.562763  [98600/467875]\n",
            "loss: 0.548369  [101500/467875]\n",
            "loss: 0.479939  [104400/467875]\n",
            "loss: 0.594408  [107300/467875]\n",
            "loss: 0.476842  [110200/467875]\n",
            "loss: 0.618699  [113100/467875]\n",
            "loss: 0.477645  [116000/467875]\n",
            "loss: 0.488704  [118900/467875]\n",
            "loss: 0.484422  [121800/467875]\n",
            "loss: 0.592122  [124700/467875]\n",
            "loss: 0.565467  [127600/467875]\n",
            "loss: 0.488757  [130500/467875]\n",
            "loss: 0.502879  [133400/467875]\n",
            "loss: 0.482639  [136300/467875]\n",
            "loss: 0.630812  [139200/467875]\n",
            "loss: 0.582205  [142100/467875]\n",
            "loss: 0.632987  [145000/467875]\n",
            "loss: 0.550009  [147900/467875]\n",
            "loss: 0.500738  [150800/467875]\n",
            "loss: 0.414564  [153700/467875]\n",
            "loss: 0.512860  [156600/467875]\n",
            "loss: 0.556808  [159500/467875]\n",
            "loss: 0.425924  [162400/467875]\n",
            "loss: 0.592768  [165300/467875]\n",
            "loss: 0.556433  [168200/467875]\n",
            "loss: 0.518130  [171100/467875]\n",
            "loss: 0.482691  [174000/467875]\n",
            "loss: 0.482102  [176900/467875]\n",
            "loss: 0.576867  [179800/467875]\n",
            "loss: 0.492304  [182700/467875]\n",
            "loss: 0.596115  [185600/467875]\n",
            "loss: 0.469411  [188500/467875]\n",
            "loss: 0.590503  [191400/467875]\n",
            "loss: 0.496682  [194300/467875]\n",
            "loss: 0.431542  [197200/467875]\n",
            "loss: 0.387086  [200100/467875]\n",
            "loss: 0.434264  [203000/467875]\n",
            "loss: 0.619776  [205900/467875]\n",
            "loss: 0.610058  [208800/467875]\n",
            "loss: 0.515380  [211700/467875]\n",
            "loss: 0.544470  [214600/467875]\n",
            "loss: 0.451140  [217500/467875]\n",
            "loss: 0.459239  [220400/467875]\n",
            "loss: 0.527010  [223300/467875]\n",
            "loss: 0.502614  [226200/467875]\n",
            "loss: 0.458570  [229100/467875]\n",
            "loss: 0.455468  [232000/467875]\n",
            "loss: 0.406561  [234900/467875]\n",
            "loss: 0.476064  [237800/467875]\n",
            "loss: 0.658344  [240700/467875]\n",
            "loss: 0.561156  [243600/467875]\n",
            "loss: 0.547058  [246500/467875]\n",
            "loss: 0.419906  [249400/467875]\n",
            "loss: 0.524658  [252300/467875]\n",
            "loss: 0.445996  [255200/467875]\n",
            "loss: 0.558225  [258100/467875]\n",
            "loss: 0.483034  [261000/467875]\n",
            "loss: 0.379648  [263900/467875]\n",
            "loss: 0.424812  [266800/467875]\n",
            "loss: 0.483778  [269700/467875]\n",
            "loss: 0.551401  [272600/467875]\n",
            "loss: 0.607569  [275500/467875]\n",
            "loss: 0.600056  [278400/467875]\n",
            "loss: 0.592367  [281300/467875]\n",
            "loss: 0.580437  [284200/467875]\n",
            "loss: 0.559937  [287100/467875]\n",
            "loss: 0.521008  [290000/467875]\n",
            "loss: 0.483231  [292900/467875]\n",
            "loss: 0.546982  [295800/467875]\n",
            "loss: 0.534637  [298700/467875]\n",
            "loss: 0.580882  [301600/467875]\n",
            "loss: 0.458180  [304500/467875]\n",
            "loss: 0.463646  [307400/467875]\n",
            "loss: 0.529297  [310300/467875]\n",
            "loss: 0.522795  [313200/467875]\n",
            "loss: 0.569042  [316100/467875]\n",
            "loss: 0.585263  [319000/467875]\n",
            "loss: 0.566846  [321900/467875]\n",
            "loss: 0.410633  [324800/467875]\n",
            "loss: 0.546252  [327700/467875]\n",
            "loss: 0.527700  [330600/467875]\n",
            "loss: 0.445566  [333500/467875]\n",
            "loss: 0.490587  [336400/467875]\n",
            "loss: 0.526353  [339300/467875]\n",
            "loss: 0.602987  [342200/467875]\n",
            "loss: 0.527065  [345100/467875]\n",
            "loss: 0.513177  [348000/467875]\n",
            "loss: 0.501151  [350900/467875]\n",
            "loss: 0.506987  [353800/467875]\n",
            "loss: 0.647632  [356700/467875]\n",
            "loss: 0.586954  [359600/467875]\n",
            "loss: 0.467900  [362500/467875]\n",
            "loss: 0.477756  [365400/467875]\n",
            "loss: 0.544945  [368300/467875]\n",
            "loss: 0.511846  [371200/467875]\n",
            "loss: 0.535336  [374100/467875]\n",
            "loss: 0.544119  [377000/467875]\n",
            "loss: 0.474532  [379900/467875]\n",
            "loss: 0.463161  [382800/467875]\n",
            "loss: 0.483299  [385700/467875]\n",
            "loss: 0.628879  [388600/467875]\n",
            "loss: 0.514113  [391500/467875]\n",
            "loss: 0.483052  [394400/467875]\n",
            "loss: 0.539060  [397300/467875]\n",
            "loss: 0.582275  [400200/467875]\n",
            "loss: 0.501438  [403100/467875]\n",
            "loss: 0.499125  [406000/467875]\n",
            "loss: 0.502388  [408900/467875]\n",
            "loss: 0.502490  [411800/467875]\n",
            "loss: 0.604225  [414700/467875]\n",
            "loss: 0.455472  [417600/467875]\n",
            "loss: 0.625120  [420500/467875]\n",
            "loss: 0.496604  [423400/467875]\n",
            "loss: 0.448785  [426300/467875]\n",
            "loss: 0.470601  [429200/467875]\n",
            "loss: 0.447777  [432100/467875]\n",
            "loss: 0.626583  [435000/467875]\n",
            "loss: 0.570733  [437900/467875]\n",
            "loss: 0.494605  [440800/467875]\n",
            "loss: 0.532380  [443700/467875]\n",
            "loss: 0.528350  [446600/467875]\n",
            "loss: 0.561934  [449500/467875]\n",
            "loss: 0.572010  [452400/467875]\n",
            "loss: 0.551365  [455300/467875]\n",
            "loss: 0.497089  [458200/467875]\n",
            "loss: 0.568244  [461100/467875]\n",
            "loss: 0.403199  [464000/467875]\n",
            "loss: 0.435993  [466900/467875]\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522701 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 8 s\n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.508451  [    0/467875]\n",
            "loss: 0.551466  [ 2900/467875]\n",
            "loss: 0.502377  [ 5800/467875]\n",
            "loss: 0.507662  [ 8700/467875]\n",
            "loss: 0.621612  [11600/467875]\n",
            "loss: 0.479220  [14500/467875]\n",
            "loss: 0.446952  [17400/467875]\n",
            "loss: 0.519180  [20300/467875]\n",
            "loss: 0.583836  [23200/467875]\n",
            "loss: 0.566709  [26100/467875]\n",
            "loss: 0.437205  [29000/467875]\n",
            "loss: 0.575838  [31900/467875]\n",
            "loss: 0.615365  [34800/467875]\n",
            "loss: 0.522205  [37700/467875]\n",
            "loss: 0.604442  [40600/467875]\n",
            "loss: 0.530882  [43500/467875]\n",
            "loss: 0.404683  [46400/467875]\n",
            "loss: 0.610130  [49300/467875]\n",
            "loss: 0.499415  [52200/467875]\n",
            "loss: 0.509207  [55100/467875]\n",
            "loss: 0.507828  [58000/467875]\n",
            "loss: 0.568326  [60900/467875]\n",
            "loss: 0.511278  [63800/467875]\n",
            "loss: 0.499979  [66700/467875]\n",
            "loss: 0.484756  [69600/467875]\n",
            "loss: 0.487375  [72500/467875]\n",
            "loss: 0.532701  [75400/467875]\n",
            "loss: 0.465832  [78300/467875]\n",
            "loss: 0.564021  [81200/467875]\n",
            "loss: 0.426780  [84100/467875]\n",
            "loss: 0.514266  [87000/467875]\n",
            "loss: 0.443278  [89900/467875]\n",
            "loss: 0.626421  [92800/467875]\n",
            "loss: 0.598068  [95700/467875]\n",
            "loss: 0.424573  [98600/467875]\n",
            "loss: 0.493851  [101500/467875]\n",
            "loss: 0.451306  [104400/467875]\n",
            "loss: 0.490873  [107300/467875]\n",
            "loss: 0.541214  [110200/467875]\n",
            "loss: 0.464870  [113100/467875]\n",
            "loss: 0.627023  [116000/467875]\n",
            "loss: 0.441593  [118900/467875]\n",
            "loss: 0.539289  [121800/467875]\n",
            "loss: 0.551793  [124700/467875]\n",
            "loss: 0.532997  [127600/467875]\n",
            "loss: 0.537657  [130500/467875]\n",
            "loss: 0.464343  [133400/467875]\n",
            "loss: 0.544330  [136300/467875]\n",
            "loss: 0.468475  [139200/467875]\n",
            "loss: 0.586976  [142100/467875]\n",
            "loss: 0.441128  [145000/467875]\n",
            "loss: 0.522933  [147900/467875]\n",
            "loss: 0.485070  [150800/467875]\n",
            "loss: 0.496389  [153700/467875]\n",
            "loss: 0.447890  [156600/467875]\n",
            "loss: 0.561566  [159500/467875]\n",
            "loss: 0.534544  [162400/467875]\n",
            "loss: 0.537207  [165300/467875]\n",
            "loss: 0.483976  [168200/467875]\n",
            "loss: 0.489557  [171100/467875]\n",
            "loss: 0.454476  [174000/467875]\n",
            "loss: 0.553130  [176900/467875]\n",
            "loss: 0.410831  [179800/467875]\n",
            "loss: 0.480238  [182700/467875]\n",
            "loss: 0.442056  [185600/467875]\n",
            "loss: 0.533200  [188500/467875]\n",
            "loss: 0.580217  [191400/467875]\n",
            "loss: 0.458780  [194300/467875]\n",
            "loss: 0.498311  [197200/467875]\n",
            "loss: 0.427250  [200100/467875]\n",
            "loss: 0.499600  [203000/467875]\n",
            "loss: 0.479977  [205900/467875]\n",
            "loss: 0.665672  [208800/467875]\n",
            "loss: 0.557191  [211700/467875]\n",
            "loss: 0.513639  [214600/467875]\n",
            "loss: 0.505001  [217500/467875]\n",
            "loss: 0.517822  [220400/467875]\n",
            "loss: 0.531949  [223300/467875]\n",
            "loss: 0.559704  [226200/467875]\n",
            "loss: 0.525950  [229100/467875]\n",
            "loss: 0.525941  [232000/467875]\n",
            "loss: 0.540902  [234900/467875]\n",
            "loss: 0.554064  [237800/467875]\n",
            "loss: 0.551537  [240700/467875]\n",
            "loss: 0.554106  [243600/467875]\n",
            "loss: 0.478682  [246500/467875]\n",
            "loss: 0.493627  [249400/467875]\n",
            "loss: 0.414851  [252300/467875]\n",
            "loss: 0.452295  [255200/467875]\n",
            "loss: 0.697494  [258100/467875]\n",
            "loss: 0.551009  [261000/467875]\n",
            "loss: 0.659891  [263900/467875]\n",
            "loss: 0.609223  [266800/467875]\n",
            "loss: 0.500417  [269700/467875]\n",
            "loss: 0.485172  [272600/467875]\n",
            "loss: 0.383888  [275500/467875]\n",
            "loss: 0.568199  [278400/467875]\n",
            "loss: 0.608946  [281300/467875]\n",
            "loss: 0.493027  [284200/467875]\n",
            "loss: 0.496633  [287100/467875]\n",
            "loss: 0.567693  [290000/467875]\n",
            "loss: 0.461319  [292900/467875]\n",
            "loss: 0.461793  [295800/467875]\n",
            "loss: 0.593351  [298700/467875]\n",
            "loss: 0.567924  [301600/467875]\n",
            "loss: 0.556898  [304500/467875]\n",
            "loss: 0.637938  [307400/467875]\n",
            "loss: 0.543516  [310300/467875]\n",
            "loss: 0.552040  [313200/467875]\n",
            "loss: 0.615606  [316100/467875]\n",
            "loss: 0.491392  [319000/467875]\n",
            "loss: 0.470355  [321900/467875]\n",
            "loss: 0.495102  [324800/467875]\n",
            "loss: 0.583792  [327700/467875]\n",
            "loss: 0.562305  [330600/467875]\n",
            "loss: 0.462739  [333500/467875]\n",
            "loss: 0.456069  [336400/467875]\n",
            "loss: 0.508447  [339300/467875]\n",
            "loss: 0.487954  [342200/467875]\n",
            "loss: 0.510698  [345100/467875]\n",
            "loss: 0.558928  [348000/467875]\n",
            "loss: 0.474114  [350900/467875]\n",
            "loss: 0.501929  [353800/467875]\n",
            "loss: 0.513843  [356700/467875]\n",
            "loss: 0.486846  [359600/467875]\n",
            "loss: 0.538819  [362500/467875]\n",
            "loss: 0.433487  [365400/467875]\n",
            "loss: 0.583924  [368300/467875]\n",
            "loss: 0.476716  [371200/467875]\n",
            "loss: 0.526246  [374100/467875]\n",
            "loss: 0.583664  [377000/467875]\n",
            "loss: 0.506598  [379900/467875]\n",
            "loss: 0.592237  [382800/467875]\n",
            "loss: 0.662086  [385700/467875]\n",
            "loss: 0.663727  [388600/467875]\n",
            "loss: 0.491760  [391500/467875]\n",
            "loss: 0.581717  [394400/467875]\n",
            "loss: 0.510030  [397300/467875]\n",
            "loss: 0.478612  [400200/467875]\n",
            "loss: 0.498075  [403100/467875]\n",
            "loss: 0.431938  [406000/467875]\n",
            "loss: 0.657668  [408900/467875]\n",
            "loss: 0.618924  [411800/467875]\n",
            "loss: 0.480739  [414700/467875]\n",
            "loss: 0.515061  [417600/467875]\n",
            "loss: 0.565633  [420500/467875]\n",
            "loss: 0.523712  [423400/467875]\n",
            "loss: 0.501070  [426300/467875]\n",
            "loss: 0.581491  [429200/467875]\n",
            "loss: 0.505649  [432100/467875]\n",
            "loss: 0.431226  [435000/467875]\n",
            "loss: 0.559852  [437900/467875]\n",
            "loss: 0.497574  [440800/467875]\n",
            "loss: 0.440699  [443700/467875]\n",
            "loss: 0.457910  [446600/467875]\n",
            "loss: 0.490141  [449500/467875]\n",
            "loss: 0.426371  [452400/467875]\n",
            "loss: 0.466694  [455300/467875]\n",
            "loss: 0.527928  [458200/467875]\n",
            "loss: 0.521731  [461100/467875]\n",
            "loss: 0.514361  [464000/467875]\n",
            "loss: 0.442137  [466900/467875]\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522874 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 8 s\n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.448973  [    0/467875]\n",
            "loss: 0.515907  [ 2900/467875]\n",
            "loss: 0.515917  [ 5800/467875]\n",
            "loss: 0.380171  [ 8700/467875]\n",
            "loss: 0.360312  [11600/467875]\n",
            "loss: 0.527677  [14500/467875]\n",
            "loss: 0.454600  [17400/467875]\n",
            "loss: 0.389506  [20300/467875]\n",
            "loss: 0.442355  [23200/467875]\n",
            "loss: 0.501041  [26100/467875]\n",
            "loss: 0.545797  [29000/467875]\n",
            "loss: 0.451189  [31900/467875]\n",
            "loss: 0.586225  [34800/467875]\n",
            "loss: 0.528499  [37700/467875]\n",
            "loss: 0.624872  [40600/467875]\n",
            "loss: 0.488270  [43500/467875]\n",
            "loss: 0.475268  [46400/467875]\n",
            "loss: 0.481509  [49300/467875]\n",
            "loss: 0.528632  [52200/467875]\n",
            "loss: 0.586187  [55100/467875]\n",
            "loss: 0.466066  [58000/467875]\n",
            "loss: 0.397552  [60900/467875]\n",
            "loss: 0.442903  [63800/467875]\n",
            "loss: 0.554727  [66700/467875]\n",
            "loss: 0.454671  [69600/467875]\n",
            "loss: 0.679335  [72500/467875]\n",
            "loss: 0.620636  [75400/467875]\n",
            "loss: 0.518691  [78300/467875]\n",
            "loss: 0.570278  [81200/467875]\n",
            "loss: 0.499613  [84100/467875]\n",
            "loss: 0.553959  [87000/467875]\n",
            "loss: 0.447850  [89900/467875]\n",
            "loss: 0.509401  [92800/467875]\n",
            "loss: 0.492751  [95700/467875]\n",
            "loss: 0.455129  [98600/467875]\n",
            "loss: 0.508366  [101500/467875]\n",
            "loss: 0.466590  [104400/467875]\n",
            "loss: 0.474635  [107300/467875]\n",
            "loss: 0.605353  [110200/467875]\n",
            "loss: 0.563841  [113100/467875]\n",
            "loss: 0.467814  [116000/467875]\n",
            "loss: 0.581525  [118900/467875]\n",
            "loss: 0.456762  [121800/467875]\n",
            "loss: 0.442664  [124700/467875]\n",
            "loss: 0.531174  [127600/467875]\n",
            "loss: 0.563314  [130500/467875]\n",
            "loss: 0.673524  [133400/467875]\n",
            "loss: 0.450813  [136300/467875]\n",
            "loss: 0.516298  [139200/467875]\n",
            "loss: 0.645014  [142100/467875]\n",
            "loss: 0.541269  [145000/467875]\n",
            "loss: 0.589404  [147900/467875]\n",
            "loss: 0.577953  [150800/467875]\n",
            "loss: 0.604004  [153700/467875]\n",
            "loss: 0.495556  [156600/467875]\n",
            "loss: 0.499076  [159500/467875]\n",
            "loss: 0.578178  [162400/467875]\n",
            "loss: 0.587037  [165300/467875]\n",
            "loss: 0.522445  [168200/467875]\n",
            "loss: 0.545966  [171100/467875]\n",
            "loss: 0.501467  [174000/467875]\n",
            "loss: 0.366870  [176900/467875]\n",
            "loss: 0.497446  [179800/467875]\n",
            "loss: 0.606341  [182700/467875]\n",
            "loss: 0.561350  [185600/467875]\n",
            "loss: 0.457135  [188500/467875]\n",
            "loss: 0.525825  [191400/467875]\n",
            "loss: 0.469860  [194300/467875]\n",
            "loss: 0.519327  [197200/467875]\n",
            "loss: 0.500870  [200100/467875]\n",
            "loss: 0.517902  [203000/467875]\n",
            "loss: 0.416664  [205900/467875]\n",
            "loss: 0.547229  [208800/467875]\n",
            "loss: 0.582070  [211700/467875]\n",
            "loss: 0.616908  [214600/467875]\n",
            "loss: 0.514632  [217500/467875]\n",
            "loss: 0.531583  [220400/467875]\n",
            "loss: 0.544871  [223300/467875]\n",
            "loss: 0.559482  [226200/467875]\n",
            "loss: 0.469985  [229100/467875]\n",
            "loss: 0.451687  [232000/467875]\n",
            "loss: 0.440104  [234900/467875]\n",
            "loss: 0.476044  [237800/467875]\n",
            "loss: 0.473364  [240700/467875]\n",
            "loss: 0.495394  [243600/467875]\n",
            "loss: 0.511005  [246500/467875]\n",
            "loss: 0.528772  [249400/467875]\n",
            "loss: 0.520045  [252300/467875]\n",
            "loss: 0.526636  [255200/467875]\n",
            "loss: 0.525584  [258100/467875]\n",
            "loss: 0.516437  [261000/467875]\n",
            "loss: 0.464832  [263900/467875]\n",
            "loss: 0.413941  [266800/467875]\n",
            "loss: 0.529077  [269700/467875]\n",
            "loss: 0.500231  [272600/467875]\n",
            "loss: 0.573462  [275500/467875]\n",
            "loss: 0.490766  [278400/467875]\n",
            "loss: 0.483456  [281300/467875]\n",
            "loss: 0.513759  [284200/467875]\n",
            "loss: 0.488091  [287100/467875]\n",
            "loss: 0.429900  [290000/467875]\n",
            "loss: 0.493184  [292900/467875]\n",
            "loss: 0.477289  [295800/467875]\n",
            "loss: 0.584151  [298700/467875]\n",
            "loss: 0.508409  [301600/467875]\n",
            "loss: 0.490180  [304500/467875]\n",
            "loss: 0.478336  [307400/467875]\n",
            "loss: 0.514540  [310300/467875]\n",
            "loss: 0.434188  [313200/467875]\n",
            "loss: 0.591863  [316100/467875]\n",
            "loss: 0.480070  [319000/467875]\n",
            "loss: 0.491198  [321900/467875]\n",
            "loss: 0.522949  [324800/467875]\n",
            "loss: 0.504253  [327700/467875]\n",
            "loss: 0.492664  [330600/467875]\n",
            "loss: 0.526872  [333500/467875]\n",
            "loss: 0.531396  [336400/467875]\n",
            "loss: 0.573482  [339300/467875]\n",
            "loss: 0.530961  [342200/467875]\n",
            "loss: 0.586369  [345100/467875]\n",
            "loss: 0.620522  [348000/467875]\n",
            "loss: 0.554328  [350900/467875]\n",
            "loss: 0.488583  [353800/467875]\n",
            "loss: 0.436617  [356700/467875]\n",
            "loss: 0.534436  [359600/467875]\n",
            "loss: 0.521536  [362500/467875]\n",
            "loss: 0.484192  [365400/467875]\n",
            "loss: 0.428117  [368300/467875]\n",
            "loss: 0.637554  [371200/467875]\n",
            "loss: 0.602094  [374100/467875]\n",
            "loss: 0.538432  [377000/467875]\n",
            "loss: 0.561493  [379900/467875]\n",
            "loss: 0.442953  [382800/467875]\n",
            "loss: 0.579623  [385700/467875]\n",
            "loss: 0.632206  [388600/467875]\n",
            "loss: 0.536059  [391500/467875]\n",
            "loss: 0.592168  [394400/467875]\n",
            "loss: 0.496469  [397300/467875]\n",
            "loss: 0.610472  [400200/467875]\n",
            "loss: 0.406969  [403100/467875]\n",
            "loss: 0.520135  [406000/467875]\n",
            "loss: 0.498518  [408900/467875]\n",
            "loss: 0.488285  [411800/467875]\n",
            "loss: 0.486253  [414700/467875]\n",
            "loss: 0.461152  [417600/467875]\n",
            "loss: 0.546793  [420500/467875]\n",
            "loss: 0.436033  [423400/467875]\n",
            "loss: 0.398882  [426300/467875]\n",
            "loss: 0.488853  [429200/467875]\n",
            "loss: 0.506075  [432100/467875]\n",
            "loss: 0.487375  [435000/467875]\n",
            "loss: 0.511290  [437900/467875]\n",
            "loss: 0.626560  [440800/467875]\n",
            "loss: 0.496275  [443700/467875]\n",
            "loss: 0.446049  [446600/467875]\n",
            "loss: 0.441439  [449500/467875]\n",
            "loss: 0.533804  [452400/467875]\n",
            "loss: 0.467510  [455300/467875]\n",
            "loss: 0.550406  [458200/467875]\n",
            "loss: 0.500760  [461100/467875]\n",
            "loss: 0.523047  [464000/467875]\n",
            "loss: 0.455994  [466900/467875]\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.524466 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 7 s\n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.458009  [    0/467875]\n",
            "loss: 0.613614  [ 2900/467875]\n",
            "loss: 0.462906  [ 5800/467875]\n",
            "loss: 0.444022  [ 8700/467875]\n",
            "loss: 0.498905  [11600/467875]\n",
            "loss: 0.562970  [14500/467875]\n",
            "loss: 0.673218  [17400/467875]\n",
            "loss: 0.521604  [20300/467875]\n",
            "loss: 0.537213  [23200/467875]\n",
            "loss: 0.515712  [26100/467875]\n",
            "loss: 0.610068  [29000/467875]\n",
            "loss: 0.553043  [31900/467875]\n",
            "loss: 0.469334  [34800/467875]\n",
            "loss: 0.539539  [37700/467875]\n",
            "loss: 0.727158  [40600/467875]\n",
            "loss: 0.503139  [43500/467875]\n",
            "loss: 0.552959  [46400/467875]\n",
            "loss: 0.538088  [49300/467875]\n",
            "loss: 0.436405  [52200/467875]\n",
            "loss: 0.566666  [55100/467875]\n",
            "loss: 0.534833  [58000/467875]\n",
            "loss: 0.477156  [60900/467875]\n",
            "loss: 0.520148  [63800/467875]\n",
            "loss: 0.534573  [66700/467875]\n",
            "loss: 0.550615  [69600/467875]\n",
            "loss: 0.502497  [72500/467875]\n",
            "loss: 0.460874  [75400/467875]\n",
            "loss: 0.433487  [78300/467875]\n",
            "loss: 0.565405  [81200/467875]\n",
            "loss: 0.469405  [84100/467875]\n",
            "loss: 0.522164  [87000/467875]\n",
            "loss: 0.490129  [89900/467875]\n",
            "loss: 0.566640  [92800/467875]\n",
            "loss: 0.462430  [95700/467875]\n",
            "loss: 0.451717  [98600/467875]\n",
            "loss: 0.586480  [101500/467875]\n",
            "loss: 0.514813  [104400/467875]\n",
            "loss: 0.418692  [107300/467875]\n",
            "loss: 0.560844  [110200/467875]\n",
            "loss: 0.458050  [113100/467875]\n",
            "loss: 0.606335  [116000/467875]\n",
            "loss: 0.525921  [118900/467875]\n",
            "loss: 0.465796  [121800/467875]\n",
            "loss: 0.535049  [124700/467875]\n",
            "loss: 0.608965  [127600/467875]\n",
            "loss: 0.674531  [130500/467875]\n",
            "loss: 0.478740  [133400/467875]\n",
            "loss: 0.526555  [136300/467875]\n",
            "loss: 0.452916  [139200/467875]\n",
            "loss: 0.522548  [142100/467875]\n",
            "loss: 0.521335  [145000/467875]\n",
            "loss: 0.563341  [147900/467875]\n",
            "loss: 0.603107  [150800/467875]\n",
            "loss: 0.515479  [153700/467875]\n",
            "loss: 0.485480  [156600/467875]\n",
            "loss: 0.488510  [159500/467875]\n",
            "loss: 0.511108  [162400/467875]\n",
            "loss: 0.472860  [165300/467875]\n",
            "loss: 0.519066  [168200/467875]\n",
            "loss: 0.502655  [171100/467875]\n",
            "loss: 0.436315  [174000/467875]\n",
            "loss: 0.523181  [176900/467875]\n",
            "loss: 0.474443  [179800/467875]\n",
            "loss: 0.591639  [182700/467875]\n",
            "loss: 0.460796  [185600/467875]\n",
            "loss: 0.559962  [188500/467875]\n",
            "loss: 0.479991  [191400/467875]\n",
            "loss: 0.464852  [194300/467875]\n",
            "loss: 0.482531  [197200/467875]\n",
            "loss: 0.488615  [200100/467875]\n",
            "loss: 0.496761  [203000/467875]\n",
            "loss: 0.566774  [205900/467875]\n",
            "loss: 0.529594  [208800/467875]\n",
            "loss: 0.530097  [211700/467875]\n",
            "loss: 0.715344  [214600/467875]\n",
            "loss: 0.531759  [217500/467875]\n",
            "loss: 0.530909  [220400/467875]\n",
            "loss: 0.436000  [223300/467875]\n",
            "loss: 0.510666  [226200/467875]\n",
            "loss: 0.613392  [229100/467875]\n",
            "loss: 0.459778  [232000/467875]\n",
            "loss: 0.431919  [234900/467875]\n",
            "loss: 0.423412  [237800/467875]\n",
            "loss: 0.605807  [240700/467875]\n",
            "loss: 0.544711  [243600/467875]\n",
            "loss: 0.415424  [246500/467875]\n",
            "loss: 0.605010  [249400/467875]\n",
            "loss: 0.534612  [252300/467875]\n",
            "loss: 0.597597  [255200/467875]\n",
            "loss: 0.613602  [258100/467875]\n",
            "loss: 0.528394  [261000/467875]\n",
            "loss: 0.531455  [263900/467875]\n",
            "loss: 0.546408  [266800/467875]\n",
            "loss: 0.475671  [269700/467875]\n",
            "loss: 0.474934  [272600/467875]\n",
            "loss: 0.573114  [275500/467875]\n",
            "loss: 0.568435  [278400/467875]\n",
            "loss: 0.510974  [281300/467875]\n",
            "loss: 0.526606  [284200/467875]\n",
            "loss: 0.441786  [287100/467875]\n",
            "loss: 0.563081  [290000/467875]\n",
            "loss: 0.604643  [292900/467875]\n",
            "loss: 0.497764  [295800/467875]\n",
            "loss: 0.409910  [298700/467875]\n",
            "loss: 0.526250  [301600/467875]\n",
            "loss: 0.556283  [304500/467875]\n",
            "loss: 0.475046  [307400/467875]\n",
            "loss: 0.461526  [310300/467875]\n",
            "loss: 0.620823  [313200/467875]\n",
            "loss: 0.488828  [316100/467875]\n",
            "loss: 0.581355  [319000/467875]\n",
            "loss: 0.492463  [321900/467875]\n",
            "loss: 0.445871  [324800/467875]\n",
            "loss: 0.524870  [327700/467875]\n",
            "loss: 0.484507  [330600/467875]\n",
            "loss: 0.565122  [333500/467875]\n",
            "loss: 0.503765  [336400/467875]\n",
            "loss: 0.428092  [339300/467875]\n",
            "loss: 0.592229  [342200/467875]\n",
            "loss: 0.456562  [345100/467875]\n",
            "loss: 0.495151  [348000/467875]\n",
            "loss: 0.627010  [350900/467875]\n",
            "loss: 0.482371  [353800/467875]\n",
            "loss: 0.515178  [356700/467875]\n",
            "loss: 0.580932  [359600/467875]\n",
            "loss: 0.493108  [362500/467875]\n",
            "loss: 0.593550  [365400/467875]\n",
            "loss: 0.469509  [368300/467875]\n",
            "loss: 0.428600  [371200/467875]\n",
            "loss: 0.626930  [374100/467875]\n",
            "loss: 0.463855  [377000/467875]\n",
            "loss: 0.524417  [379900/467875]\n",
            "loss: 0.596996  [382800/467875]\n",
            "loss: 0.471976  [385700/467875]\n",
            "loss: 0.500341  [388600/467875]\n",
            "loss: 0.499221  [391500/467875]\n",
            "loss: 0.462637  [394400/467875]\n",
            "loss: 0.509371  [397300/467875]\n",
            "loss: 0.511710  [400200/467875]\n",
            "loss: 0.605812  [403100/467875]\n",
            "loss: 0.484733  [406000/467875]\n",
            "loss: 0.628719  [408900/467875]\n",
            "loss: 0.692887  [411800/467875]\n",
            "loss: 0.545914  [414700/467875]\n",
            "loss: 0.481227  [417600/467875]\n",
            "loss: 0.491313  [420500/467875]\n",
            "loss: 0.525360  [423400/467875]\n",
            "loss: 0.471995  [426300/467875]\n",
            "loss: 0.461773  [429200/467875]\n",
            "loss: 0.482619  [432100/467875]\n",
            "loss: 0.610583  [435000/467875]\n",
            "loss: 0.484202  [437900/467875]\n",
            "loss: 0.558989  [440800/467875]\n",
            "loss: 0.535005  [443700/467875]\n",
            "loss: 0.523055  [446600/467875]\n",
            "loss: 0.617114  [449500/467875]\n",
            "loss: 0.483419  [452400/467875]\n",
            "loss: 0.577561  [455300/467875]\n",
            "loss: 0.459918  [458200/467875]\n",
            "loss: 0.706279  [461100/467875]\n",
            "loss: 0.449506  [464000/467875]\n",
            "loss: 0.458812  [466900/467875]\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522965 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 7 s\n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.479072  [    0/467875]\n",
            "loss: 0.553825  [ 2900/467875]\n",
            "loss: 0.491510  [ 5800/467875]\n",
            "loss: 0.507298  [ 8700/467875]\n",
            "loss: 0.530651  [11600/467875]\n",
            "loss: 0.646771  [14500/467875]\n",
            "loss: 0.538543  [17400/467875]\n",
            "loss: 0.580608  [20300/467875]\n",
            "loss: 0.530564  [23200/467875]\n",
            "loss: 0.437023  [26100/467875]\n",
            "loss: 0.552123  [29000/467875]\n",
            "loss: 0.425027  [31900/467875]\n",
            "loss: 0.584125  [34800/467875]\n",
            "loss: 0.488744  [37700/467875]\n",
            "loss: 0.473319  [40600/467875]\n",
            "loss: 0.484409  [43500/467875]\n",
            "loss: 0.452203  [46400/467875]\n",
            "loss: 0.601690  [49300/467875]\n",
            "loss: 0.577989  [52200/467875]\n",
            "loss: 0.493517  [55100/467875]\n",
            "loss: 0.540393  [58000/467875]\n",
            "loss: 0.626218  [60900/467875]\n",
            "loss: 0.685918  [63800/467875]\n",
            "loss: 0.473009  [66700/467875]\n",
            "loss: 0.544250  [69600/467875]\n",
            "loss: 0.540187  [72500/467875]\n",
            "loss: 0.515553  [75400/467875]\n",
            "loss: 0.474607  [78300/467875]\n",
            "loss: 0.452938  [81200/467875]\n",
            "loss: 0.562290  [84100/467875]\n",
            "loss: 0.496953  [87000/467875]\n",
            "loss: 0.514857  [89900/467875]\n",
            "loss: 0.604598  [92800/467875]\n",
            "loss: 0.622382  [95700/467875]\n",
            "loss: 0.696409  [98600/467875]\n",
            "loss: 0.579949  [101500/467875]\n",
            "loss: 0.506572  [104400/467875]\n",
            "loss: 0.600861  [107300/467875]\n",
            "loss: 0.489133  [110200/467875]\n",
            "loss: 0.506295  [113100/467875]\n",
            "loss: 0.607393  [116000/467875]\n",
            "loss: 0.465948  [118900/467875]\n",
            "loss: 0.660157  [121800/467875]\n",
            "loss: 0.429562  [124700/467875]\n",
            "loss: 0.609122  [127600/467875]\n",
            "loss: 0.409021  [130500/467875]\n",
            "loss: 0.470697  [133400/467875]\n",
            "loss: 0.406040  [136300/467875]\n",
            "loss: 0.456836  [139200/467875]\n",
            "loss: 0.466303  [142100/467875]\n",
            "loss: 0.483129  [145000/467875]\n",
            "loss: 0.478175  [147900/467875]\n",
            "loss: 0.439121  [150800/467875]\n",
            "loss: 0.525567  [153700/467875]\n",
            "loss: 0.388311  [156600/467875]\n",
            "loss: 0.495451  [159500/467875]\n",
            "loss: 0.439087  [162400/467875]\n",
            "loss: 0.478761  [165300/467875]\n",
            "loss: 0.500108  [168200/467875]\n",
            "loss: 0.410146  [171100/467875]\n",
            "loss: 0.554247  [174000/467875]\n",
            "loss: 0.504031  [176900/467875]\n",
            "loss: 0.519846  [179800/467875]\n",
            "loss: 0.477159  [182700/467875]\n",
            "loss: 0.614423  [185600/467875]\n",
            "loss: 0.463961  [188500/467875]\n",
            "loss: 0.480581  [191400/467875]\n",
            "loss: 0.515724  [194300/467875]\n",
            "loss: 0.571316  [197200/467875]\n",
            "loss: 0.558940  [200100/467875]\n",
            "loss: 0.504508  [203000/467875]\n",
            "loss: 0.596027  [205900/467875]\n",
            "loss: 0.507347  [208800/467875]\n",
            "loss: 0.574638  [211700/467875]\n",
            "loss: 0.573540  [214600/467875]\n",
            "loss: 0.440278  [217500/467875]\n",
            "loss: 0.484583  [220400/467875]\n",
            "loss: 0.483573  [223300/467875]\n",
            "loss: 0.520801  [226200/467875]\n",
            "loss: 0.607486  [229100/467875]\n",
            "loss: 0.552916  [232000/467875]\n",
            "loss: 0.496010  [234900/467875]\n",
            "loss: 0.451790  [237800/467875]\n",
            "loss: 0.506282  [240700/467875]\n",
            "loss: 0.556884  [243600/467875]\n",
            "loss: 0.512551  [246500/467875]\n",
            "loss: 0.512903  [249400/467875]\n",
            "loss: 0.521939  [252300/467875]\n",
            "loss: 0.394821  [255200/467875]\n",
            "loss: 0.500656  [258100/467875]\n",
            "loss: 0.422193  [261000/467875]\n",
            "loss: 0.498603  [263900/467875]\n",
            "loss: 0.489225  [266800/467875]\n",
            "loss: 0.473558  [269700/467875]\n",
            "loss: 0.415704  [272600/467875]\n",
            "loss: 0.477326  [275500/467875]\n",
            "loss: 0.461982  [278400/467875]\n",
            "loss: 0.507255  [281300/467875]\n",
            "loss: 0.630421  [284200/467875]\n",
            "loss: 0.555911  [287100/467875]\n",
            "loss: 0.594552  [290000/467875]\n",
            "loss: 0.576087  [292900/467875]\n",
            "loss: 0.635527  [295800/467875]\n",
            "loss: 0.463904  [298700/467875]\n",
            "loss: 0.551941  [301600/467875]\n",
            "loss: 0.548202  [304500/467875]\n",
            "loss: 0.658594  [307400/467875]\n",
            "loss: 0.591740  [310300/467875]\n",
            "loss: 0.502806  [313200/467875]\n",
            "loss: 0.539279  [316100/467875]\n",
            "loss: 0.379753  [319000/467875]\n",
            "loss: 0.784234  [321900/467875]\n",
            "loss: 0.424982  [324800/467875]\n",
            "loss: 0.428165  [327700/467875]\n",
            "loss: 0.530438  [330600/467875]\n",
            "loss: 0.475940  [333500/467875]\n",
            "loss: 0.556466  [336400/467875]\n",
            "loss: 0.496626  [339300/467875]\n",
            "loss: 0.452737  [342200/467875]\n",
            "loss: 0.562678  [345100/467875]\n",
            "loss: 0.582038  [348000/467875]\n",
            "loss: 0.487655  [350900/467875]\n",
            "loss: 0.467443  [353800/467875]\n",
            "loss: 0.455284  [356700/467875]\n",
            "loss: 0.547915  [359600/467875]\n",
            "loss: 0.468579  [362500/467875]\n",
            "loss: 0.531874  [365400/467875]\n",
            "loss: 0.407571  [368300/467875]\n",
            "loss: 0.618879  [371200/467875]\n",
            "loss: 0.445660  [374100/467875]\n",
            "loss: 0.487394  [377000/467875]\n",
            "loss: 0.420995  [379900/467875]\n",
            "loss: 0.610479  [382800/467875]\n",
            "loss: 0.519024  [385700/467875]\n",
            "loss: 0.706966  [388600/467875]\n",
            "loss: 0.467145  [391500/467875]\n",
            "loss: 0.641858  [394400/467875]\n",
            "loss: 0.650351  [397300/467875]\n",
            "loss: 0.495797  [400200/467875]\n",
            "loss: 0.482334  [403100/467875]\n",
            "loss: 0.524536  [406000/467875]\n",
            "loss: 0.630026  [408900/467875]\n",
            "loss: 0.530443  [411800/467875]\n",
            "loss: 0.477160  [414700/467875]\n",
            "loss: 0.480860  [417600/467875]\n",
            "loss: 0.674296  [420500/467875]\n",
            "loss: 0.571189  [423400/467875]\n",
            "loss: 0.496185  [426300/467875]\n",
            "loss: 0.414591  [429200/467875]\n",
            "loss: 0.610807  [432100/467875]\n",
            "loss: 0.580955  [435000/467875]\n",
            "loss: 0.506009  [437900/467875]\n",
            "loss: 0.611848  [440800/467875]\n",
            "loss: 0.495578  [443700/467875]\n",
            "loss: 0.458371  [446600/467875]\n",
            "loss: 0.670281  [449500/467875]\n",
            "loss: 0.389787  [452400/467875]\n",
            "loss: 0.523279  [455300/467875]\n",
            "loss: 0.562835  [458200/467875]\n",
            "loss: 0.554578  [461100/467875]\n",
            "loss: 0.539554  [464000/467875]\n",
            "loss: 0.515900  [466900/467875]\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522516 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 8 s\n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.480523  [    0/467875]\n",
            "loss: 0.509405  [ 2900/467875]\n",
            "loss: 0.525976  [ 5800/467875]\n",
            "loss: 0.516922  [ 8700/467875]\n",
            "loss: 0.430575  [11600/467875]\n",
            "loss: 0.508974  [14500/467875]\n",
            "loss: 0.522023  [17400/467875]\n",
            "loss: 0.542673  [20300/467875]\n",
            "loss: 0.455909  [23200/467875]\n",
            "loss: 0.562280  [26100/467875]\n",
            "loss: 0.502434  [29000/467875]\n",
            "loss: 0.540068  [31900/467875]\n",
            "loss: 0.451625  [34800/467875]\n",
            "loss: 0.510825  [37700/467875]\n",
            "loss: 0.508201  [40600/467875]\n",
            "loss: 0.696103  [43500/467875]\n",
            "loss: 0.559367  [46400/467875]\n",
            "loss: 0.597269  [49300/467875]\n",
            "loss: 0.479156  [52200/467875]\n",
            "loss: 0.597479  [55100/467875]\n",
            "loss: 0.469012  [58000/467875]\n",
            "loss: 0.508987  [60900/467875]\n",
            "loss: 0.590609  [63800/467875]\n",
            "loss: 0.429108  [66700/467875]\n",
            "loss: 0.632753  [69600/467875]\n",
            "loss: 0.433030  [72500/467875]\n",
            "loss: 0.539093  [75400/467875]\n",
            "loss: 0.488889  [78300/467875]\n",
            "loss: 0.416290  [81200/467875]\n",
            "loss: 0.571262  [84100/467875]\n",
            "loss: 0.425209  [87000/467875]\n",
            "loss: 0.580394  [89900/467875]\n",
            "loss: 0.523282  [92800/467875]\n",
            "loss: 0.451021  [95700/467875]\n",
            "loss: 0.607325  [98600/467875]\n",
            "loss: 0.457119  [101500/467875]\n",
            "loss: 0.519081  [104400/467875]\n",
            "loss: 0.538411  [107300/467875]\n",
            "loss: 0.458705  [110200/467875]\n",
            "loss: 0.485569  [113100/467875]\n",
            "loss: 0.500627  [116000/467875]\n",
            "loss: 0.642893  [118900/467875]\n",
            "loss: 0.533221  [121800/467875]\n",
            "loss: 0.453586  [124700/467875]\n",
            "loss: 0.554174  [127600/467875]\n",
            "loss: 0.490252  [130500/467875]\n",
            "loss: 0.628218  [133400/467875]\n",
            "loss: 0.431138  [136300/467875]\n",
            "loss: 0.486841  [139200/467875]\n",
            "loss: 0.637390  [142100/467875]\n",
            "loss: 0.542063  [145000/467875]\n",
            "loss: 0.455647  [147900/467875]\n",
            "loss: 0.467372  [150800/467875]\n",
            "loss: 0.587885  [153700/467875]\n",
            "loss: 0.503532  [156600/467875]\n",
            "loss: 0.539887  [159500/467875]\n",
            "loss: 0.491884  [162400/467875]\n",
            "loss: 0.557977  [165300/467875]\n",
            "loss: 0.479178  [168200/467875]\n",
            "loss: 0.421029  [171100/467875]\n",
            "loss: 0.689193  [174000/467875]\n",
            "loss: 0.478026  [176900/467875]\n",
            "loss: 0.507183  [179800/467875]\n",
            "loss: 0.564122  [182700/467875]\n",
            "loss: 0.429728  [185600/467875]\n",
            "loss: 0.520913  [188500/467875]\n",
            "loss: 0.413783  [191400/467875]\n",
            "loss: 0.632505  [194300/467875]\n",
            "loss: 0.583215  [197200/467875]\n",
            "loss: 0.506799  [200100/467875]\n",
            "loss: 0.480921  [203000/467875]\n",
            "loss: 0.428381  [205900/467875]\n",
            "loss: 0.516786  [208800/467875]\n",
            "loss: 0.595119  [211700/467875]\n",
            "loss: 0.410456  [214600/467875]\n",
            "loss: 0.578699  [217500/467875]\n",
            "loss: 0.579200  [220400/467875]\n",
            "loss: 0.548196  [223300/467875]\n",
            "loss: 0.463604  [226200/467875]\n",
            "loss: 0.602014  [229100/467875]\n",
            "loss: 0.563989  [232000/467875]\n",
            "loss: 0.618091  [234900/467875]\n",
            "loss: 0.564717  [237800/467875]\n",
            "loss: 0.646586  [240700/467875]\n",
            "loss: 0.557964  [243600/467875]\n",
            "loss: 0.488216  [246500/467875]\n",
            "loss: 0.534379  [249400/467875]\n",
            "loss: 0.539021  [252300/467875]\n",
            "loss: 0.436431  [255200/467875]\n",
            "loss: 0.585615  [258100/467875]\n",
            "loss: 0.577492  [261000/467875]\n",
            "loss: 0.555828  [263900/467875]\n",
            "loss: 0.460721  [266800/467875]\n",
            "loss: 0.551307  [269700/467875]\n",
            "loss: 0.610597  [272600/467875]\n",
            "loss: 0.468314  [275500/467875]\n",
            "loss: 0.665449  [278400/467875]\n",
            "loss: 0.653433  [281300/467875]\n",
            "loss: 0.445494  [284200/467875]\n",
            "loss: 0.622794  [287100/467875]\n",
            "loss: 0.473163  [290000/467875]\n",
            "loss: 0.456707  [292900/467875]\n",
            "loss: 0.520418  [295800/467875]\n",
            "loss: 0.507783  [298700/467875]\n",
            "loss: 0.482633  [301600/467875]\n",
            "loss: 0.529056  [304500/467875]\n",
            "loss: 0.450091  [307400/467875]\n",
            "loss: 0.470375  [310300/467875]\n",
            "loss: 0.474537  [313200/467875]\n",
            "loss: 0.546462  [316100/467875]\n",
            "loss: 0.655773  [319000/467875]\n",
            "loss: 0.533501  [321900/467875]\n",
            "loss: 0.502811  [324800/467875]\n",
            "loss: 0.488787  [327700/467875]\n",
            "loss: 0.632904  [330600/467875]\n",
            "loss: 0.545426  [333500/467875]\n",
            "loss: 0.468904  [336400/467875]\n",
            "loss: 0.550988  [339300/467875]\n",
            "loss: 0.521425  [342200/467875]\n",
            "loss: 0.513654  [345100/467875]\n",
            "loss: 0.527598  [348000/467875]\n",
            "loss: 0.561276  [350900/467875]\n",
            "loss: 0.586757  [353800/467875]\n",
            "loss: 0.641269  [356700/467875]\n",
            "loss: 0.497381  [359600/467875]\n",
            "loss: 0.352834  [362500/467875]\n",
            "loss: 0.555429  [365400/467875]\n",
            "loss: 0.530910  [368300/467875]\n",
            "loss: 0.540222  [371200/467875]\n",
            "loss: 0.504558  [374100/467875]\n",
            "loss: 0.447953  [377000/467875]\n",
            "loss: 0.521310  [379900/467875]\n",
            "loss: 0.542681  [382800/467875]\n",
            "loss: 0.513244  [385700/467875]\n",
            "loss: 0.640207  [388600/467875]\n",
            "loss: 0.522824  [391500/467875]\n",
            "loss: 0.415095  [394400/467875]\n",
            "loss: 0.521775  [397300/467875]\n",
            "loss: 0.523715  [400200/467875]\n",
            "loss: 0.545299  [403100/467875]\n",
            "loss: 0.471981  [406000/467875]\n",
            "loss: 0.568790  [408900/467875]\n",
            "loss: 0.604520  [411800/467875]\n",
            "loss: 0.544640  [414700/467875]\n",
            "loss: 0.464741  [417600/467875]\n",
            "loss: 0.538014  [420500/467875]\n",
            "loss: 0.532280  [423400/467875]\n",
            "loss: 0.588282  [426300/467875]\n",
            "loss: 0.624208  [429200/467875]\n",
            "loss: 0.507159  [432100/467875]\n",
            "loss: 0.403009  [435000/467875]\n",
            "loss: 0.743670  [437900/467875]\n",
            "loss: 0.497793  [440800/467875]\n",
            "loss: 0.505538  [443700/467875]\n",
            "loss: 0.589183  [446600/467875]\n",
            "loss: 0.606584  [449500/467875]\n",
            "loss: 0.471059  [452400/467875]\n",
            "loss: 0.598354  [455300/467875]\n",
            "loss: 0.531762  [458200/467875]\n",
            "loss: 0.493663  [461100/467875]\n",
            "loss: 0.451748  [464000/467875]\n",
            "loss: 0.555615  [466900/467875]\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.523268 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 7 s\n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.490775  [    0/467875]\n",
            "loss: 0.444061  [ 2900/467875]\n",
            "loss: 0.524778  [ 5800/467875]\n",
            "loss: 0.395593  [ 8700/467875]\n",
            "loss: 0.511256  [11600/467875]\n",
            "loss: 0.506737  [14500/467875]\n",
            "loss: 0.551617  [17400/467875]\n",
            "loss: 0.521007  [20300/467875]\n",
            "loss: 0.498564  [23200/467875]\n",
            "loss: 0.527767  [26100/467875]\n",
            "loss: 0.567945  [29000/467875]\n",
            "loss: 0.533858  [31900/467875]\n",
            "loss: 0.526687  [34800/467875]\n",
            "loss: 0.544485  [37700/467875]\n",
            "loss: 0.594863  [40600/467875]\n",
            "loss: 0.498637  [43500/467875]\n",
            "loss: 0.600700  [46400/467875]\n",
            "loss: 0.530455  [49300/467875]\n",
            "loss: 0.507297  [52200/467875]\n",
            "loss: 0.549875  [55100/467875]\n",
            "loss: 0.555294  [58000/467875]\n",
            "loss: 0.487037  [60900/467875]\n",
            "loss: 0.541879  [63800/467875]\n",
            "loss: 0.538181  [66700/467875]\n",
            "loss: 0.442371  [69600/467875]\n",
            "loss: 0.499941  [72500/467875]\n",
            "loss: 0.644114  [75400/467875]\n",
            "loss: 0.417833  [78300/467875]\n",
            "loss: 0.517340  [81200/467875]\n",
            "loss: 0.452312  [84100/467875]\n",
            "loss: 0.547892  [87000/467875]\n",
            "loss: 0.455249  [89900/467875]\n",
            "loss: 0.479162  [92800/467875]\n",
            "loss: 0.480440  [95700/467875]\n",
            "loss: 0.504142  [98600/467875]\n",
            "loss: 0.397696  [101500/467875]\n",
            "loss: 0.557839  [104400/467875]\n",
            "loss: 0.435725  [107300/467875]\n",
            "loss: 0.483483  [110200/467875]\n",
            "loss: 0.444403  [113100/467875]\n",
            "loss: 0.603705  [116000/467875]\n",
            "loss: 0.498029  [118900/467875]\n",
            "loss: 0.522558  [121800/467875]\n",
            "loss: 0.695416  [124700/467875]\n",
            "loss: 0.474417  [127600/467875]\n",
            "loss: 0.531380  [130500/467875]\n",
            "loss: 0.482407  [133400/467875]\n",
            "loss: 0.546955  [136300/467875]\n",
            "loss: 0.588327  [139200/467875]\n",
            "loss: 0.499963  [142100/467875]\n",
            "loss: 0.528691  [145000/467875]\n",
            "loss: 0.451957  [147900/467875]\n",
            "loss: 0.587299  [150800/467875]\n",
            "loss: 0.587638  [153700/467875]\n",
            "loss: 0.442219  [156600/467875]\n",
            "loss: 0.556227  [159500/467875]\n",
            "loss: 0.561156  [162400/467875]\n",
            "loss: 0.560584  [165300/467875]\n",
            "loss: 0.426318  [168200/467875]\n",
            "loss: 0.496662  [171100/467875]\n",
            "loss: 0.354201  [174000/467875]\n",
            "loss: 0.516795  [176900/467875]\n",
            "loss: 0.610281  [179800/467875]\n",
            "loss: 0.485529  [182700/467875]\n",
            "loss: 0.507129  [185600/467875]\n",
            "loss: 0.567511  [188500/467875]\n",
            "loss: 0.462620  [191400/467875]\n",
            "loss: 0.477809  [194300/467875]\n",
            "loss: 0.468565  [197200/467875]\n",
            "loss: 0.588564  [200100/467875]\n",
            "loss: 0.582623  [203000/467875]\n",
            "loss: 0.540934  [205900/467875]\n",
            "loss: 0.498554  [208800/467875]\n",
            "loss: 0.496753  [211700/467875]\n",
            "loss: 0.501893  [214600/467875]\n",
            "loss: 0.472225  [217500/467875]\n",
            "loss: 0.580945  [220400/467875]\n",
            "loss: 0.484279  [223300/467875]\n",
            "loss: 0.510420  [226200/467875]\n",
            "loss: 0.503327  [229100/467875]\n",
            "loss: 0.464406  [232000/467875]\n",
            "loss: 0.592171  [234900/467875]\n",
            "loss: 0.546879  [237800/467875]\n",
            "loss: 0.456794  [240700/467875]\n",
            "loss: 0.542693  [243600/467875]\n",
            "loss: 0.468011  [246500/467875]\n",
            "loss: 0.462325  [249400/467875]\n",
            "loss: 0.538337  [252300/467875]\n",
            "loss: 0.491929  [255200/467875]\n",
            "loss: 0.516503  [258100/467875]\n",
            "loss: 0.663145  [261000/467875]\n",
            "loss: 0.580574  [263900/467875]\n",
            "loss: 0.459335  [266800/467875]\n",
            "loss: 0.557876  [269700/467875]\n",
            "loss: 0.522674  [272600/467875]\n",
            "loss: 0.577808  [275500/467875]\n",
            "loss: 0.589944  [278400/467875]\n",
            "loss: 0.659321  [281300/467875]\n",
            "loss: 0.516593  [284200/467875]\n",
            "loss: 0.475286  [287100/467875]\n",
            "loss: 0.457063  [290000/467875]\n",
            "loss: 0.584968  [292900/467875]\n",
            "loss: 0.481335  [295800/467875]\n",
            "loss: 0.468605  [298700/467875]\n",
            "loss: 0.532772  [301600/467875]\n",
            "loss: 0.521806  [304500/467875]\n",
            "loss: 0.482334  [307400/467875]\n",
            "loss: 0.550556  [310300/467875]\n",
            "loss: 0.516119  [313200/467875]\n",
            "loss: 0.462890  [316100/467875]\n",
            "loss: 0.465249  [319000/467875]\n",
            "loss: 0.552069  [321900/467875]\n",
            "loss: 0.640197  [324800/467875]\n",
            "loss: 0.454638  [327700/467875]\n",
            "loss: 0.580779  [330600/467875]\n",
            "loss: 0.480245  [333500/467875]\n",
            "loss: 0.454933  [336400/467875]\n",
            "loss: 0.411580  [339300/467875]\n",
            "loss: 0.573031  [342200/467875]\n",
            "loss: 0.522844  [345100/467875]\n",
            "loss: 0.487230  [348000/467875]\n",
            "loss: 0.516454  [350900/467875]\n",
            "loss: 0.480564  [353800/467875]\n",
            "loss: 0.565118  [356700/467875]\n",
            "loss: 0.666886  [359600/467875]\n",
            "loss: 0.504017  [362500/467875]\n",
            "loss: 0.593128  [365400/467875]\n",
            "loss: 0.550275  [368300/467875]\n",
            "loss: 0.471677  [371200/467875]\n",
            "loss: 0.510360  [374100/467875]\n",
            "loss: 0.560976  [377000/467875]\n",
            "loss: 0.577590  [379900/467875]\n",
            "loss: 0.632915  [382800/467875]\n",
            "loss: 0.510390  [385700/467875]\n",
            "loss: 0.510130  [388600/467875]\n",
            "loss: 0.554915  [391500/467875]\n",
            "loss: 0.471333  [394400/467875]\n",
            "loss: 0.562191  [397300/467875]\n",
            "loss: 0.395278  [400200/467875]\n",
            "loss: 0.633602  [403100/467875]\n",
            "loss: 0.573412  [406000/467875]\n",
            "loss: 0.524072  [408900/467875]\n",
            "loss: 0.481504  [411800/467875]\n",
            "loss: 0.522383  [414700/467875]\n",
            "loss: 0.420866  [417600/467875]\n",
            "loss: 0.481812  [420500/467875]\n",
            "loss: 0.567475  [423400/467875]\n",
            "loss: 0.491938  [426300/467875]\n",
            "loss: 0.518421  [429200/467875]\n",
            "loss: 0.578441  [432100/467875]\n",
            "loss: 0.543921  [435000/467875]\n",
            "loss: 0.343979  [437900/467875]\n",
            "loss: 0.619828  [440800/467875]\n",
            "loss: 0.526254  [443700/467875]\n",
            "loss: 0.641530  [446600/467875]\n",
            "loss: 0.422056  [449500/467875]\n",
            "loss: 0.513593  [452400/467875]\n",
            "loss: 0.566229  [455300/467875]\n",
            "loss: 0.572529  [458200/467875]\n",
            "loss: 0.484349  [461100/467875]\n",
            "loss: 0.401250  [464000/467875]\n",
            "loss: 0.519746  [466900/467875]\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522970 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 6 s\n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.492335  [    0/467875]\n",
            "loss: 0.526088  [ 2900/467875]\n",
            "loss: 0.416065  [ 5800/467875]\n",
            "loss: 0.532601  [ 8700/467875]\n",
            "loss: 0.541150  [11600/467875]\n",
            "loss: 0.531972  [14500/467875]\n",
            "loss: 0.554023  [17400/467875]\n",
            "loss: 0.435493  [20300/467875]\n",
            "loss: 0.573102  [23200/467875]\n",
            "loss: 0.504228  [26100/467875]\n",
            "loss: 0.398933  [29000/467875]\n",
            "loss: 0.545344  [31900/467875]\n",
            "loss: 0.627205  [34800/467875]\n",
            "loss: 0.553130  [37700/467875]\n",
            "loss: 0.590244  [40600/467875]\n",
            "loss: 0.480923  [43500/467875]\n",
            "loss: 0.470084  [46400/467875]\n",
            "loss: 0.426243  [49300/467875]\n",
            "loss: 0.574378  [52200/467875]\n",
            "loss: 0.593624  [55100/467875]\n",
            "loss: 0.523507  [58000/467875]\n",
            "loss: 0.590292  [60900/467875]\n",
            "loss: 0.511162  [63800/467875]\n",
            "loss: 0.639975  [66700/467875]\n",
            "loss: 0.507181  [69600/467875]\n",
            "loss: 0.513783  [72500/467875]\n",
            "loss: 0.485964  [75400/467875]\n",
            "loss: 0.535923  [78300/467875]\n",
            "loss: 0.530774  [81200/467875]\n",
            "loss: 0.477442  [84100/467875]\n",
            "loss: 0.545274  [87000/467875]\n",
            "loss: 0.450785  [89900/467875]\n",
            "loss: 0.573388  [92800/467875]\n",
            "loss: 0.441403  [95700/467875]\n",
            "loss: 0.581324  [98600/467875]\n",
            "loss: 0.435925  [101500/467875]\n",
            "loss: 0.548612  [104400/467875]\n",
            "loss: 0.400033  [107300/467875]\n",
            "loss: 0.520872  [110200/467875]\n",
            "loss: 0.588404  [113100/467875]\n",
            "loss: 0.424158  [116000/467875]\n",
            "loss: 0.465544  [118900/467875]\n",
            "loss: 0.536174  [121800/467875]\n",
            "loss: 0.462934  [124700/467875]\n",
            "loss: 0.488162  [127600/467875]\n",
            "loss: 0.528873  [130500/467875]\n",
            "loss: 0.523885  [133400/467875]\n",
            "loss: 0.482234  [136300/467875]\n",
            "loss: 0.514465  [139200/467875]\n",
            "loss: 0.576107  [142100/467875]\n",
            "loss: 0.454745  [145000/467875]\n",
            "loss: 0.456975  [147900/467875]\n",
            "loss: 0.540117  [150800/467875]\n",
            "loss: 0.488098  [153700/467875]\n",
            "loss: 0.477937  [156600/467875]\n",
            "loss: 0.451573  [159500/467875]\n",
            "loss: 0.468505  [162400/467875]\n",
            "loss: 0.567783  [165300/467875]\n",
            "loss: 0.442117  [168200/467875]\n",
            "loss: 0.601107  [171100/467875]\n",
            "loss: 0.532589  [174000/467875]\n",
            "loss: 0.385171  [176900/467875]\n",
            "loss: 0.513449  [179800/467875]\n",
            "loss: 0.491879  [182700/467875]\n",
            "loss: 0.584631  [185600/467875]\n",
            "loss: 0.565315  [188500/467875]\n",
            "loss: 0.556664  [191400/467875]\n",
            "loss: 0.492303  [194300/467875]\n",
            "loss: 0.530663  [197200/467875]\n",
            "loss: 0.493039  [200100/467875]\n",
            "loss: 0.592215  [203000/467875]\n",
            "loss: 0.505484  [205900/467875]\n",
            "loss: 0.566121  [208800/467875]\n",
            "loss: 0.559042  [211700/467875]\n",
            "loss: 0.421206  [214600/467875]\n",
            "loss: 0.423879  [217500/467875]\n",
            "loss: 0.451500  [220400/467875]\n",
            "loss: 0.497834  [223300/467875]\n",
            "loss: 0.503562  [226200/467875]\n",
            "loss: 0.531899  [229100/467875]\n",
            "loss: 0.620595  [232000/467875]\n",
            "loss: 0.522296  [234900/467875]\n",
            "loss: 0.590647  [237800/467875]\n",
            "loss: 0.441810  [240700/467875]\n",
            "loss: 0.535971  [243600/467875]\n",
            "loss: 0.482538  [246500/467875]\n",
            "loss: 0.388614  [249400/467875]\n",
            "loss: 0.450993  [252300/467875]\n",
            "loss: 0.532453  [255200/467875]\n",
            "loss: 0.488414  [258100/467875]\n",
            "loss: 0.577340  [261000/467875]\n",
            "loss: 0.561344  [263900/467875]\n",
            "loss: 0.585619  [266800/467875]\n",
            "loss: 0.390202  [269700/467875]\n",
            "loss: 0.367481  [272600/467875]\n",
            "loss: 0.457622  [275500/467875]\n",
            "loss: 0.471848  [278400/467875]\n",
            "loss: 0.486478  [281300/467875]\n",
            "loss: 0.512552  [284200/467875]\n",
            "loss: 0.662516  [287100/467875]\n",
            "loss: 0.542705  [290000/467875]\n",
            "loss: 0.536368  [292900/467875]\n",
            "loss: 0.487367  [295800/467875]\n",
            "loss: 0.518874  [298700/467875]\n",
            "loss: 0.526531  [301600/467875]\n",
            "loss: 0.558231  [304500/467875]\n",
            "loss: 0.444384  [307400/467875]\n",
            "loss: 0.550941  [310300/467875]\n",
            "loss: 0.497675  [313200/467875]\n",
            "loss: 0.529968  [316100/467875]\n",
            "loss: 0.591812  [319000/467875]\n",
            "loss: 0.496028  [321900/467875]\n",
            "loss: 0.573727  [324800/467875]\n",
            "loss: 0.552671  [327700/467875]\n",
            "loss: 0.616956  [330600/467875]\n",
            "loss: 0.597869  [333500/467875]\n",
            "loss: 0.459372  [336400/467875]\n",
            "loss: 0.468026  [339300/467875]\n",
            "loss: 0.522109  [342200/467875]\n",
            "loss: 0.628160  [345100/467875]\n",
            "loss: 0.475721  [348000/467875]\n",
            "loss: 0.509032  [350900/467875]\n",
            "loss: 0.485992  [353800/467875]\n",
            "loss: 0.573924  [356700/467875]\n",
            "loss: 0.402752  [359600/467875]\n",
            "loss: 0.562899  [362500/467875]\n",
            "loss: 0.631496  [365400/467875]\n",
            "loss: 0.506281  [368300/467875]\n",
            "loss: 0.527778  [371200/467875]\n",
            "loss: 0.449428  [374100/467875]\n",
            "loss: 0.512053  [377000/467875]\n",
            "loss: 0.504404  [379900/467875]\n",
            "loss: 0.529977  [382800/467875]\n",
            "loss: 0.557868  [385700/467875]\n",
            "loss: 0.389748  [388600/467875]\n",
            "loss: 0.590538  [391500/467875]\n",
            "loss: 0.464218  [394400/467875]\n",
            "loss: 0.440587  [397300/467875]\n",
            "loss: 0.572446  [400200/467875]\n",
            "loss: 0.626773  [403100/467875]\n",
            "loss: 0.500109  [406000/467875]\n",
            "loss: 0.573373  [408900/467875]\n",
            "loss: 0.526949  [411800/467875]\n",
            "loss: 0.453398  [414700/467875]\n",
            "loss: 0.614379  [417600/467875]\n",
            "loss: 0.508228  [420500/467875]\n",
            "loss: 0.488720  [423400/467875]\n",
            "loss: 0.574689  [426300/467875]\n",
            "loss: 0.581811  [429200/467875]\n",
            "loss: 0.499257  [432100/467875]\n",
            "loss: 0.515090  [435000/467875]\n",
            "loss: 0.528796  [437900/467875]\n",
            "loss: 0.526127  [440800/467875]\n",
            "loss: 0.569514  [443700/467875]\n",
            "loss: 0.583597  [446600/467875]\n",
            "loss: 0.559830  [449500/467875]\n",
            "loss: 0.622121  [452400/467875]\n",
            "loss: 0.403070  [455300/467875]\n",
            "loss: 0.541971  [458200/467875]\n",
            "loss: 0.594269  [461100/467875]\n",
            "loss: 0.501085  [464000/467875]\n",
            "loss: 0.483192  [466900/467875]\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522834 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 8 s\n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.411496  [    0/467875]\n",
            "loss: 0.537207  [ 2900/467875]\n",
            "loss: 0.539568  [ 5800/467875]\n",
            "loss: 0.531093  [ 8700/467875]\n",
            "loss: 0.522117  [11600/467875]\n",
            "loss: 0.420035  [14500/467875]\n",
            "loss: 0.471343  [17400/467875]\n",
            "loss: 0.501854  [20300/467875]\n",
            "loss: 0.558024  [23200/467875]\n",
            "loss: 0.524283  [26100/467875]\n",
            "loss: 0.430738  [29000/467875]\n",
            "loss: 0.443786  [31900/467875]\n",
            "loss: 0.535260  [34800/467875]\n",
            "loss: 0.503380  [37700/467875]\n",
            "loss: 0.511149  [40600/467875]\n",
            "loss: 0.605932  [43500/467875]\n",
            "loss: 0.473371  [46400/467875]\n",
            "loss: 0.551649  [49300/467875]\n",
            "loss: 0.424113  [52200/467875]\n",
            "loss: 0.548282  [55100/467875]\n",
            "loss: 0.444857  [58000/467875]\n",
            "loss: 0.383802  [60900/467875]\n",
            "loss: 0.534515  [63800/467875]\n",
            "loss: 0.516312  [66700/467875]\n",
            "loss: 0.546351  [69600/467875]\n",
            "loss: 0.526360  [72500/467875]\n",
            "loss: 0.503396  [75400/467875]\n",
            "loss: 0.555690  [78300/467875]\n",
            "loss: 0.395750  [81200/467875]\n",
            "loss: 0.495727  [84100/467875]\n",
            "loss: 0.528182  [87000/467875]\n",
            "loss: 0.621705  [89900/467875]\n",
            "loss: 0.458621  [92800/467875]\n",
            "loss: 0.582999  [95700/467875]\n",
            "loss: 0.456690  [98600/467875]\n",
            "loss: 0.475435  [101500/467875]\n",
            "loss: 0.598505  [104400/467875]\n",
            "loss: 0.620583  [107300/467875]\n",
            "loss: 0.541036  [110200/467875]\n",
            "loss: 0.490038  [113100/467875]\n",
            "loss: 0.653359  [116000/467875]\n",
            "loss: 0.502060  [118900/467875]\n",
            "loss: 0.458332  [121800/467875]\n",
            "loss: 0.617297  [124700/467875]\n",
            "loss: 0.532405  [127600/467875]\n",
            "loss: 0.654933  [130500/467875]\n",
            "loss: 0.585031  [133400/467875]\n",
            "loss: 0.431521  [136300/467875]\n",
            "loss: 0.525769  [139200/467875]\n",
            "loss: 0.445674  [142100/467875]\n",
            "loss: 0.477333  [145000/467875]\n",
            "loss: 0.496907  [147900/467875]\n",
            "loss: 0.412538  [150800/467875]\n",
            "loss: 0.474451  [153700/467875]\n",
            "loss: 0.492738  [156600/467875]\n",
            "loss: 0.476555  [159500/467875]\n",
            "loss: 0.472368  [162400/467875]\n",
            "loss: 0.547284  [165300/467875]\n",
            "loss: 0.511466  [168200/467875]\n",
            "loss: 0.479321  [171100/467875]\n",
            "loss: 0.527203  [174000/467875]\n",
            "loss: 0.475912  [176900/467875]\n",
            "loss: 0.408671  [179800/467875]\n",
            "loss: 0.540023  [182700/467875]\n",
            "loss: 0.531096  [185600/467875]\n",
            "loss: 0.482196  [188500/467875]\n",
            "loss: 0.533529  [191400/467875]\n",
            "loss: 0.508964  [194300/467875]\n",
            "loss: 0.450270  [197200/467875]\n",
            "loss: 0.502810  [200100/467875]\n",
            "loss: 0.632906  [203000/467875]\n",
            "loss: 0.568709  [205900/467875]\n",
            "loss: 0.589413  [208800/467875]\n",
            "loss: 0.451882  [211700/467875]\n",
            "loss: 0.610563  [214600/467875]\n",
            "loss: 0.482553  [217500/467875]\n",
            "loss: 0.531790  [220400/467875]\n",
            "loss: 0.469311  [223300/467875]\n",
            "loss: 0.521175  [226200/467875]\n",
            "loss: 0.485865  [229100/467875]\n",
            "loss: 0.445697  [232000/467875]\n",
            "loss: 0.460610  [234900/467875]\n",
            "loss: 0.577472  [237800/467875]\n",
            "loss: 0.590681  [240700/467875]\n",
            "loss: 0.498727  [243600/467875]\n",
            "loss: 0.560888  [246500/467875]\n",
            "loss: 0.550855  [249400/467875]\n",
            "loss: 0.580215  [252300/467875]\n",
            "loss: 0.620284  [255200/467875]\n",
            "loss: 0.624078  [258100/467875]\n",
            "loss: 0.430412  [261000/467875]\n",
            "loss: 0.464904  [263900/467875]\n",
            "loss: 0.502409  [266800/467875]\n",
            "loss: 0.505860  [269700/467875]\n",
            "loss: 0.554255  [272600/467875]\n",
            "loss: 0.528691  [275500/467875]\n",
            "loss: 0.609797  [278400/467875]\n",
            "loss: 0.545767  [281300/467875]\n",
            "loss: 0.675508  [284200/467875]\n",
            "loss: 0.610255  [287100/467875]\n",
            "loss: 0.644158  [290000/467875]\n",
            "loss: 0.496032  [292900/467875]\n",
            "loss: 0.558845  [295800/467875]\n",
            "loss: 0.636380  [298700/467875]\n",
            "loss: 0.563641  [301600/467875]\n",
            "loss: 0.519346  [304500/467875]\n",
            "loss: 0.497256  [307400/467875]\n",
            "loss: 0.487672  [310300/467875]\n",
            "loss: 0.440942  [313200/467875]\n",
            "loss: 0.585635  [316100/467875]\n",
            "loss: 0.422518  [319000/467875]\n",
            "loss: 0.560119  [321900/467875]\n",
            "loss: 0.508600  [324800/467875]\n",
            "loss: 0.395934  [327700/467875]\n",
            "loss: 0.511736  [330600/467875]\n",
            "loss: 0.469676  [333500/467875]\n",
            "loss: 0.480382  [336400/467875]\n",
            "loss: 0.479110  [339300/467875]\n",
            "loss: 0.550215  [342200/467875]\n",
            "loss: 0.442039  [345100/467875]\n",
            "loss: 0.591602  [348000/467875]\n",
            "loss: 0.482949  [350900/467875]\n",
            "loss: 0.557105  [353800/467875]\n",
            "loss: 0.512007  [356700/467875]\n",
            "loss: 0.584271  [359600/467875]\n",
            "loss: 0.464847  [362500/467875]\n",
            "loss: 0.510028  [365400/467875]\n",
            "loss: 0.475385  [368300/467875]\n",
            "loss: 0.509328  [371200/467875]\n",
            "loss: 0.475497  [374100/467875]\n",
            "loss: 0.535496  [377000/467875]\n",
            "loss: 0.584234  [379900/467875]\n",
            "loss: 0.645948  [382800/467875]\n",
            "loss: 0.416271  [385700/467875]\n",
            "loss: 0.442355  [388600/467875]\n",
            "loss: 0.483118  [391500/467875]\n",
            "loss: 0.440306  [394400/467875]\n",
            "loss: 0.485336  [397300/467875]\n",
            "loss: 0.660396  [400200/467875]\n",
            "loss: 0.454955  [403100/467875]\n",
            "loss: 0.586590  [406000/467875]\n",
            "loss: 0.518468  [408900/467875]\n",
            "loss: 0.486200  [411800/467875]\n",
            "loss: 0.548789  [414700/467875]\n",
            "loss: 0.466423  [417600/467875]\n",
            "loss: 0.534556  [420500/467875]\n",
            "loss: 0.632752  [423400/467875]\n",
            "loss: 0.442509  [426300/467875]\n",
            "loss: 0.454380  [429200/467875]\n",
            "loss: 0.531933  [432100/467875]\n",
            "loss: 0.557173  [435000/467875]\n",
            "loss: 0.375903  [437900/467875]\n",
            "loss: 0.501874  [440800/467875]\n",
            "loss: 0.513159  [443700/467875]\n",
            "loss: 0.684473  [446600/467875]\n",
            "loss: 0.533447  [449500/467875]\n",
            "loss: 0.578192  [452400/467875]\n",
            "loss: 0.521223  [455300/467875]\n",
            "loss: 0.540024  [458200/467875]\n",
            "loss: 0.471245  [461100/467875]\n",
            "loss: 0.553868  [464000/467875]\n",
            "loss: 0.498674  [466900/467875]\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.522388 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 8 s\n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.429060  [    0/467875]\n",
            "loss: 0.592493  [ 2900/467875]\n",
            "loss: 0.399696  [ 5800/467875]\n",
            "loss: 0.506839  [ 8700/467875]\n",
            "loss: 0.578923  [11600/467875]\n",
            "loss: 0.533217  [14500/467875]\n",
            "loss: 0.527173  [17400/467875]\n",
            "loss: 0.441466  [20300/467875]\n",
            "loss: 0.454739  [23200/467875]\n",
            "loss: 0.420244  [26100/467875]\n",
            "loss: 0.450876  [29000/467875]\n",
            "loss: 0.524500  [31900/467875]\n",
            "loss: 0.441964  [34800/467875]\n",
            "loss: 0.508143  [37700/467875]\n",
            "loss: 0.509042  [40600/467875]\n",
            "loss: 0.497708  [43500/467875]\n",
            "loss: 0.478796  [46400/467875]\n",
            "loss: 0.609421  [49300/467875]\n",
            "loss: 0.457004  [52200/467875]\n",
            "loss: 0.620111  [55100/467875]\n",
            "loss: 0.583802  [58000/467875]\n",
            "loss: 0.454257  [60900/467875]\n",
            "loss: 0.470987  [63800/467875]\n",
            "loss: 0.496193  [66700/467875]\n",
            "loss: 0.434091  [69600/467875]\n",
            "loss: 0.609734  [72500/467875]\n",
            "loss: 0.533533  [75400/467875]\n",
            "loss: 0.405046  [78300/467875]\n",
            "loss: 0.631771  [81200/467875]\n",
            "loss: 0.490712  [84100/467875]\n",
            "loss: 0.570414  [87000/467875]\n",
            "loss: 0.547517  [89900/467875]\n",
            "loss: 0.593811  [92800/467875]\n",
            "loss: 0.464526  [95700/467875]\n",
            "loss: 0.573014  [98600/467875]\n",
            "loss: 0.486362  [101500/467875]\n",
            "loss: 0.482924  [104400/467875]\n",
            "loss: 0.488123  [107300/467875]\n",
            "loss: 0.478292  [110200/467875]\n",
            "loss: 0.590747  [113100/467875]\n",
            "loss: 0.582514  [116000/467875]\n",
            "loss: 0.587775  [118900/467875]\n",
            "loss: 0.503792  [121800/467875]\n",
            "loss: 0.531307  [124700/467875]\n",
            "loss: 0.565866  [127600/467875]\n",
            "loss: 0.563885  [130500/467875]\n",
            "loss: 0.657870  [133400/467875]\n",
            "loss: 0.473744  [136300/467875]\n",
            "loss: 0.499297  [139200/467875]\n",
            "loss: 0.542659  [142100/467875]\n",
            "loss: 0.485686  [145000/467875]\n",
            "loss: 0.626563  [147900/467875]\n",
            "loss: 0.510608  [150800/467875]\n",
            "loss: 0.474119  [153700/467875]\n",
            "loss: 0.714395  [156600/467875]\n",
            "loss: 0.532458  [159500/467875]\n",
            "loss: 0.572761  [162400/467875]\n",
            "loss: 0.521756  [165300/467875]\n",
            "loss: 0.484611  [168200/467875]\n",
            "loss: 0.461181  [171100/467875]\n",
            "loss: 0.457245  [174000/467875]\n",
            "loss: 0.652819  [176900/467875]\n",
            "loss: 0.552458  [179800/467875]\n",
            "loss: 0.552772  [182700/467875]\n",
            "loss: 0.513855  [185600/467875]\n",
            "loss: 0.553577  [188500/467875]\n",
            "loss: 0.533317  [191400/467875]\n",
            "loss: 0.366951  [194300/467875]\n",
            "loss: 0.469280  [197200/467875]\n",
            "loss: 0.486891  [200100/467875]\n",
            "loss: 0.489741  [203000/467875]\n",
            "loss: 0.590495  [205900/467875]\n",
            "loss: 0.464697  [208800/467875]\n",
            "loss: 0.518950  [211700/467875]\n",
            "loss: 0.520606  [214600/467875]\n",
            "loss: 0.464943  [217500/467875]\n",
            "loss: 0.540515  [220400/467875]\n",
            "loss: 0.634633  [223300/467875]\n",
            "loss: 0.565085  [226200/467875]\n",
            "loss: 0.531085  [229100/467875]\n",
            "loss: 0.558147  [232000/467875]\n",
            "loss: 0.525734  [234900/467875]\n",
            "loss: 0.541858  [237800/467875]\n",
            "loss: 0.567067  [240700/467875]\n",
            "loss: 0.499474  [243600/467875]\n",
            "loss: 0.429943  [246500/467875]\n",
            "loss: 0.504731  [249400/467875]\n",
            "loss: 0.426212  [252300/467875]\n",
            "loss: 0.465947  [255200/467875]\n",
            "loss: 0.559269  [258100/467875]\n",
            "loss: 0.353679  [261000/467875]\n",
            "loss: 0.505581  [263900/467875]\n",
            "loss: 0.513291  [266800/467875]\n",
            "loss: 0.656345  [269700/467875]\n",
            "loss: 0.565871  [272600/467875]\n",
            "loss: 0.574635  [275500/467875]\n",
            "loss: 0.502448  [278400/467875]\n",
            "loss: 0.456571  [281300/467875]\n",
            "loss: 0.564745  [284200/467875]\n",
            "loss: 0.552824  [287100/467875]\n",
            "loss: 0.449989  [290000/467875]\n",
            "loss: 0.433505  [292900/467875]\n",
            "loss: 0.614643  [295800/467875]\n",
            "loss: 0.650137  [298700/467875]\n",
            "loss: 0.452565  [301600/467875]\n",
            "loss: 0.455648  [304500/467875]\n",
            "loss: 0.527136  [307400/467875]\n",
            "loss: 0.524175  [310300/467875]\n",
            "loss: 0.455446  [313200/467875]\n",
            "loss: 0.493330  [316100/467875]\n",
            "loss: 0.560426  [319000/467875]\n",
            "loss: 0.424516  [321900/467875]\n",
            "loss: 0.614106  [324800/467875]\n",
            "loss: 0.514194  [327700/467875]\n",
            "loss: 0.625169  [330600/467875]\n",
            "loss: 0.577046  [333500/467875]\n",
            "loss: 0.522847  [336400/467875]\n",
            "loss: 0.591747  [339300/467875]\n",
            "loss: 0.473049  [342200/467875]\n",
            "loss: 0.576932  [345100/467875]\n",
            "loss: 0.430243  [348000/467875]\n",
            "loss: 0.526578  [350900/467875]\n",
            "loss: 0.458607  [353800/467875]\n",
            "loss: 0.474970  [356700/467875]\n",
            "loss: 0.478627  [359600/467875]\n",
            "loss: 0.447706  [362500/467875]\n",
            "loss: 0.383092  [365400/467875]\n",
            "loss: 0.423131  [368300/467875]\n",
            "loss: 0.524609  [371200/467875]\n",
            "loss: 0.490201  [374100/467875]\n",
            "loss: 0.526263  [377000/467875]\n",
            "loss: 0.475321  [379900/467875]\n",
            "loss: 0.461348  [382800/467875]\n",
            "loss: 0.559924  [385700/467875]\n",
            "loss: 0.509438  [388600/467875]\n",
            "loss: 0.440590  [391500/467875]\n",
            "loss: 0.491297  [394400/467875]\n",
            "loss: 0.595192  [397300/467875]\n",
            "loss: 0.613860  [400200/467875]\n",
            "loss: 0.446328  [403100/467875]\n",
            "loss: 0.425202  [406000/467875]\n",
            "loss: 0.635313  [408900/467875]\n",
            "loss: 0.649715  [411800/467875]\n",
            "loss: 0.549637  [414700/467875]\n",
            "loss: 0.528668  [417600/467875]\n",
            "loss: 0.415222  [420500/467875]\n",
            "loss: 0.560079  [423400/467875]\n",
            "loss: 0.490438  [426300/467875]\n",
            "loss: 0.505738  [429200/467875]\n",
            "loss: 0.589281  [432100/467875]\n",
            "loss: 0.626109  [435000/467875]\n",
            "loss: 0.546100  [437900/467875]\n",
            "loss: 0.584989  [440800/467875]\n",
            "loss: 0.515739  [443700/467875]\n",
            "loss: 0.552966  [446600/467875]\n",
            "loss: 0.496288  [449500/467875]\n",
            "loss: 0.502701  [452400/467875]\n",
            "loss: 0.548623  [455300/467875]\n",
            "loss: 0.537892  [458200/467875]\n",
            "loss: 0.555572  [461100/467875]\n",
            "loss: 0.476192  [464000/467875]\n",
            "loss: 0.460128  [466900/467875]\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.523680 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 6 s\n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.627096  [    0/467875]\n",
            "loss: 0.522880  [ 2900/467875]\n",
            "loss: 0.557144  [ 5800/467875]\n",
            "loss: 0.671368  [ 8700/467875]\n",
            "loss: 0.611358  [11600/467875]\n",
            "loss: 0.539652  [14500/467875]\n",
            "loss: 0.550338  [17400/467875]\n",
            "loss: 0.477818  [20300/467875]\n",
            "loss: 0.538850  [23200/467875]\n",
            "loss: 0.461601  [26100/467875]\n",
            "loss: 0.475728  [29000/467875]\n",
            "loss: 0.523544  [31900/467875]\n",
            "loss: 0.446252  [34800/467875]\n",
            "loss: 0.577255  [37700/467875]\n",
            "loss: 0.481031  [40600/467875]\n",
            "loss: 0.559220  [43500/467875]\n",
            "loss: 0.581125  [46400/467875]\n",
            "loss: 0.485682  [49300/467875]\n",
            "loss: 0.572368  [52200/467875]\n",
            "loss: 0.404893  [55100/467875]\n",
            "loss: 0.460490  [58000/467875]\n",
            "loss: 0.518442  [60900/467875]\n",
            "loss: 0.511306  [63800/467875]\n",
            "loss: 0.534983  [66700/467875]\n",
            "loss: 0.458074  [69600/467875]\n",
            "loss: 0.498545  [72500/467875]\n",
            "loss: 0.567097  [75400/467875]\n",
            "loss: 0.480181  [78300/467875]\n",
            "loss: 0.562620  [81200/467875]\n",
            "loss: 0.552934  [84100/467875]\n",
            "loss: 0.426973  [87000/467875]\n",
            "loss: 0.510690  [89900/467875]\n",
            "loss: 0.555973  [92800/467875]\n",
            "loss: 0.536675  [95700/467875]\n",
            "loss: 0.590627  [98600/467875]\n",
            "loss: 0.553552  [101500/467875]\n",
            "loss: 0.563889  [104400/467875]\n",
            "loss: 0.434031  [107300/467875]\n",
            "loss: 0.529993  [110200/467875]\n",
            "loss: 0.473056  [113100/467875]\n",
            "loss: 0.513556  [116000/467875]\n",
            "loss: 0.559843  [118900/467875]\n",
            "loss: 0.501015  [121800/467875]\n",
            "loss: 0.445845  [124700/467875]\n",
            "loss: 0.547716  [127600/467875]\n",
            "loss: 0.636087  [130500/467875]\n",
            "loss: 0.569623  [133400/467875]\n",
            "loss: 0.621289  [136300/467875]\n",
            "loss: 0.593063  [139200/467875]\n",
            "loss: 0.484428  [142100/467875]\n",
            "loss: 0.487061  [145000/467875]\n",
            "loss: 0.429533  [147900/467875]\n",
            "loss: 0.523889  [150800/467875]\n",
            "loss: 0.423811  [153700/467875]\n",
            "loss: 0.463318  [156600/467875]\n",
            "loss: 0.454993  [159500/467875]\n",
            "loss: 0.631741  [162400/467875]\n",
            "loss: 0.567091  [165300/467875]\n",
            "loss: 0.474075  [168200/467875]\n",
            "loss: 0.503945  [171100/467875]\n",
            "loss: 0.561578  [174000/467875]\n",
            "loss: 0.459335  [176900/467875]\n",
            "loss: 0.506862  [179800/467875]\n",
            "loss: 0.527093  [182700/467875]\n",
            "loss: 0.654355  [185600/467875]\n",
            "loss: 0.554563  [188500/467875]\n",
            "loss: 0.489998  [191400/467875]\n",
            "loss: 0.560744  [194300/467875]\n",
            "loss: 0.493805  [197200/467875]\n",
            "loss: 0.395882  [200100/467875]\n",
            "loss: 0.684962  [203000/467875]\n",
            "loss: 0.482377  [205900/467875]\n",
            "loss: 0.522134  [208800/467875]\n",
            "loss: 0.594148  [211700/467875]\n",
            "loss: 0.529690  [214600/467875]\n",
            "loss: 0.469581  [217500/467875]\n",
            "loss: 0.557933  [220400/467875]\n",
            "loss: 0.636363  [223300/467875]\n",
            "loss: 0.468733  [226200/467875]\n",
            "loss: 0.532418  [229100/467875]\n",
            "loss: 0.539056  [232000/467875]\n",
            "loss: 0.587517  [234900/467875]\n",
            "loss: 0.552856  [237800/467875]\n",
            "loss: 0.587609  [240700/467875]\n",
            "loss: 0.522275  [243600/467875]\n",
            "loss: 0.449984  [246500/467875]\n",
            "loss: 0.437373  [249400/467875]\n",
            "loss: 0.444335  [252300/467875]\n",
            "loss: 0.537317  [255200/467875]\n",
            "loss: 0.527229  [258100/467875]\n",
            "loss: 0.521085  [261000/467875]\n",
            "loss: 0.411310  [263900/467875]\n",
            "loss: 0.529030  [266800/467875]\n",
            "loss: 0.583087  [269700/467875]\n",
            "loss: 0.453348  [272600/467875]\n",
            "loss: 0.529747  [275500/467875]\n",
            "loss: 0.593026  [278400/467875]\n",
            "loss: 0.502890  [281300/467875]\n",
            "loss: 0.499233  [284200/467875]\n",
            "loss: 0.640520  [287100/467875]\n",
            "loss: 0.498331  [290000/467875]\n",
            "loss: 0.474973  [292900/467875]\n",
            "loss: 0.470146  [295800/467875]\n",
            "loss: 0.621143  [298700/467875]\n",
            "loss: 0.436136  [301600/467875]\n",
            "loss: 0.565352  [304500/467875]\n",
            "loss: 0.484691  [307400/467875]\n",
            "loss: 0.474108  [310300/467875]\n",
            "loss: 0.632994  [313200/467875]\n",
            "loss: 0.447698  [316100/467875]\n",
            "loss: 0.470758  [319000/467875]\n",
            "loss: 0.427741  [321900/467875]\n",
            "loss: 0.536956  [324800/467875]\n",
            "loss: 0.578024  [327700/467875]\n",
            "loss: 0.468895  [330600/467875]\n",
            "loss: 0.481549  [333500/467875]\n",
            "loss: 0.568407  [336400/467875]\n",
            "loss: 0.570467  [339300/467875]\n",
            "loss: 0.510490  [342200/467875]\n",
            "loss: 0.544708  [345100/467875]\n",
            "loss: 0.514428  [348000/467875]\n",
            "loss: 0.424882  [350900/467875]\n",
            "loss: 0.498192  [353800/467875]\n",
            "loss: 0.397788  [356700/467875]\n",
            "loss: 0.534605  [359600/467875]\n",
            "loss: 0.617539  [362500/467875]\n",
            "loss: 0.590319  [365400/467875]\n",
            "loss: 0.553792  [368300/467875]\n",
            "loss: 0.486832  [371200/467875]\n",
            "loss: 0.599308  [374100/467875]\n",
            "loss: 0.502343  [377000/467875]\n",
            "loss: 0.524758  [379900/467875]\n",
            "loss: 0.485591  [382800/467875]\n",
            "loss: 0.558072  [385700/467875]\n",
            "loss: 0.523799  [388600/467875]\n",
            "loss: 0.423174  [391500/467875]\n",
            "loss: 0.454293  [394400/467875]\n",
            "loss: 0.549631  [397300/467875]\n",
            "loss: 0.568749  [400200/467875]\n",
            "loss: 0.540168  [403100/467875]\n",
            "loss: 0.529344  [406000/467875]\n",
            "loss: 0.478484  [408900/467875]\n",
            "loss: 0.492114  [411800/467875]\n",
            "loss: 0.492472  [414700/467875]\n",
            "loss: 0.580666  [417600/467875]\n",
            "loss: 0.634546  [420500/467875]\n",
            "loss: 0.433784  [423400/467875]\n",
            "loss: 0.510882  [426300/467875]\n",
            "loss: 0.573054  [429200/467875]\n",
            "loss: 0.558993  [432100/467875]\n",
            "loss: 0.510703  [435000/467875]\n",
            "loss: 0.450149  [437900/467875]\n",
            "loss: 0.501234  [440800/467875]\n",
            "loss: 0.600189  [443700/467875]\n",
            "loss: 0.604743  [446600/467875]\n",
            "loss: 0.464104  [449500/467875]\n",
            "loss: 0.682154  [452400/467875]\n",
            "loss: 0.455353  [455300/467875]\n",
            "loss: 0.514515  [458200/467875]\n",
            "loss: 0.407158  [461100/467875]\n",
            "loss: 0.474216  [464000/467875]\n",
            "loss: 0.527112  [466900/467875]\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522639 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 6 s\n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.449810  [    0/467875]\n",
            "loss: 0.627318  [ 2900/467875]\n",
            "loss: 0.524258  [ 5800/467875]\n",
            "loss: 0.487128  [ 8700/467875]\n",
            "loss: 0.463294  [11600/467875]\n",
            "loss: 0.501588  [14500/467875]\n",
            "loss: 0.640918  [17400/467875]\n",
            "loss: 0.483100  [20300/467875]\n",
            "loss: 0.564484  [23200/467875]\n",
            "loss: 0.471641  [26100/467875]\n",
            "loss: 0.476003  [29000/467875]\n",
            "loss: 0.487674  [31900/467875]\n",
            "loss: 0.500297  [34800/467875]\n",
            "loss: 0.405812  [37700/467875]\n",
            "loss: 0.626116  [40600/467875]\n",
            "loss: 0.562984  [43500/467875]\n",
            "loss: 0.456989  [46400/467875]\n",
            "loss: 0.585516  [49300/467875]\n",
            "loss: 0.466128  [52200/467875]\n",
            "loss: 0.605697  [55100/467875]\n",
            "loss: 0.553740  [58000/467875]\n",
            "loss: 0.492176  [60900/467875]\n",
            "loss: 0.543281  [63800/467875]\n",
            "loss: 0.545718  [66700/467875]\n",
            "loss: 0.507827  [69600/467875]\n",
            "loss: 0.528847  [72500/467875]\n",
            "loss: 0.530702  [75400/467875]\n",
            "loss: 0.554599  [78300/467875]\n",
            "loss: 0.588257  [81200/467875]\n",
            "loss: 0.622956  [84100/467875]\n",
            "loss: 0.501218  [87000/467875]\n",
            "loss: 0.575220  [89900/467875]\n",
            "loss: 0.556586  [92800/467875]\n",
            "loss: 0.583305  [95700/467875]\n",
            "loss: 0.457346  [98600/467875]\n",
            "loss: 0.559008  [101500/467875]\n",
            "loss: 0.699293  [104400/467875]\n",
            "loss: 0.448822  [107300/467875]\n",
            "loss: 0.492808  [110200/467875]\n",
            "loss: 0.468794  [113100/467875]\n",
            "loss: 0.536900  [116000/467875]\n",
            "loss: 0.696583  [118900/467875]\n",
            "loss: 0.499801  [121800/467875]\n",
            "loss: 0.540514  [124700/467875]\n",
            "loss: 0.405794  [127600/467875]\n",
            "loss: 0.538337  [130500/467875]\n",
            "loss: 0.527434  [133400/467875]\n",
            "loss: 0.416433  [136300/467875]\n",
            "loss: 0.566843  [139200/467875]\n",
            "loss: 0.569417  [142100/467875]\n",
            "loss: 0.529097  [145000/467875]\n",
            "loss: 0.491996  [147900/467875]\n",
            "loss: 0.488442  [150800/467875]\n",
            "loss: 0.477791  [153700/467875]\n",
            "loss: 0.415507  [156600/467875]\n",
            "loss: 0.559919  [159500/467875]\n",
            "loss: 0.568267  [162400/467875]\n",
            "loss: 0.610864  [165300/467875]\n",
            "loss: 0.568672  [168200/467875]\n",
            "loss: 0.595828  [171100/467875]\n",
            "loss: 0.519784  [174000/467875]\n",
            "loss: 0.519666  [176900/467875]\n",
            "loss: 0.651843  [179800/467875]\n",
            "loss: 0.478855  [182700/467875]\n",
            "loss: 0.551833  [185600/467875]\n",
            "loss: 0.436082  [188500/467875]\n",
            "loss: 0.529317  [191400/467875]\n",
            "loss: 0.666954  [194300/467875]\n",
            "loss: 0.471686  [197200/467875]\n",
            "loss: 0.564930  [200100/467875]\n",
            "loss: 0.506698  [203000/467875]\n",
            "loss: 0.384864  [205900/467875]\n",
            "loss: 0.458011  [208800/467875]\n",
            "loss: 0.493699  [211700/467875]\n",
            "loss: 0.543222  [214600/467875]\n",
            "loss: 0.516797  [217500/467875]\n",
            "loss: 0.486763  [220400/467875]\n",
            "loss: 0.406371  [223300/467875]\n",
            "loss: 0.465944  [226200/467875]\n",
            "loss: 0.473857  [229100/467875]\n",
            "loss: 0.447106  [232000/467875]\n",
            "loss: 0.443026  [234900/467875]\n",
            "loss: 0.521621  [237800/467875]\n",
            "loss: 0.601835  [240700/467875]\n",
            "loss: 0.489617  [243600/467875]\n",
            "loss: 0.443964  [246500/467875]\n",
            "loss: 0.450160  [249400/467875]\n",
            "loss: 0.523006  [252300/467875]\n",
            "loss: 0.491007  [255200/467875]\n",
            "loss: 0.512739  [258100/467875]\n",
            "loss: 0.620415  [261000/467875]\n",
            "loss: 0.555219  [263900/467875]\n",
            "loss: 0.464402  [266800/467875]\n",
            "loss: 0.532701  [269700/467875]\n",
            "loss: 0.605793  [272600/467875]\n",
            "loss: 0.507519  [275500/467875]\n",
            "loss: 0.586231  [278400/467875]\n",
            "loss: 0.488992  [281300/467875]\n",
            "loss: 0.599548  [284200/467875]\n",
            "loss: 0.626308  [287100/467875]\n",
            "loss: 0.520600  [290000/467875]\n",
            "loss: 0.585289  [292900/467875]\n",
            "loss: 0.513522  [295800/467875]\n",
            "loss: 0.563438  [298700/467875]\n",
            "loss: 0.534849  [301600/467875]\n",
            "loss: 0.528431  [304500/467875]\n",
            "loss: 0.525945  [307400/467875]\n",
            "loss: 0.423036  [310300/467875]\n",
            "loss: 0.522248  [313200/467875]\n",
            "loss: 0.577061  [316100/467875]\n",
            "loss: 0.596361  [319000/467875]\n",
            "loss: 0.471611  [321900/467875]\n",
            "loss: 0.458456  [324800/467875]\n",
            "loss: 0.507247  [327700/467875]\n",
            "loss: 0.423218  [330600/467875]\n",
            "loss: 0.514679  [333500/467875]\n",
            "loss: 0.572225  [336400/467875]\n",
            "loss: 0.473643  [339300/467875]\n",
            "loss: 0.569205  [342200/467875]\n",
            "loss: 0.545966  [345100/467875]\n",
            "loss: 0.508611  [348000/467875]\n",
            "loss: 0.516756  [350900/467875]\n",
            "loss: 0.486756  [353800/467875]\n",
            "loss: 0.635761  [356700/467875]\n",
            "loss: 0.544813  [359600/467875]\n",
            "loss: 0.575725  [362500/467875]\n",
            "loss: 0.462453  [365400/467875]\n",
            "loss: 0.482177  [368300/467875]\n",
            "loss: 0.492237  [371200/467875]\n",
            "loss: 0.556774  [374100/467875]\n",
            "loss: 0.550099  [377000/467875]\n",
            "loss: 0.450345  [379900/467875]\n",
            "loss: 0.600670  [382800/467875]\n",
            "loss: 0.547938  [385700/467875]\n",
            "loss: 0.467015  [388600/467875]\n",
            "loss: 0.602491  [391500/467875]\n",
            "loss: 0.480229  [394400/467875]\n",
            "loss: 0.524506  [397300/467875]\n",
            "loss: 0.587933  [400200/467875]\n",
            "loss: 0.677420  [403100/467875]\n",
            "loss: 0.476989  [406000/467875]\n",
            "loss: 0.580822  [408900/467875]\n",
            "loss: 0.611090  [411800/467875]\n",
            "loss: 0.595553  [414700/467875]\n",
            "loss: 0.551673  [417600/467875]\n",
            "loss: 0.517564  [420500/467875]\n",
            "loss: 0.480148  [423400/467875]\n",
            "loss: 0.495746  [426300/467875]\n",
            "loss: 0.612493  [429200/467875]\n",
            "loss: 0.462662  [432100/467875]\n",
            "loss: 0.415976  [435000/467875]\n",
            "loss: 0.482082  [437900/467875]\n",
            "loss: 0.526024  [440800/467875]\n",
            "loss: 0.494426  [443700/467875]\n",
            "loss: 0.472290  [446600/467875]\n",
            "loss: 0.381530  [449500/467875]\n",
            "loss: 0.573998  [452400/467875]\n",
            "loss: 0.508949  [455300/467875]\n",
            "loss: 0.443889  [458200/467875]\n",
            "loss: 0.596340  [461100/467875]\n",
            "loss: 0.475302  [464000/467875]\n",
            "loss: 0.518682  [466900/467875]\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522097 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 6 s\n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.473937  [    0/467875]\n",
            "loss: 0.548639  [ 2900/467875]\n",
            "loss: 0.609230  [ 5800/467875]\n",
            "loss: 0.592786  [ 8700/467875]\n",
            "loss: 0.512076  [11600/467875]\n",
            "loss: 0.600129  [14500/467875]\n",
            "loss: 0.582460  [17400/467875]\n",
            "loss: 0.424440  [20300/467875]\n",
            "loss: 0.551183  [23200/467875]\n",
            "loss: 0.454947  [26100/467875]\n",
            "loss: 0.513198  [29000/467875]\n",
            "loss: 0.423774  [31900/467875]\n",
            "loss: 0.514101  [34800/467875]\n",
            "loss: 0.565034  [37700/467875]\n",
            "loss: 0.578221  [40600/467875]\n",
            "loss: 0.501926  [43500/467875]\n",
            "loss: 0.590054  [46400/467875]\n",
            "loss: 0.589072  [49300/467875]\n",
            "loss: 0.519037  [52200/467875]\n",
            "loss: 0.466984  [55100/467875]\n",
            "loss: 0.521027  [58000/467875]\n",
            "loss: 0.512618  [60900/467875]\n",
            "loss: 0.513466  [63800/467875]\n",
            "loss: 0.662416  [66700/467875]\n",
            "loss: 0.432255  [69600/467875]\n",
            "loss: 0.526441  [72500/467875]\n",
            "loss: 0.565954  [75400/467875]\n",
            "loss: 0.459741  [78300/467875]\n",
            "loss: 0.534313  [81200/467875]\n",
            "loss: 0.443887  [84100/467875]\n",
            "loss: 0.573212  [87000/467875]\n",
            "loss: 0.511689  [89900/467875]\n",
            "loss: 0.495834  [92800/467875]\n",
            "loss: 0.662331  [95700/467875]\n",
            "loss: 0.425125  [98600/467875]\n",
            "loss: 0.497510  [101500/467875]\n",
            "loss: 0.546601  [104400/467875]\n",
            "loss: 0.599382  [107300/467875]\n",
            "loss: 0.567398  [110200/467875]\n",
            "loss: 0.492149  [113100/467875]\n",
            "loss: 0.539476  [116000/467875]\n",
            "loss: 0.519863  [118900/467875]\n",
            "loss: 0.550359  [121800/467875]\n",
            "loss: 0.579817  [124700/467875]\n",
            "loss: 0.504185  [127600/467875]\n",
            "loss: 0.432698  [130500/467875]\n",
            "loss: 0.468040  [133400/467875]\n",
            "loss: 0.617941  [136300/467875]\n",
            "loss: 0.545469  [139200/467875]\n",
            "loss: 0.455179  [142100/467875]\n",
            "loss: 0.389222  [145000/467875]\n",
            "loss: 0.535176  [147900/467875]\n",
            "loss: 0.486634  [150800/467875]\n",
            "loss: 0.574637  [153700/467875]\n",
            "loss: 0.527749  [156600/467875]\n",
            "loss: 0.413329  [159500/467875]\n",
            "loss: 0.497971  [162400/467875]\n",
            "loss: 0.551558  [165300/467875]\n",
            "loss: 0.550253  [168200/467875]\n",
            "loss: 0.531003  [171100/467875]\n",
            "loss: 0.584872  [174000/467875]\n",
            "loss: 0.571810  [176900/467875]\n",
            "loss: 0.457677  [179800/467875]\n",
            "loss: 0.556186  [182700/467875]\n",
            "loss: 0.495068  [185600/467875]\n",
            "loss: 0.515207  [188500/467875]\n",
            "loss: 0.423583  [191400/467875]\n",
            "loss: 0.486362  [194300/467875]\n",
            "loss: 0.541922  [197200/467875]\n",
            "loss: 0.614455  [200100/467875]\n",
            "loss: 0.601534  [203000/467875]\n",
            "loss: 0.486264  [205900/467875]\n",
            "loss: 0.596749  [208800/467875]\n",
            "loss: 0.477499  [211700/467875]\n",
            "loss: 0.578758  [214600/467875]\n",
            "loss: 0.597382  [217500/467875]\n",
            "loss: 0.529908  [220400/467875]\n",
            "loss: 0.490665  [223300/467875]\n",
            "loss: 0.413771  [226200/467875]\n",
            "loss: 0.489553  [229100/467875]\n",
            "loss: 0.457194  [232000/467875]\n",
            "loss: 0.627082  [234900/467875]\n",
            "loss: 0.517527  [237800/467875]\n",
            "loss: 0.466641  [240700/467875]\n",
            "loss: 0.552692  [243600/467875]\n",
            "loss: 0.540354  [246500/467875]\n",
            "loss: 0.387413  [249400/467875]\n",
            "loss: 0.610934  [252300/467875]\n",
            "loss: 0.443164  [255200/467875]\n",
            "loss: 0.452499  [258100/467875]\n",
            "loss: 0.540982  [261000/467875]\n",
            "loss: 0.401730  [263900/467875]\n",
            "loss: 0.653242  [266800/467875]\n",
            "loss: 0.526341  [269700/467875]\n",
            "loss: 0.668695  [272600/467875]\n",
            "loss: 0.518971  [275500/467875]\n",
            "loss: 0.567450  [278400/467875]\n",
            "loss: 0.497686  [281300/467875]\n",
            "loss: 0.533740  [284200/467875]\n",
            "loss: 0.507860  [287100/467875]\n",
            "loss: 0.589545  [290000/467875]\n",
            "loss: 0.528040  [292900/467875]\n",
            "loss: 0.599922  [295800/467875]\n",
            "loss: 0.403724  [298700/467875]\n",
            "loss: 0.497170  [301600/467875]\n",
            "loss: 0.463705  [304500/467875]\n",
            "loss: 0.513921  [307400/467875]\n",
            "loss: 0.506686  [310300/467875]\n",
            "loss: 0.615606  [313200/467875]\n",
            "loss: 0.545166  [316100/467875]\n",
            "loss: 0.506401  [319000/467875]\n",
            "loss: 0.627905  [321900/467875]\n",
            "loss: 0.501227  [324800/467875]\n",
            "loss: 0.622271  [327700/467875]\n",
            "loss: 0.549929  [330600/467875]\n",
            "loss: 0.607834  [333500/467875]\n",
            "loss: 0.440942  [336400/467875]\n",
            "loss: 0.527318  [339300/467875]\n",
            "loss: 0.505481  [342200/467875]\n",
            "loss: 0.516711  [345100/467875]\n",
            "loss: 0.562025  [348000/467875]\n",
            "loss: 0.459689  [350900/467875]\n",
            "loss: 0.555974  [353800/467875]\n",
            "loss: 0.416287  [356700/467875]\n",
            "loss: 0.525221  [359600/467875]\n",
            "loss: 0.551289  [362500/467875]\n",
            "loss: 0.530152  [365400/467875]\n",
            "loss: 0.556033  [368300/467875]\n",
            "loss: 0.512008  [371200/467875]\n",
            "loss: 0.501947  [374100/467875]\n",
            "loss: 0.414652  [377000/467875]\n",
            "loss: 0.479302  [379900/467875]\n",
            "loss: 0.546061  [382800/467875]\n",
            "loss: 0.531067  [385700/467875]\n",
            "loss: 0.601750  [388600/467875]\n",
            "loss: 0.540587  [391500/467875]\n",
            "loss: 0.491527  [394400/467875]\n",
            "loss: 0.495773  [397300/467875]\n",
            "loss: 0.433768  [400200/467875]\n",
            "loss: 0.498159  [403100/467875]\n",
            "loss: 0.565757  [406000/467875]\n",
            "loss: 0.417885  [408900/467875]\n",
            "loss: 0.585458  [411800/467875]\n",
            "loss: 0.438545  [414700/467875]\n",
            "loss: 0.483686  [417600/467875]\n",
            "loss: 0.691481  [420500/467875]\n",
            "loss: 0.460106  [423400/467875]\n",
            "loss: 0.450754  [426300/467875]\n",
            "loss: 0.530202  [429200/467875]\n",
            "loss: 0.530724  [432100/467875]\n",
            "loss: 0.516197  [435000/467875]\n",
            "loss: 0.553338  [437900/467875]\n",
            "loss: 0.537960  [440800/467875]\n",
            "loss: 0.508830  [443700/467875]\n",
            "loss: 0.436755  [446600/467875]\n",
            "loss: 0.515723  [449500/467875]\n",
            "loss: 0.524303  [452400/467875]\n",
            "loss: 0.498279  [455300/467875]\n",
            "loss: 0.510604  [458200/467875]\n",
            "loss: 0.461577  [461100/467875]\n",
            "loss: 0.574110  [464000/467875]\n",
            "loss: 0.490651  [466900/467875]\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522103 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 5 s\n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.500300  [    0/467875]\n",
            "loss: 0.666216  [ 2900/467875]\n",
            "loss: 0.461856  [ 5800/467875]\n",
            "loss: 0.551636  [ 8700/467875]\n",
            "loss: 0.495657  [11600/467875]\n",
            "loss: 0.600473  [14500/467875]\n",
            "loss: 0.485351  [17400/467875]\n",
            "loss: 0.467022  [20300/467875]\n",
            "loss: 0.485361  [23200/467875]\n",
            "loss: 0.539260  [26100/467875]\n",
            "loss: 0.495826  [29000/467875]\n",
            "loss: 0.517628  [31900/467875]\n",
            "loss: 0.453686  [34800/467875]\n",
            "loss: 0.523702  [37700/467875]\n",
            "loss: 0.537545  [40600/467875]\n",
            "loss: 0.554710  [43500/467875]\n",
            "loss: 0.449315  [46400/467875]\n",
            "loss: 0.489247  [49300/467875]\n",
            "loss: 0.580871  [52200/467875]\n",
            "loss: 0.501328  [55100/467875]\n",
            "loss: 0.532541  [58000/467875]\n",
            "loss: 0.652270  [60900/467875]\n",
            "loss: 0.510098  [63800/467875]\n",
            "loss: 0.448614  [66700/467875]\n",
            "loss: 0.633026  [69600/467875]\n",
            "loss: 0.512661  [72500/467875]\n",
            "loss: 0.497426  [75400/467875]\n",
            "loss: 0.402528  [78300/467875]\n",
            "loss: 0.589357  [81200/467875]\n",
            "loss: 0.610155  [84100/467875]\n",
            "loss: 0.547103  [87000/467875]\n",
            "loss: 0.498764  [89900/467875]\n",
            "loss: 0.534735  [92800/467875]\n",
            "loss: 0.513931  [95700/467875]\n",
            "loss: 0.428429  [98600/467875]\n",
            "loss: 0.436649  [101500/467875]\n",
            "loss: 0.535497  [104400/467875]\n",
            "loss: 0.441810  [107300/467875]\n",
            "loss: 0.381964  [110200/467875]\n",
            "loss: 0.539697  [113100/467875]\n",
            "loss: 0.495642  [116000/467875]\n",
            "loss: 0.513326  [118900/467875]\n",
            "loss: 0.515822  [121800/467875]\n",
            "loss: 0.565137  [124700/467875]\n",
            "loss: 0.468550  [127600/467875]\n",
            "loss: 0.560384  [130500/467875]\n",
            "loss: 0.511358  [133400/467875]\n",
            "loss: 0.540385  [136300/467875]\n",
            "loss: 0.599313  [139200/467875]\n",
            "loss: 0.458067  [142100/467875]\n",
            "loss: 0.511150  [145000/467875]\n",
            "loss: 0.543788  [147900/467875]\n",
            "loss: 0.508473  [150800/467875]\n",
            "loss: 0.493644  [153700/467875]\n",
            "loss: 0.466464  [156600/467875]\n",
            "loss: 0.633453  [159500/467875]\n",
            "loss: 0.520162  [162400/467875]\n",
            "loss: 0.471673  [165300/467875]\n",
            "loss: 0.538292  [168200/467875]\n",
            "loss: 0.417641  [171100/467875]\n",
            "loss: 0.528152  [174000/467875]\n",
            "loss: 0.582537  [176900/467875]\n",
            "loss: 0.527181  [179800/467875]\n",
            "loss: 0.518413  [182700/467875]\n",
            "loss: 0.643586  [185600/467875]\n",
            "loss: 0.453522  [188500/467875]\n",
            "loss: 0.544401  [191400/467875]\n",
            "loss: 0.634654  [194300/467875]\n",
            "loss: 0.459630  [197200/467875]\n",
            "loss: 0.471495  [200100/467875]\n",
            "loss: 0.523216  [203000/467875]\n",
            "loss: 0.408257  [205900/467875]\n",
            "loss: 0.519874  [208800/467875]\n",
            "loss: 0.552488  [211700/467875]\n",
            "loss: 0.532011  [214600/467875]\n",
            "loss: 0.513271  [217500/467875]\n",
            "loss: 0.521946  [220400/467875]\n",
            "loss: 0.638682  [223300/467875]\n",
            "loss: 0.496570  [226200/467875]\n",
            "loss: 0.565587  [229100/467875]\n",
            "loss: 0.352060  [232000/467875]\n",
            "loss: 0.479906  [234900/467875]\n",
            "loss: 0.531717  [237800/467875]\n",
            "loss: 0.395421  [240700/467875]\n",
            "loss: 0.465761  [243600/467875]\n",
            "loss: 0.571872  [246500/467875]\n",
            "loss: 0.595308  [249400/467875]\n",
            "loss: 0.565849  [252300/467875]\n",
            "loss: 0.485358  [255200/467875]\n",
            "loss: 0.496103  [258100/467875]\n",
            "loss: 0.593100  [261000/467875]\n",
            "loss: 0.615335  [263900/467875]\n",
            "loss: 0.568213  [266800/467875]\n",
            "loss: 0.490101  [269700/467875]\n",
            "loss: 0.660067  [272600/467875]\n",
            "loss: 0.489892  [275500/467875]\n",
            "loss: 0.472059  [278400/467875]\n",
            "loss: 0.587756  [281300/467875]\n",
            "loss: 0.510818  [284200/467875]\n",
            "loss: 0.635775  [287100/467875]\n",
            "loss: 0.513419  [290000/467875]\n",
            "loss: 0.490434  [292900/467875]\n",
            "loss: 0.441343  [295800/467875]\n",
            "loss: 0.424754  [298700/467875]\n",
            "loss: 0.469862  [301600/467875]\n",
            "loss: 0.508426  [304500/467875]\n",
            "loss: 0.461508  [307400/467875]\n",
            "loss: 0.506815  [310300/467875]\n",
            "loss: 0.520892  [313200/467875]\n",
            "loss: 0.619796  [316100/467875]\n",
            "loss: 0.439696  [319000/467875]\n",
            "loss: 0.414403  [321900/467875]\n",
            "loss: 0.584557  [324800/467875]\n",
            "loss: 0.489953  [327700/467875]\n",
            "loss: 0.544660  [330600/467875]\n",
            "loss: 0.478971  [333500/467875]\n",
            "loss: 0.532041  [336400/467875]\n",
            "loss: 0.418832  [339300/467875]\n",
            "loss: 0.556129  [342200/467875]\n",
            "loss: 0.590464  [345100/467875]\n",
            "loss: 0.668280  [348000/467875]\n",
            "loss: 0.477440  [350900/467875]\n",
            "loss: 0.466910  [353800/467875]\n",
            "loss: 0.539572  [356700/467875]\n",
            "loss: 0.483491  [359600/467875]\n",
            "loss: 0.559545  [362500/467875]\n",
            "loss: 0.710737  [365400/467875]\n",
            "loss: 0.485809  [368300/467875]\n",
            "loss: 0.512078  [371200/467875]\n",
            "loss: 0.619219  [374100/467875]\n",
            "loss: 0.533376  [377000/467875]\n",
            "loss: 0.626993  [379900/467875]\n",
            "loss: 0.571021  [382800/467875]\n",
            "loss: 0.670755  [385700/467875]\n",
            "loss: 0.455440  [388600/467875]\n",
            "loss: 0.537615  [391500/467875]\n",
            "loss: 0.463119  [394400/467875]\n",
            "loss: 0.501150  [397300/467875]\n",
            "loss: 0.519649  [400200/467875]\n",
            "loss: 0.596251  [403100/467875]\n",
            "loss: 0.481063  [406000/467875]\n",
            "loss: 0.385342  [408900/467875]\n",
            "loss: 0.492291  [411800/467875]\n",
            "loss: 0.445386  [414700/467875]\n",
            "loss: 0.570198  [417600/467875]\n",
            "loss: 0.510935  [420500/467875]\n",
            "loss: 0.400412  [423400/467875]\n",
            "loss: 0.541106  [426300/467875]\n",
            "loss: 0.551460  [429200/467875]\n",
            "loss: 0.627828  [432100/467875]\n",
            "loss: 0.519314  [435000/467875]\n",
            "loss: 0.481980  [437900/467875]\n",
            "loss: 0.516894  [440800/467875]\n",
            "loss: 0.446205  [443700/467875]\n",
            "loss: 0.512443  [446600/467875]\n",
            "loss: 0.401626  [449500/467875]\n",
            "loss: 0.470619  [452400/467875]\n",
            "loss: 0.586294  [455300/467875]\n",
            "loss: 0.575359  [458200/467875]\n",
            "loss: 0.487827  [461100/467875]\n",
            "loss: 0.492637  [464000/467875]\n",
            "loss: 0.479327  [466900/467875]\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.524332 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 4 s\n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.464314  [    0/467875]\n",
            "loss: 0.517492  [ 2900/467875]\n",
            "loss: 0.591424  [ 5800/467875]\n",
            "loss: 0.450176  [ 8700/467875]\n",
            "loss: 0.436154  [11600/467875]\n",
            "loss: 0.563511  [14500/467875]\n",
            "loss: 0.456984  [17400/467875]\n",
            "loss: 0.494594  [20300/467875]\n",
            "loss: 0.372372  [23200/467875]\n",
            "loss: 0.545884  [26100/467875]\n",
            "loss: 0.444170  [29000/467875]\n",
            "loss: 0.537064  [31900/467875]\n",
            "loss: 0.522964  [34800/467875]\n",
            "loss: 0.413610  [37700/467875]\n",
            "loss: 0.528278  [40600/467875]\n",
            "loss: 0.482116  [43500/467875]\n",
            "loss: 0.541604  [46400/467875]\n",
            "loss: 0.585284  [49300/467875]\n",
            "loss: 0.634654  [52200/467875]\n",
            "loss: 0.669272  [55100/467875]\n",
            "loss: 0.526156  [58000/467875]\n",
            "loss: 0.557338  [60900/467875]\n",
            "loss: 0.440694  [63800/467875]\n",
            "loss: 0.551102  [66700/467875]\n",
            "loss: 0.465447  [69600/467875]\n",
            "loss: 0.438907  [72500/467875]\n",
            "loss: 0.513509  [75400/467875]\n",
            "loss: 0.422051  [78300/467875]\n",
            "loss: 0.484543  [81200/467875]\n",
            "loss: 0.604144  [84100/467875]\n",
            "loss: 0.669072  [87000/467875]\n",
            "loss: 0.662501  [89900/467875]\n",
            "loss: 0.505267  [92800/467875]\n",
            "loss: 0.440292  [95700/467875]\n",
            "loss: 0.516358  [98600/467875]\n",
            "loss: 0.529812  [101500/467875]\n",
            "loss: 0.463123  [104400/467875]\n",
            "loss: 0.541507  [107300/467875]\n",
            "loss: 0.571146  [110200/467875]\n",
            "loss: 0.508221  [113100/467875]\n",
            "loss: 0.547824  [116000/467875]\n",
            "loss: 0.606292  [118900/467875]\n",
            "loss: 0.363888  [121800/467875]\n",
            "loss: 0.563265  [124700/467875]\n",
            "loss: 0.559164  [127600/467875]\n",
            "loss: 0.476703  [130500/467875]\n",
            "loss: 0.436627  [133400/467875]\n",
            "loss: 0.476002  [136300/467875]\n",
            "loss: 0.518289  [139200/467875]\n",
            "loss: 0.528405  [142100/467875]\n",
            "loss: 0.577477  [145000/467875]\n",
            "loss: 0.562021  [147900/467875]\n",
            "loss: 0.513776  [150800/467875]\n",
            "loss: 0.530625  [153700/467875]\n",
            "loss: 0.486268  [156600/467875]\n",
            "loss: 0.429094  [159500/467875]\n",
            "loss: 0.765996  [162400/467875]\n",
            "loss: 0.498883  [165300/467875]\n",
            "loss: 0.594572  [168200/467875]\n",
            "loss: 0.522048  [171100/467875]\n",
            "loss: 0.641947  [174000/467875]\n",
            "loss: 0.478082  [176900/467875]\n",
            "loss: 0.636095  [179800/467875]\n",
            "loss: 0.374364  [182700/467875]\n",
            "loss: 0.512319  [185600/467875]\n",
            "loss: 0.563170  [188500/467875]\n",
            "loss: 0.495575  [191400/467875]\n",
            "loss: 0.547748  [194300/467875]\n",
            "loss: 0.571625  [197200/467875]\n",
            "loss: 0.554011  [200100/467875]\n",
            "loss: 0.465011  [203000/467875]\n",
            "loss: 0.522917  [205900/467875]\n",
            "loss: 0.564257  [208800/467875]\n",
            "loss: 0.433654  [211700/467875]\n",
            "loss: 0.596497  [214600/467875]\n",
            "loss: 0.445363  [217500/467875]\n",
            "loss: 0.427889  [220400/467875]\n",
            "loss: 0.444072  [223300/467875]\n",
            "loss: 0.535481  [226200/467875]\n",
            "loss: 0.517144  [229100/467875]\n",
            "loss: 0.584425  [232000/467875]\n",
            "loss: 0.436812  [234900/467875]\n",
            "loss: 0.506899  [237800/467875]\n",
            "loss: 0.564030  [240700/467875]\n",
            "loss: 0.491198  [243600/467875]\n",
            "loss: 0.522650  [246500/467875]\n",
            "loss: 0.525124  [249400/467875]\n",
            "loss: 0.543841  [252300/467875]\n",
            "loss: 0.559814  [255200/467875]\n",
            "loss: 0.441731  [258100/467875]\n",
            "loss: 0.487912  [261000/467875]\n",
            "loss: 0.508754  [263900/467875]\n",
            "loss: 0.514015  [266800/467875]\n",
            "loss: 0.531070  [269700/467875]\n",
            "loss: 0.541838  [272600/467875]\n",
            "loss: 0.529684  [275500/467875]\n",
            "loss: 0.503597  [278400/467875]\n",
            "loss: 0.538342  [281300/467875]\n",
            "loss: 0.634444  [284200/467875]\n",
            "loss: 0.666716  [287100/467875]\n",
            "loss: 0.499296  [290000/467875]\n",
            "loss: 0.514395  [292900/467875]\n",
            "loss: 0.452368  [295800/467875]\n",
            "loss: 0.467949  [298700/467875]\n",
            "loss: 0.712159  [301600/467875]\n",
            "loss: 0.573992  [304500/467875]\n",
            "loss: 0.575289  [307400/467875]\n",
            "loss: 0.526991  [310300/467875]\n",
            "loss: 0.525416  [313200/467875]\n",
            "loss: 0.453959  [316100/467875]\n",
            "loss: 0.646045  [319000/467875]\n",
            "loss: 0.614228  [321900/467875]\n",
            "loss: 0.500632  [324800/467875]\n",
            "loss: 0.530381  [327700/467875]\n",
            "loss: 0.470385  [330600/467875]\n",
            "loss: 0.592298  [333500/467875]\n",
            "loss: 0.516657  [336400/467875]\n",
            "loss: 0.452519  [339300/467875]\n",
            "loss: 0.526527  [342200/467875]\n",
            "loss: 0.563914  [345100/467875]\n",
            "loss: 0.582036  [348000/467875]\n",
            "loss: 0.481091  [350900/467875]\n",
            "loss: 0.578801  [353800/467875]\n",
            "loss: 0.603630  [356700/467875]\n",
            "loss: 0.543563  [359600/467875]\n",
            "loss: 0.415697  [362500/467875]\n",
            "loss: 0.511529  [365400/467875]\n",
            "loss: 0.559866  [368300/467875]\n",
            "loss: 0.626565  [371200/467875]\n",
            "loss: 0.575155  [374100/467875]\n",
            "loss: 0.635608  [377000/467875]\n",
            "loss: 0.704597  [379900/467875]\n",
            "loss: 0.512373  [382800/467875]\n",
            "loss: 0.386879  [385700/467875]\n",
            "loss: 0.525441  [388600/467875]\n",
            "loss: 0.685941  [391500/467875]\n",
            "loss: 0.508592  [394400/467875]\n",
            "loss: 0.499818  [397300/467875]\n",
            "loss: 0.611189  [400200/467875]\n",
            "loss: 0.523905  [403100/467875]\n",
            "loss: 0.486865  [406000/467875]\n",
            "loss: 0.419803  [408900/467875]\n",
            "loss: 0.477017  [411800/467875]\n",
            "loss: 0.593433  [414700/467875]\n",
            "loss: 0.587688  [417600/467875]\n",
            "loss: 0.475221  [420500/467875]\n",
            "loss: 0.640608  [423400/467875]\n",
            "loss: 0.499277  [426300/467875]\n",
            "loss: 0.512763  [429200/467875]\n",
            "loss: 0.480331  [432100/467875]\n",
            "loss: 0.483361  [435000/467875]\n",
            "loss: 0.559853  [437900/467875]\n",
            "loss: 0.480275  [440800/467875]\n",
            "loss: 0.559747  [443700/467875]\n",
            "loss: 0.459637  [446600/467875]\n",
            "loss: 0.516345  [449500/467875]\n",
            "loss: 0.489220  [452400/467875]\n",
            "loss: 0.504002  [455300/467875]\n",
            "loss: 0.491782  [458200/467875]\n",
            "loss: 0.569735  [461100/467875]\n",
            "loss: 0.431092  [464000/467875]\n",
            "loss: 0.575586  [466900/467875]\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522610 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 2 s\n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.539749  [    0/467875]\n",
            "loss: 0.446625  [ 2900/467875]\n",
            "loss: 0.534905  [ 5800/467875]\n",
            "loss: 0.468553  [ 8700/467875]\n",
            "loss: 0.590997  [11600/467875]\n",
            "loss: 0.554796  [14500/467875]\n",
            "loss: 0.576707  [17400/467875]\n",
            "loss: 0.545998  [20300/467875]\n",
            "loss: 0.455626  [23200/467875]\n",
            "loss: 0.511081  [26100/467875]\n",
            "loss: 0.500764  [29000/467875]\n",
            "loss: 0.514288  [31900/467875]\n",
            "loss: 0.507640  [34800/467875]\n",
            "loss: 0.584689  [37700/467875]\n",
            "loss: 0.610229  [40600/467875]\n",
            "loss: 0.537396  [43500/467875]\n",
            "loss: 0.414107  [46400/467875]\n",
            "loss: 0.536674  [49300/467875]\n",
            "loss: 0.608945  [52200/467875]\n",
            "loss: 0.613116  [55100/467875]\n",
            "loss: 0.580628  [58000/467875]\n",
            "loss: 0.624403  [60900/467875]\n",
            "loss: 0.673146  [63800/467875]\n",
            "loss: 0.530161  [66700/467875]\n",
            "loss: 0.706236  [69600/467875]\n",
            "loss: 0.408910  [72500/467875]\n",
            "loss: 0.489242  [75400/467875]\n",
            "loss: 0.500630  [78300/467875]\n",
            "loss: 0.585363  [81200/467875]\n",
            "loss: 0.551474  [84100/467875]\n",
            "loss: 0.605110  [87000/467875]\n",
            "loss: 0.562411  [89900/467875]\n",
            "loss: 0.511423  [92800/467875]\n",
            "loss: 0.498777  [95700/467875]\n",
            "loss: 0.686859  [98600/467875]\n",
            "loss: 0.579772  [101500/467875]\n",
            "loss: 0.475014  [104400/467875]\n",
            "loss: 0.556308  [107300/467875]\n",
            "loss: 0.522331  [110200/467875]\n",
            "loss: 0.407908  [113100/467875]\n",
            "loss: 0.546920  [116000/467875]\n",
            "loss: 0.564113  [118900/467875]\n",
            "loss: 0.411565  [121800/467875]\n",
            "loss: 0.528591  [124700/467875]\n",
            "loss: 0.395661  [127600/467875]\n",
            "loss: 0.464312  [130500/467875]\n",
            "loss: 0.601244  [133400/467875]\n",
            "loss: 0.429603  [136300/467875]\n",
            "loss: 0.531920  [139200/467875]\n",
            "loss: 0.493986  [142100/467875]\n",
            "loss: 0.502470  [145000/467875]\n",
            "loss: 0.590432  [147900/467875]\n",
            "loss: 0.610087  [150800/467875]\n",
            "loss: 0.595845  [153700/467875]\n",
            "loss: 0.403627  [156600/467875]\n",
            "loss: 0.531755  [159500/467875]\n",
            "loss: 0.588957  [162400/467875]\n",
            "loss: 0.505678  [165300/467875]\n",
            "loss: 0.482170  [168200/467875]\n",
            "loss: 0.580640  [171100/467875]\n",
            "loss: 0.491625  [174000/467875]\n",
            "loss: 0.626494  [176900/467875]\n",
            "loss: 0.557765  [179800/467875]\n",
            "loss: 0.445790  [182700/467875]\n",
            "loss: 0.560743  [185600/467875]\n",
            "loss: 0.489545  [188500/467875]\n",
            "loss: 0.575145  [191400/467875]\n",
            "loss: 0.426712  [194300/467875]\n",
            "loss: 0.395828  [197200/467875]\n",
            "loss: 0.523441  [200100/467875]\n",
            "loss: 0.611864  [203000/467875]\n",
            "loss: 0.486052  [205900/467875]\n",
            "loss: 0.579610  [208800/467875]\n",
            "loss: 0.549802  [211700/467875]\n",
            "loss: 0.417250  [214600/467875]\n",
            "loss: 0.434699  [217500/467875]\n",
            "loss: 0.442828  [220400/467875]\n",
            "loss: 0.498581  [223300/467875]\n",
            "loss: 0.501109  [226200/467875]\n",
            "loss: 0.536647  [229100/467875]\n",
            "loss: 0.524990  [232000/467875]\n",
            "loss: 0.554949  [234900/467875]\n",
            "loss: 0.556664  [237800/467875]\n",
            "loss: 0.489707  [240700/467875]\n",
            "loss: 0.535532  [243600/467875]\n",
            "loss: 0.429291  [246500/467875]\n",
            "loss: 0.570466  [249400/467875]\n",
            "loss: 0.475492  [252300/467875]\n",
            "loss: 0.527089  [255200/467875]\n",
            "loss: 0.536831  [258100/467875]\n",
            "loss: 0.508439  [261000/467875]\n",
            "loss: 0.440307  [263900/467875]\n",
            "loss: 0.658495  [266800/467875]\n",
            "loss: 0.568099  [269700/467875]\n",
            "loss: 0.446173  [272600/467875]\n",
            "loss: 0.480751  [275500/467875]\n",
            "loss: 0.548186  [278400/467875]\n",
            "loss: 0.441965  [281300/467875]\n",
            "loss: 0.466889  [284200/467875]\n",
            "loss: 0.565069  [287100/467875]\n",
            "loss: 0.470697  [290000/467875]\n",
            "loss: 0.657955  [292900/467875]\n",
            "loss: 0.411891  [295800/467875]\n",
            "loss: 0.598302  [298700/467875]\n",
            "loss: 0.511335  [301600/467875]\n",
            "loss: 0.541751  [304500/467875]\n",
            "loss: 0.446082  [307400/467875]\n",
            "loss: 0.413251  [310300/467875]\n",
            "loss: 0.443985  [313200/467875]\n",
            "loss: 0.653157  [316100/467875]\n",
            "loss: 0.567617  [319000/467875]\n",
            "loss: 0.537421  [321900/467875]\n",
            "loss: 0.465049  [324800/467875]\n",
            "loss: 0.471481  [327700/467875]\n",
            "loss: 0.387073  [330600/467875]\n",
            "loss: 0.547154  [333500/467875]\n",
            "loss: 0.444860  [336400/467875]\n",
            "loss: 0.392766  [339300/467875]\n",
            "loss: 0.489118  [342200/467875]\n",
            "loss: 0.461332  [345100/467875]\n",
            "loss: 0.489814  [348000/467875]\n",
            "loss: 0.582448  [350900/467875]\n",
            "loss: 0.559102  [353800/467875]\n",
            "loss: 0.532943  [356700/467875]\n",
            "loss: 0.509266  [359600/467875]\n",
            "loss: 0.547474  [362500/467875]\n",
            "loss: 0.535190  [365400/467875]\n",
            "loss: 0.520878  [368300/467875]\n",
            "loss: 0.578305  [371200/467875]\n",
            "loss: 0.660041  [374100/467875]\n",
            "loss: 0.592178  [377000/467875]\n",
            "loss: 0.529052  [379900/467875]\n",
            "loss: 0.447912  [382800/467875]\n",
            "loss: 0.661820  [385700/467875]\n",
            "loss: 0.536739  [388600/467875]\n",
            "loss: 0.614515  [391500/467875]\n",
            "loss: 0.444082  [394400/467875]\n",
            "loss: 0.511547  [397300/467875]\n",
            "loss: 0.495926  [400200/467875]\n",
            "loss: 0.460570  [403100/467875]\n",
            "loss: 0.603230  [406000/467875]\n",
            "loss: 0.423882  [408900/467875]\n",
            "loss: 0.616067  [411800/467875]\n",
            "loss: 0.525639  [414700/467875]\n",
            "loss: 0.482125  [417600/467875]\n",
            "loss: 0.485936  [420500/467875]\n",
            "loss: 0.482913  [423400/467875]\n",
            "loss: 0.465549  [426300/467875]\n",
            "loss: 0.607417  [429200/467875]\n",
            "loss: 0.537438  [432100/467875]\n",
            "loss: 0.442658  [435000/467875]\n",
            "loss: 0.573451  [437900/467875]\n",
            "loss: 0.677526  [440800/467875]\n",
            "loss: 0.494863  [443700/467875]\n",
            "loss: 0.426133  [446600/467875]\n",
            "loss: 0.519824  [449500/467875]\n",
            "loss: 0.417380  [452400/467875]\n",
            "loss: 0.670704  [455300/467875]\n",
            "loss: 0.382221  [458200/467875]\n",
            "loss: 0.676643  [461100/467875]\n",
            "loss: 0.536860  [464000/467875]\n",
            "loss: 0.444147  [466900/467875]\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.522042 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 3 s\n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.537934  [    0/467875]\n",
            "loss: 0.478530  [ 2900/467875]\n",
            "loss: 0.471652  [ 5800/467875]\n",
            "loss: 0.610863  [ 8700/467875]\n",
            "loss: 0.412885  [11600/467875]\n",
            "loss: 0.464199  [14500/467875]\n",
            "loss: 0.617624  [17400/467875]\n",
            "loss: 0.493521  [20300/467875]\n",
            "loss: 0.523223  [23200/467875]\n",
            "loss: 0.470944  [26100/467875]\n",
            "loss: 0.531192  [29000/467875]\n",
            "loss: 0.519116  [31900/467875]\n",
            "loss: 0.556215  [34800/467875]\n",
            "loss: 0.480414  [37700/467875]\n",
            "loss: 0.448788  [40600/467875]\n",
            "loss: 0.496629  [43500/467875]\n",
            "loss: 0.489615  [46400/467875]\n",
            "loss: 0.713406  [49300/467875]\n",
            "loss: 0.488420  [52200/467875]\n",
            "loss: 0.538349  [55100/467875]\n",
            "loss: 0.406374  [58000/467875]\n",
            "loss: 0.547815  [60900/467875]\n",
            "loss: 0.438795  [63800/467875]\n",
            "loss: 0.679633  [66700/467875]\n",
            "loss: 0.482260  [69600/467875]\n",
            "loss: 0.612898  [72500/467875]\n",
            "loss: 0.515433  [75400/467875]\n",
            "loss: 0.477879  [78300/467875]\n",
            "loss: 0.433072  [81200/467875]\n",
            "loss: 0.487353  [84100/467875]\n",
            "loss: 0.438395  [87000/467875]\n",
            "loss: 0.527637  [89900/467875]\n",
            "loss: 0.488910  [92800/467875]\n",
            "loss: 0.561923  [95700/467875]\n",
            "loss: 0.651771  [98600/467875]\n",
            "loss: 0.442693  [101500/467875]\n",
            "loss: 0.491993  [104400/467875]\n",
            "loss: 0.549778  [107300/467875]\n",
            "loss: 0.424243  [110200/467875]\n",
            "loss: 0.426869  [113100/467875]\n",
            "loss: 0.409875  [116000/467875]\n",
            "loss: 0.540932  [118900/467875]\n",
            "loss: 0.552567  [121800/467875]\n",
            "loss: 0.513054  [124700/467875]\n",
            "loss: 0.520324  [127600/467875]\n",
            "loss: 0.469323  [130500/467875]\n",
            "loss: 0.596030  [133400/467875]\n",
            "loss: 0.541212  [136300/467875]\n",
            "loss: 0.490689  [139200/467875]\n",
            "loss: 0.560635  [142100/467875]\n",
            "loss: 0.457148  [145000/467875]\n",
            "loss: 0.434248  [147900/467875]\n",
            "loss: 0.579297  [150800/467875]\n",
            "loss: 0.511587  [153700/467875]\n",
            "loss: 0.494275  [156600/467875]\n",
            "loss: 0.585754  [159500/467875]\n",
            "loss: 0.448367  [162400/467875]\n",
            "loss: 0.512990  [165300/467875]\n",
            "loss: 0.392736  [168200/467875]\n",
            "loss: 0.432138  [171100/467875]\n",
            "loss: 0.460495  [174000/467875]\n",
            "loss: 0.452809  [176900/467875]\n",
            "loss: 0.523253  [179800/467875]\n",
            "loss: 0.469456  [182700/467875]\n",
            "loss: 0.412622  [185600/467875]\n",
            "loss: 0.444766  [188500/467875]\n",
            "loss: 0.405551  [191400/467875]\n",
            "loss: 0.552333  [194300/467875]\n",
            "loss: 0.547583  [197200/467875]\n",
            "loss: 0.500086  [200100/467875]\n",
            "loss: 0.429965  [203000/467875]\n",
            "loss: 0.536248  [205900/467875]\n",
            "loss: 0.486819  [208800/467875]\n",
            "loss: 0.545236  [211700/467875]\n",
            "loss: 0.577290  [214600/467875]\n",
            "loss: 0.590870  [217500/467875]\n",
            "loss: 0.494328  [220400/467875]\n",
            "loss: 0.488638  [223300/467875]\n",
            "loss: 0.555880  [226200/467875]\n",
            "loss: 0.586137  [229100/467875]\n",
            "loss: 0.503345  [232000/467875]\n",
            "loss: 0.517744  [234900/467875]\n",
            "loss: 0.465649  [237800/467875]\n",
            "loss: 0.564535  [240700/467875]\n",
            "loss: 0.571999  [243600/467875]\n",
            "loss: 0.495537  [246500/467875]\n",
            "loss: 0.427111  [249400/467875]\n",
            "loss: 0.472351  [252300/467875]\n",
            "loss: 0.449183  [255200/467875]\n",
            "loss: 0.376143  [258100/467875]\n",
            "loss: 0.554528  [261000/467875]\n",
            "loss: 0.536158  [263900/467875]\n",
            "loss: 0.540472  [266800/467875]\n",
            "loss: 0.596319  [269700/467875]\n",
            "loss: 0.597491  [272600/467875]\n",
            "loss: 0.582680  [275500/467875]\n",
            "loss: 0.529499  [278400/467875]\n",
            "loss: 0.571949  [281300/467875]\n",
            "loss: 0.606234  [284200/467875]\n",
            "loss: 0.543781  [287100/467875]\n",
            "loss: 0.424780  [290000/467875]\n",
            "loss: 0.473384  [292900/467875]\n",
            "loss: 0.554966  [295800/467875]\n",
            "loss: 0.392457  [298700/467875]\n",
            "loss: 0.596268  [301600/467875]\n",
            "loss: 0.437156  [304500/467875]\n",
            "loss: 0.467947  [307400/467875]\n",
            "loss: 0.608219  [310300/467875]\n",
            "loss: 0.473255  [313200/467875]\n",
            "loss: 0.569917  [316100/467875]\n",
            "loss: 0.653536  [319000/467875]\n",
            "loss: 0.433923  [321900/467875]\n",
            "loss: 0.483255  [324800/467875]\n",
            "loss: 0.574049  [327700/467875]\n",
            "loss: 0.503391  [330600/467875]\n",
            "loss: 0.481018  [333500/467875]\n",
            "loss: 0.580423  [336400/467875]\n",
            "loss: 0.477685  [339300/467875]\n",
            "loss: 0.496845  [342200/467875]\n",
            "loss: 0.373272  [345100/467875]\n",
            "loss: 0.514545  [348000/467875]\n",
            "loss: 0.539158  [350900/467875]\n",
            "loss: 0.535305  [353800/467875]\n",
            "loss: 0.460908  [356700/467875]\n",
            "loss: 0.518836  [359600/467875]\n",
            "loss: 0.597645  [362500/467875]\n",
            "loss: 0.519056  [365400/467875]\n",
            "loss: 0.493115  [368300/467875]\n",
            "loss: 0.510170  [371200/467875]\n",
            "loss: 0.476441  [374100/467875]\n",
            "loss: 0.546171  [377000/467875]\n",
            "loss: 0.570942  [379900/467875]\n",
            "loss: 0.572278  [382800/467875]\n",
            "loss: 0.545147  [385700/467875]\n",
            "loss: 0.438546  [388600/467875]\n",
            "loss: 0.533647  [391500/467875]\n",
            "loss: 0.579492  [394400/467875]\n",
            "loss: 0.433781  [397300/467875]\n",
            "loss: 0.531534  [400200/467875]\n",
            "loss: 0.600245  [403100/467875]\n",
            "loss: 0.529744  [406000/467875]\n",
            "loss: 0.592219  [408900/467875]\n",
            "loss: 0.735267  [411800/467875]\n",
            "loss: 0.421685  [414700/467875]\n",
            "loss: 0.574881  [417600/467875]\n",
            "loss: 0.563751  [420500/467875]\n",
            "loss: 0.496780  [423400/467875]\n",
            "loss: 0.615350  [426300/467875]\n",
            "loss: 0.433837  [429200/467875]\n",
            "loss: 0.432654  [432100/467875]\n",
            "loss: 0.565461  [435000/467875]\n",
            "loss: 0.556220  [437900/467875]\n",
            "loss: 0.612594  [440800/467875]\n",
            "loss: 0.482342  [443700/467875]\n",
            "loss: 0.580891  [446600/467875]\n",
            "loss: 0.532349  [449500/467875]\n",
            "loss: 0.504554  [452400/467875]\n",
            "loss: 0.435852  [455300/467875]\n",
            "loss: 0.608495  [458200/467875]\n",
            "loss: 0.494291  [461100/467875]\n",
            "loss: 0.559953  [464000/467875]\n",
            "loss: 0.489025  [466900/467875]\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522792 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 4 s\n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.520453  [    0/467875]\n",
            "loss: 0.433287  [ 2900/467875]\n",
            "loss: 0.621318  [ 5800/467875]\n",
            "loss: 0.595730  [ 8700/467875]\n",
            "loss: 0.539664  [11600/467875]\n",
            "loss: 0.404920  [14500/467875]\n",
            "loss: 0.479554  [17400/467875]\n",
            "loss: 0.604788  [20300/467875]\n",
            "loss: 0.478350  [23200/467875]\n",
            "loss: 0.493248  [26100/467875]\n",
            "loss: 0.489876  [29000/467875]\n",
            "loss: 0.421604  [31900/467875]\n",
            "loss: 0.480016  [34800/467875]\n",
            "loss: 0.422895  [37700/467875]\n",
            "loss: 0.562991  [40600/467875]\n",
            "loss: 0.507032  [43500/467875]\n",
            "loss: 0.427258  [46400/467875]\n",
            "loss: 0.485264  [49300/467875]\n",
            "loss: 0.467007  [52200/467875]\n",
            "loss: 0.549372  [55100/467875]\n",
            "loss: 0.598513  [58000/467875]\n",
            "loss: 0.423329  [60900/467875]\n",
            "loss: 0.631163  [63800/467875]\n",
            "loss: 0.550565  [66700/467875]\n",
            "loss: 0.469049  [69600/467875]\n",
            "loss: 0.547207  [72500/467875]\n",
            "loss: 0.481547  [75400/467875]\n",
            "loss: 0.436613  [78300/467875]\n",
            "loss: 0.554201  [81200/467875]\n",
            "loss: 0.489548  [84100/467875]\n",
            "loss: 0.593865  [87000/467875]\n",
            "loss: 0.616965  [89900/467875]\n",
            "loss: 0.512193  [92800/467875]\n",
            "loss: 0.573483  [95700/467875]\n",
            "loss: 0.577464  [98600/467875]\n",
            "loss: 0.491579  [101500/467875]\n",
            "loss: 0.497180  [104400/467875]\n",
            "loss: 0.651451  [107300/467875]\n",
            "loss: 0.568349  [110200/467875]\n",
            "loss: 0.558874  [113100/467875]\n",
            "loss: 0.568004  [116000/467875]\n",
            "loss: 0.548284  [118900/467875]\n",
            "loss: 0.602275  [121800/467875]\n",
            "loss: 0.452236  [124700/467875]\n",
            "loss: 0.447825  [127600/467875]\n",
            "loss: 0.589066  [130500/467875]\n",
            "loss: 0.495555  [133400/467875]\n",
            "loss: 0.647265  [136300/467875]\n",
            "loss: 0.497740  [139200/467875]\n",
            "loss: 0.490119  [142100/467875]\n",
            "loss: 0.457672  [145000/467875]\n",
            "loss: 0.512240  [147900/467875]\n",
            "loss: 0.517111  [150800/467875]\n",
            "loss: 0.502964  [153700/467875]\n",
            "loss: 0.575401  [156600/467875]\n",
            "loss: 0.437864  [159500/467875]\n",
            "loss: 0.473323  [162400/467875]\n",
            "loss: 0.504210  [165300/467875]\n",
            "loss: 0.487705  [168200/467875]\n",
            "loss: 0.506886  [171100/467875]\n",
            "loss: 0.505560  [174000/467875]\n",
            "loss: 0.558899  [176900/467875]\n",
            "loss: 0.405241  [179800/467875]\n",
            "loss: 0.519277  [182700/467875]\n",
            "loss: 0.572126  [185600/467875]\n",
            "loss: 0.449403  [188500/467875]\n",
            "loss: 0.557603  [191400/467875]\n",
            "loss: 0.496238  [194300/467875]\n",
            "loss: 0.528237  [197200/467875]\n",
            "loss: 0.511445  [200100/467875]\n",
            "loss: 0.565760  [203000/467875]\n",
            "loss: 0.504409  [205900/467875]\n",
            "loss: 0.452686  [208800/467875]\n",
            "loss: 0.491774  [211700/467875]\n",
            "loss: 0.437090  [214600/467875]\n",
            "loss: 0.388826  [217500/467875]\n",
            "loss: 0.541305  [220400/467875]\n",
            "loss: 0.577364  [223300/467875]\n",
            "loss: 0.573302  [226200/467875]\n",
            "loss: 0.528302  [229100/467875]\n",
            "loss: 0.545157  [232000/467875]\n",
            "loss: 0.379363  [234900/467875]\n",
            "loss: 0.505591  [237800/467875]\n",
            "loss: 0.408280  [240700/467875]\n",
            "loss: 0.571522  [243600/467875]\n",
            "loss: 0.480972  [246500/467875]\n",
            "loss: 0.660905  [249400/467875]\n",
            "loss: 0.491262  [252300/467875]\n",
            "loss: 0.576756  [255200/467875]\n",
            "loss: 0.395456  [258100/467875]\n",
            "loss: 0.572754  [261000/467875]\n",
            "loss: 0.551299  [263900/467875]\n",
            "loss: 0.433882  [266800/467875]\n",
            "loss: 0.445446  [269700/467875]\n",
            "loss: 0.577564  [272600/467875]\n",
            "loss: 0.546894  [275500/467875]\n",
            "loss: 0.500883  [278400/467875]\n",
            "loss: 0.480056  [281300/467875]\n",
            "loss: 0.523962  [284200/467875]\n",
            "loss: 0.480799  [287100/467875]\n",
            "loss: 0.512814  [290000/467875]\n",
            "loss: 0.423138  [292900/467875]\n",
            "loss: 0.499750  [295800/467875]\n",
            "loss: 0.453664  [298700/467875]\n",
            "loss: 0.419968  [301600/467875]\n",
            "loss: 0.530377  [304500/467875]\n",
            "loss: 0.612063  [307400/467875]\n",
            "loss: 0.494310  [310300/467875]\n",
            "loss: 0.491554  [313200/467875]\n",
            "loss: 0.517452  [316100/467875]\n",
            "loss: 0.481371  [319000/467875]\n",
            "loss: 0.554870  [321900/467875]\n",
            "loss: 0.595115  [324800/467875]\n",
            "loss: 0.473903  [327700/467875]\n",
            "loss: 0.436589  [330600/467875]\n",
            "loss: 0.527383  [333500/467875]\n",
            "loss: 0.612755  [336400/467875]\n",
            "loss: 0.711918  [339300/467875]\n",
            "loss: 0.543121  [342200/467875]\n",
            "loss: 0.600228  [345100/467875]\n",
            "loss: 0.510490  [348000/467875]\n",
            "loss: 0.562407  [350900/467875]\n",
            "loss: 0.593899  [353800/467875]\n",
            "loss: 0.513296  [356700/467875]\n",
            "loss: 0.557625  [359600/467875]\n",
            "loss: 0.449823  [362500/467875]\n",
            "loss: 0.450583  [365400/467875]\n",
            "loss: 0.518951  [368300/467875]\n",
            "loss: 0.514165  [371200/467875]\n",
            "loss: 0.484263  [374100/467875]\n",
            "loss: 0.599363  [377000/467875]\n",
            "loss: 0.463717  [379900/467875]\n",
            "loss: 0.528457  [382800/467875]\n",
            "loss: 0.427417  [385700/467875]\n",
            "loss: 0.591172  [388600/467875]\n",
            "loss: 0.556050  [391500/467875]\n",
            "loss: 0.466992  [394400/467875]\n",
            "loss: 0.560589  [397300/467875]\n",
            "loss: 0.601344  [400200/467875]\n",
            "loss: 0.412982  [403100/467875]\n",
            "loss: 0.423658  [406000/467875]\n",
            "loss: 0.473313  [408900/467875]\n",
            "loss: 0.747786  [411800/467875]\n",
            "loss: 0.616702  [414700/467875]\n",
            "loss: 0.689837  [417600/467875]\n",
            "loss: 0.659630  [420500/467875]\n",
            "loss: 0.560791  [423400/467875]\n",
            "loss: 0.452868  [426300/467875]\n",
            "loss: 0.507527  [429200/467875]\n",
            "loss: 0.444845  [432100/467875]\n",
            "loss: 0.471685  [435000/467875]\n",
            "loss: 0.476631  [437900/467875]\n",
            "loss: 0.533539  [440800/467875]\n",
            "loss: 0.462452  [443700/467875]\n",
            "loss: 0.591190  [446600/467875]\n",
            "loss: 0.420283  [449500/467875]\n",
            "loss: 0.501128  [452400/467875]\n",
            "loss: 0.569767  [455300/467875]\n",
            "loss: 0.545680  [458200/467875]\n",
            "loss: 0.545964  [461100/467875]\n",
            "loss: 0.520769  [464000/467875]\n",
            "loss: 0.531623  [466900/467875]\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.523020 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 5 s\n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.607101  [    0/467875]\n",
            "loss: 0.567398  [ 2900/467875]\n",
            "loss: 0.504101  [ 5800/467875]\n",
            "loss: 0.617827  [ 8700/467875]\n",
            "loss: 0.537523  [11600/467875]\n",
            "loss: 0.602164  [14500/467875]\n",
            "loss: 0.580877  [17400/467875]\n",
            "loss: 0.408158  [20300/467875]\n",
            "loss: 0.664945  [23200/467875]\n",
            "loss: 0.490664  [26100/467875]\n",
            "loss: 0.381830  [29000/467875]\n",
            "loss: 0.427354  [31900/467875]\n",
            "loss: 0.452312  [34800/467875]\n",
            "loss: 0.599347  [37700/467875]\n",
            "loss: 0.535905  [40600/467875]\n",
            "loss: 0.446377  [43500/467875]\n",
            "loss: 0.446668  [46400/467875]\n",
            "loss: 0.545021  [49300/467875]\n",
            "loss: 0.573780  [52200/467875]\n",
            "loss: 0.567368  [55100/467875]\n",
            "loss: 0.450951  [58000/467875]\n",
            "loss: 0.585456  [60900/467875]\n",
            "loss: 0.541598  [63800/467875]\n",
            "loss: 0.516462  [66700/467875]\n",
            "loss: 0.509873  [69600/467875]\n",
            "loss: 0.563120  [72500/467875]\n",
            "loss: 0.562541  [75400/467875]\n",
            "loss: 0.597683  [78300/467875]\n",
            "loss: 0.530856  [81200/467875]\n",
            "loss: 0.485531  [84100/467875]\n",
            "loss: 0.619148  [87000/467875]\n",
            "loss: 0.525776  [89900/467875]\n",
            "loss: 0.551822  [92800/467875]\n",
            "loss: 0.448222  [95700/467875]\n",
            "loss: 0.459126  [98600/467875]\n",
            "loss: 0.463643  [101500/467875]\n",
            "loss: 0.494838  [104400/467875]\n",
            "loss: 0.571680  [107300/467875]\n",
            "loss: 0.697186  [110200/467875]\n",
            "loss: 0.517682  [113100/467875]\n",
            "loss: 0.445560  [116000/467875]\n",
            "loss: 0.655055  [118900/467875]\n",
            "loss: 0.422579  [121800/467875]\n",
            "loss: 0.494139  [124700/467875]\n",
            "loss: 0.526710  [127600/467875]\n",
            "loss: 0.493662  [130500/467875]\n",
            "loss: 0.523367  [133400/467875]\n",
            "loss: 0.489422  [136300/467875]\n",
            "loss: 0.613120  [139200/467875]\n",
            "loss: 0.445582  [142100/467875]\n",
            "loss: 0.537024  [145000/467875]\n",
            "loss: 0.456850  [147900/467875]\n",
            "loss: 0.542326  [150800/467875]\n",
            "loss: 0.508586  [153700/467875]\n",
            "loss: 0.548497  [156600/467875]\n",
            "loss: 0.548407  [159500/467875]\n",
            "loss: 0.421136  [162400/467875]\n",
            "loss: 0.603157  [165300/467875]\n",
            "loss: 0.463154  [168200/467875]\n",
            "loss: 0.640817  [171100/467875]\n",
            "loss: 0.524529  [174000/467875]\n",
            "loss: 0.557654  [176900/467875]\n",
            "loss: 0.468782  [179800/467875]\n",
            "loss: 0.561277  [182700/467875]\n",
            "loss: 0.703901  [185600/467875]\n",
            "loss: 0.507905  [188500/467875]\n",
            "loss: 0.554902  [191400/467875]\n",
            "loss: 0.541138  [194300/467875]\n",
            "loss: 0.569650  [197200/467875]\n",
            "loss: 0.557265  [200100/467875]\n",
            "loss: 0.615370  [203000/467875]\n",
            "loss: 0.522116  [205900/467875]\n",
            "loss: 0.595820  [208800/467875]\n",
            "loss: 0.502907  [211700/467875]\n",
            "loss: 0.487085  [214600/467875]\n",
            "loss: 0.582297  [217500/467875]\n",
            "loss: 0.460648  [220400/467875]\n",
            "loss: 0.454968  [223300/467875]\n",
            "loss: 0.577358  [226200/467875]\n",
            "loss: 0.522062  [229100/467875]\n",
            "loss: 0.501058  [232000/467875]\n",
            "loss: 0.467442  [234900/467875]\n",
            "loss: 0.531191  [237800/467875]\n",
            "loss: 0.505950  [240700/467875]\n",
            "loss: 0.500771  [243600/467875]\n",
            "loss: 0.648578  [246500/467875]\n",
            "loss: 0.472215  [249400/467875]\n",
            "loss: 0.481267  [252300/467875]\n",
            "loss: 0.536370  [255200/467875]\n",
            "loss: 0.510763  [258100/467875]\n",
            "loss: 0.506282  [261000/467875]\n",
            "loss: 0.422674  [263900/467875]\n",
            "loss: 0.516088  [266800/467875]\n",
            "loss: 0.534381  [269700/467875]\n",
            "loss: 0.469368  [272600/467875]\n",
            "loss: 0.540829  [275500/467875]\n",
            "loss: 0.470729  [278400/467875]\n",
            "loss: 0.474861  [281300/467875]\n",
            "loss: 0.449052  [284200/467875]\n",
            "loss: 0.538031  [287100/467875]\n",
            "loss: 0.524435  [290000/467875]\n",
            "loss: 0.587234  [292900/467875]\n",
            "loss: 0.480609  [295800/467875]\n",
            "loss: 0.425768  [298700/467875]\n",
            "loss: 0.519748  [301600/467875]\n",
            "loss: 0.412698  [304500/467875]\n",
            "loss: 0.617361  [307400/467875]\n",
            "loss: 0.538018  [310300/467875]\n",
            "loss: 0.376657  [313200/467875]\n",
            "loss: 0.570449  [316100/467875]\n",
            "loss: 0.650475  [319000/467875]\n",
            "loss: 0.571537  [321900/467875]\n",
            "loss: 0.696683  [324800/467875]\n",
            "loss: 0.495522  [327700/467875]\n",
            "loss: 0.515108  [330600/467875]\n",
            "loss: 0.623474  [333500/467875]\n",
            "loss: 0.491842  [336400/467875]\n",
            "loss: 0.483830  [339300/467875]\n",
            "loss: 0.459803  [342200/467875]\n",
            "loss: 0.504645  [345100/467875]\n",
            "loss: 0.397559  [348000/467875]\n",
            "loss: 0.467976  [350900/467875]\n",
            "loss: 0.469492  [353800/467875]\n",
            "loss: 0.544363  [356700/467875]\n",
            "loss: 0.585431  [359600/467875]\n",
            "loss: 0.454023  [362500/467875]\n",
            "loss: 0.548484  [365400/467875]\n",
            "loss: 0.533747  [368300/467875]\n",
            "loss: 0.600365  [371200/467875]\n",
            "loss: 0.489277  [374100/467875]\n",
            "loss: 0.487102  [377000/467875]\n",
            "loss: 0.473447  [379900/467875]\n",
            "loss: 0.514968  [382800/467875]\n",
            "loss: 0.530894  [385700/467875]\n",
            "loss: 0.515188  [388600/467875]\n",
            "loss: 0.470327  [391500/467875]\n",
            "loss: 0.512704  [394400/467875]\n",
            "loss: 0.466699  [397300/467875]\n",
            "loss: 0.411299  [400200/467875]\n",
            "loss: 0.528814  [403100/467875]\n",
            "loss: 0.524895  [406000/467875]\n",
            "loss: 0.600205  [408900/467875]\n",
            "loss: 0.576937  [411800/467875]\n",
            "loss: 0.602299  [414700/467875]\n",
            "loss: 0.485833  [417600/467875]\n",
            "loss: 0.474930  [420500/467875]\n",
            "loss: 0.530788  [423400/467875]\n",
            "loss: 0.509580  [426300/467875]\n",
            "loss: 0.487184  [429200/467875]\n",
            "loss: 0.430083  [432100/467875]\n",
            "loss: 0.523118  [435000/467875]\n",
            "loss: 0.430186  [437900/467875]\n",
            "loss: 0.533730  [440800/467875]\n",
            "loss: 0.571716  [443700/467875]\n",
            "loss: 0.504372  [446600/467875]\n",
            "loss: 0.626450  [449500/467875]\n",
            "loss: 0.424671  [452400/467875]\n",
            "loss: 0.497631  [455300/467875]\n",
            "loss: 0.413167  [458200/467875]\n",
            "loss: 0.639676  [461100/467875]\n",
            "loss: 0.531416  [464000/467875]\n",
            "loss: 0.523794  [466900/467875]\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.523132 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 5 s\n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.617602  [    0/467875]\n",
            "loss: 0.598122  [ 2900/467875]\n",
            "loss: 0.550996  [ 5800/467875]\n",
            "loss: 0.509857  [ 8700/467875]\n",
            "loss: 0.675283  [11600/467875]\n",
            "loss: 0.583119  [14500/467875]\n",
            "loss: 0.467032  [17400/467875]\n",
            "loss: 0.496832  [20300/467875]\n",
            "loss: 0.549260  [23200/467875]\n",
            "loss: 0.551564  [26100/467875]\n",
            "loss: 0.521426  [29000/467875]\n",
            "loss: 0.442369  [31900/467875]\n",
            "loss: 0.543110  [34800/467875]\n",
            "loss: 0.474131  [37700/467875]\n",
            "loss: 0.467316  [40600/467875]\n",
            "loss: 0.575147  [43500/467875]\n",
            "loss: 0.490526  [46400/467875]\n",
            "loss: 0.448198  [49300/467875]\n",
            "loss: 0.546191  [52200/467875]\n",
            "loss: 0.424687  [55100/467875]\n",
            "loss: 0.571955  [58000/467875]\n",
            "loss: 0.418598  [60900/467875]\n",
            "loss: 0.581936  [63800/467875]\n",
            "loss: 0.511260  [66700/467875]\n",
            "loss: 0.633592  [69600/467875]\n",
            "loss: 0.564738  [72500/467875]\n",
            "loss: 0.526378  [75400/467875]\n",
            "loss: 0.496169  [78300/467875]\n",
            "loss: 0.525231  [81200/467875]\n",
            "loss: 0.677964  [84100/467875]\n",
            "loss: 0.495740  [87000/467875]\n",
            "loss: 0.515167  [89900/467875]\n",
            "loss: 0.564349  [92800/467875]\n",
            "loss: 0.659009  [95700/467875]\n",
            "loss: 0.434786  [98600/467875]\n",
            "loss: 0.577104  [101500/467875]\n",
            "loss: 0.410656  [104400/467875]\n",
            "loss: 0.540951  [107300/467875]\n",
            "loss: 0.477667  [110200/467875]\n",
            "loss: 0.585478  [113100/467875]\n",
            "loss: 0.496265  [116000/467875]\n",
            "loss: 0.596530  [118900/467875]\n",
            "loss: 0.501763  [121800/467875]\n",
            "loss: 0.454492  [124700/467875]\n",
            "loss: 0.456564  [127600/467875]\n",
            "loss: 0.688084  [130500/467875]\n",
            "loss: 0.408658  [133400/467875]\n",
            "loss: 0.569405  [136300/467875]\n",
            "loss: 0.446300  [139200/467875]\n",
            "loss: 0.548220  [142100/467875]\n",
            "loss: 0.535537  [145000/467875]\n",
            "loss: 0.529928  [147900/467875]\n",
            "loss: 0.507831  [150800/467875]\n",
            "loss: 0.477816  [153700/467875]\n",
            "loss: 0.541329  [156600/467875]\n",
            "loss: 0.461289  [159500/467875]\n",
            "loss: 0.575702  [162400/467875]\n",
            "loss: 0.539568  [165300/467875]\n",
            "loss: 0.531739  [168200/467875]\n",
            "loss: 0.471510  [171100/467875]\n",
            "loss: 0.548323  [174000/467875]\n",
            "loss: 0.524068  [176900/467875]\n",
            "loss: 0.406874  [179800/467875]\n",
            "loss: 0.497034  [182700/467875]\n",
            "loss: 0.582794  [185600/467875]\n",
            "loss: 0.533900  [188500/467875]\n",
            "loss: 0.553578  [191400/467875]\n",
            "loss: 0.380881  [194300/467875]\n",
            "loss: 0.555686  [197200/467875]\n",
            "loss: 0.491390  [200100/467875]\n",
            "loss: 0.533501  [203000/467875]\n",
            "loss: 0.465991  [205900/467875]\n",
            "loss: 0.470029  [208800/467875]\n",
            "loss: 0.578970  [211700/467875]\n",
            "loss: 0.512786  [214600/467875]\n",
            "loss: 0.562390  [217500/467875]\n",
            "loss: 0.455931  [220400/467875]\n",
            "loss: 0.440192  [223300/467875]\n",
            "loss: 0.554930  [226200/467875]\n",
            "loss: 0.571234  [229100/467875]\n",
            "loss: 0.456888  [232000/467875]\n",
            "loss: 0.571763  [234900/467875]\n",
            "loss: 0.425499  [237800/467875]\n",
            "loss: 0.578416  [240700/467875]\n",
            "loss: 0.387470  [243600/467875]\n",
            "loss: 0.464618  [246500/467875]\n",
            "loss: 0.421518  [249400/467875]\n",
            "loss: 0.502442  [252300/467875]\n",
            "loss: 0.550969  [255200/467875]\n",
            "loss: 0.711765  [258100/467875]\n",
            "loss: 0.495934  [261000/467875]\n",
            "loss: 0.621859  [263900/467875]\n",
            "loss: 0.573273  [266800/467875]\n",
            "loss: 0.561094  [269700/467875]\n",
            "loss: 0.524483  [272600/467875]\n",
            "loss: 0.430149  [275500/467875]\n",
            "loss: 0.670376  [278400/467875]\n",
            "loss: 0.545438  [281300/467875]\n",
            "loss: 0.482514  [284200/467875]\n",
            "loss: 0.531226  [287100/467875]\n",
            "loss: 0.588398  [290000/467875]\n",
            "loss: 0.394578  [292900/467875]\n",
            "loss: 0.406062  [295800/467875]\n",
            "loss: 0.517607  [298700/467875]\n",
            "loss: 0.534012  [301600/467875]\n",
            "loss: 0.482448  [304500/467875]\n",
            "loss: 0.549878  [307400/467875]\n",
            "loss: 0.401524  [310300/467875]\n",
            "loss: 0.583440  [313200/467875]\n",
            "loss: 0.438507  [316100/467875]\n",
            "loss: 0.665615  [319000/467875]\n",
            "loss: 0.494514  [321900/467875]\n",
            "loss: 0.500103  [324800/467875]\n",
            "loss: 0.424818  [327700/467875]\n",
            "loss: 0.527209  [330600/467875]\n",
            "loss: 0.535716  [333500/467875]\n",
            "loss: 0.581161  [336400/467875]\n",
            "loss: 0.627063  [339300/467875]\n",
            "loss: 0.528506  [342200/467875]\n",
            "loss: 0.511911  [345100/467875]\n",
            "loss: 0.533347  [348000/467875]\n",
            "loss: 0.602038  [350900/467875]\n",
            "loss: 0.576293  [353800/467875]\n",
            "loss: 0.523725  [356700/467875]\n",
            "loss: 0.524906  [359600/467875]\n",
            "loss: 0.598443  [362500/467875]\n",
            "loss: 0.568096  [365400/467875]\n",
            "loss: 0.568548  [368300/467875]\n",
            "loss: 0.529647  [371200/467875]\n",
            "loss: 0.498648  [374100/467875]\n",
            "loss: 0.510845  [377000/467875]\n",
            "loss: 0.606004  [379900/467875]\n",
            "loss: 0.536221  [382800/467875]\n",
            "loss: 0.477138  [385700/467875]\n",
            "loss: 0.470096  [388600/467875]\n",
            "loss: 0.548326  [391500/467875]\n",
            "loss: 0.534680  [394400/467875]\n",
            "loss: 0.480185  [397300/467875]\n",
            "loss: 0.521911  [400200/467875]\n",
            "loss: 0.624213  [403100/467875]\n",
            "loss: 0.579707  [406000/467875]\n",
            "loss: 0.494722  [408900/467875]\n",
            "loss: 0.516988  [411800/467875]\n",
            "loss: 0.488298  [414700/467875]\n",
            "loss: 0.474498  [417600/467875]\n",
            "loss: 0.473648  [420500/467875]\n",
            "loss: 0.624699  [423400/467875]\n",
            "loss: 0.546059  [426300/467875]\n",
            "loss: 0.609282  [429200/467875]\n",
            "loss: 0.600976  [432100/467875]\n",
            "loss: 0.565348  [435000/467875]\n",
            "loss: 0.461050  [437900/467875]\n",
            "loss: 0.532827  [440800/467875]\n",
            "loss: 0.519399  [443700/467875]\n",
            "loss: 0.453385  [446600/467875]\n",
            "loss: 0.559938  [449500/467875]\n",
            "loss: 0.530289  [452400/467875]\n",
            "loss: 0.479064  [455300/467875]\n",
            "loss: 0.470933  [458200/467875]\n",
            "loss: 0.507499  [461100/467875]\n",
            "loss: 0.596419  [464000/467875]\n",
            "loss: 0.626220  [466900/467875]\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.523443 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 4 s\n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.625473  [    0/467875]\n",
            "loss: 0.558731  [ 2900/467875]\n",
            "loss: 0.534444  [ 5800/467875]\n",
            "loss: 0.470113  [ 8700/467875]\n",
            "loss: 0.437847  [11600/467875]\n",
            "loss: 0.595625  [14500/467875]\n",
            "loss: 0.497434  [17400/467875]\n",
            "loss: 0.543360  [20300/467875]\n",
            "loss: 0.449070  [23200/467875]\n",
            "loss: 0.685520  [26100/467875]\n",
            "loss: 0.423502  [29000/467875]\n",
            "loss: 0.453722  [31900/467875]\n",
            "loss: 0.625291  [34800/467875]\n",
            "loss: 0.525735  [37700/467875]\n",
            "loss: 0.602339  [40600/467875]\n",
            "loss: 0.595013  [43500/467875]\n",
            "loss: 0.539681  [46400/467875]\n",
            "loss: 0.496839  [49300/467875]\n",
            "loss: 0.524997  [52200/467875]\n",
            "loss: 0.528955  [55100/467875]\n",
            "loss: 0.418511  [58000/467875]\n",
            "loss: 0.584294  [60900/467875]\n",
            "loss: 0.390175  [63800/467875]\n",
            "loss: 0.451913  [66700/467875]\n",
            "loss: 0.437523  [69600/467875]\n",
            "loss: 0.426983  [72500/467875]\n",
            "loss: 0.583638  [75400/467875]\n",
            "loss: 0.586562  [78300/467875]\n",
            "loss: 0.494864  [81200/467875]\n",
            "loss: 0.556287  [84100/467875]\n",
            "loss: 0.568668  [87000/467875]\n",
            "loss: 0.598332  [89900/467875]\n",
            "loss: 0.479620  [92800/467875]\n",
            "loss: 0.595981  [95700/467875]\n",
            "loss: 0.635268  [98600/467875]\n",
            "loss: 0.458532  [101500/467875]\n",
            "loss: 0.460033  [104400/467875]\n",
            "loss: 0.557625  [107300/467875]\n",
            "loss: 0.559711  [110200/467875]\n",
            "loss: 0.517733  [113100/467875]\n",
            "loss: 0.482416  [116000/467875]\n",
            "loss: 0.496207  [118900/467875]\n",
            "loss: 0.595952  [121800/467875]\n",
            "loss: 0.519238  [124700/467875]\n",
            "loss: 0.662714  [127600/467875]\n",
            "loss: 0.508466  [130500/467875]\n",
            "loss: 0.496589  [133400/467875]\n",
            "loss: 0.462193  [136300/467875]\n",
            "loss: 0.490521  [139200/467875]\n",
            "loss: 0.559603  [142100/467875]\n",
            "loss: 0.547553  [145000/467875]\n",
            "loss: 0.427302  [147900/467875]\n",
            "loss: 0.485410  [150800/467875]\n",
            "loss: 0.466420  [153700/467875]\n",
            "loss: 0.488782  [156600/467875]\n",
            "loss: 0.527600  [159500/467875]\n",
            "loss: 0.499925  [162400/467875]\n",
            "loss: 0.452059  [165300/467875]\n",
            "loss: 0.413842  [168200/467875]\n",
            "loss: 0.605084  [171100/467875]\n",
            "loss: 0.512238  [174000/467875]\n",
            "loss: 0.379886  [176900/467875]\n",
            "loss: 0.526416  [179800/467875]\n",
            "loss: 0.589874  [182700/467875]\n",
            "loss: 0.502875  [185600/467875]\n",
            "loss: 0.472484  [188500/467875]\n",
            "loss: 0.457461  [191400/467875]\n",
            "loss: 0.423012  [194300/467875]\n",
            "loss: 0.581778  [197200/467875]\n",
            "loss: 0.567772  [200100/467875]\n",
            "loss: 0.499881  [203000/467875]\n",
            "loss: 0.541103  [205900/467875]\n",
            "loss: 0.441313  [208800/467875]\n",
            "loss: 0.593391  [211700/467875]\n",
            "loss: 0.415326  [214600/467875]\n",
            "loss: 0.485551  [217500/467875]\n",
            "loss: 0.540920  [220400/467875]\n",
            "loss: 0.627808  [223300/467875]\n",
            "loss: 0.502894  [226200/467875]\n",
            "loss: 0.472868  [229100/467875]\n",
            "loss: 0.509669  [232000/467875]\n",
            "loss: 0.568137  [234900/467875]\n",
            "loss: 0.485925  [237800/467875]\n",
            "loss: 0.546385  [240700/467875]\n",
            "loss: 0.410429  [243600/467875]\n",
            "loss: 0.596515  [246500/467875]\n",
            "loss: 0.485300  [249400/467875]\n",
            "loss: 0.603594  [252300/467875]\n",
            "loss: 0.474632  [255200/467875]\n",
            "loss: 0.510935  [258100/467875]\n",
            "loss: 0.460543  [261000/467875]\n",
            "loss: 0.477914  [263900/467875]\n",
            "loss: 0.503584  [266800/467875]\n",
            "loss: 0.505898  [269700/467875]\n",
            "loss: 0.447053  [272600/467875]\n",
            "loss: 0.555918  [275500/467875]\n",
            "loss: 0.512121  [278400/467875]\n",
            "loss: 0.516563  [281300/467875]\n",
            "loss: 0.550621  [284200/467875]\n",
            "loss: 0.668066  [287100/467875]\n",
            "loss: 0.462077  [290000/467875]\n",
            "loss: 0.472417  [292900/467875]\n",
            "loss: 0.615875  [295800/467875]\n",
            "loss: 0.451427  [298700/467875]\n",
            "loss: 0.402248  [301600/467875]\n",
            "loss: 0.555910  [304500/467875]\n",
            "loss: 0.487223  [307400/467875]\n",
            "loss: 0.498947  [310300/467875]\n",
            "loss: 0.497718  [313200/467875]\n",
            "loss: 0.596534  [316100/467875]\n",
            "loss: 0.524669  [319000/467875]\n",
            "loss: 0.550774  [321900/467875]\n",
            "loss: 0.527289  [324800/467875]\n",
            "loss: 0.540430  [327700/467875]\n",
            "loss: 0.540274  [330600/467875]\n",
            "loss: 0.584822  [333500/467875]\n",
            "loss: 0.435551  [336400/467875]\n",
            "loss: 0.531500  [339300/467875]\n",
            "loss: 0.443840  [342200/467875]\n",
            "loss: 0.385909  [345100/467875]\n",
            "loss: 0.397168  [348000/467875]\n",
            "loss: 0.528796  [350900/467875]\n",
            "loss: 0.634296  [353800/467875]\n",
            "loss: 0.553629  [356700/467875]\n",
            "loss: 0.560137  [359600/467875]\n",
            "loss: 0.460953  [362500/467875]\n",
            "loss: 0.446032  [365400/467875]\n",
            "loss: 0.577803  [368300/467875]\n",
            "loss: 0.536669  [371200/467875]\n",
            "loss: 0.494019  [374100/467875]\n",
            "loss: 0.539091  [377000/467875]\n",
            "loss: 0.551522  [379900/467875]\n",
            "loss: 0.547990  [382800/467875]\n",
            "loss: 0.518268  [385700/467875]\n",
            "loss: 0.476425  [388600/467875]\n",
            "loss: 0.459602  [391500/467875]\n",
            "loss: 0.328785  [394400/467875]\n",
            "loss: 0.471893  [397300/467875]\n",
            "loss: 0.444496  [400200/467875]\n",
            "loss: 0.555523  [403100/467875]\n",
            "loss: 0.497733  [406000/467875]\n",
            "loss: 0.457306  [408900/467875]\n",
            "loss: 0.423248  [411800/467875]\n",
            "loss: 0.432893  [414700/467875]\n",
            "loss: 0.596757  [417600/467875]\n",
            "loss: 0.500109  [420500/467875]\n",
            "loss: 0.520510  [423400/467875]\n",
            "loss: 0.499788  [426300/467875]\n",
            "loss: 0.629108  [429200/467875]\n",
            "loss: 0.419922  [432100/467875]\n",
            "loss: 0.421662  [435000/467875]\n",
            "loss: 0.581708  [437900/467875]\n",
            "loss: 0.644155  [440800/467875]\n",
            "loss: 0.562102  [443700/467875]\n",
            "loss: 0.537072  [446600/467875]\n",
            "loss: 0.532601  [449500/467875]\n",
            "loss: 0.364192  [452400/467875]\n",
            "loss: 0.519360  [455300/467875]\n",
            "loss: 0.597333  [458200/467875]\n",
            "loss: 0.571597  [461100/467875]\n",
            "loss: 0.570139  [464000/467875]\n",
            "loss: 0.545791  [466900/467875]\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.522505 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 6 s\n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.472545  [    0/467875]\n",
            "loss: 0.554561  [ 2900/467875]\n",
            "loss: 0.620510  [ 5800/467875]\n",
            "loss: 0.437051  [ 8700/467875]\n",
            "loss: 0.527539  [11600/467875]\n",
            "loss: 0.455981  [14500/467875]\n",
            "loss: 0.502042  [17400/467875]\n",
            "loss: 0.492974  [20300/467875]\n",
            "loss: 0.633365  [23200/467875]\n",
            "loss: 0.480667  [26100/467875]\n",
            "loss: 0.495151  [29000/467875]\n",
            "loss: 0.498164  [31900/467875]\n",
            "loss: 0.580510  [34800/467875]\n",
            "loss: 0.552246  [37700/467875]\n",
            "loss: 0.611979  [40600/467875]\n",
            "loss: 0.548025  [43500/467875]\n",
            "loss: 0.598616  [46400/467875]\n",
            "loss: 0.574190  [49300/467875]\n",
            "loss: 0.397091  [52200/467875]\n",
            "loss: 0.613941  [55100/467875]\n",
            "loss: 0.506998  [58000/467875]\n",
            "loss: 0.602968  [60900/467875]\n",
            "loss: 0.559274  [63800/467875]\n",
            "loss: 0.463750  [66700/467875]\n",
            "loss: 0.581874  [69600/467875]\n",
            "loss: 0.545082  [72500/467875]\n",
            "loss: 0.509517  [75400/467875]\n",
            "loss: 0.645688  [78300/467875]\n",
            "loss: 0.494720  [81200/467875]\n",
            "loss: 0.611533  [84100/467875]\n",
            "loss: 0.510101  [87000/467875]\n",
            "loss: 0.393098  [89900/467875]\n",
            "loss: 0.476847  [92800/467875]\n",
            "loss: 0.525622  [95700/467875]\n",
            "loss: 0.493774  [98600/467875]\n",
            "loss: 0.628483  [101500/467875]\n",
            "loss: 0.475663  [104400/467875]\n",
            "loss: 0.560593  [107300/467875]\n",
            "loss: 0.631379  [110200/467875]\n",
            "loss: 0.543276  [113100/467875]\n",
            "loss: 0.412412  [116000/467875]\n",
            "loss: 0.550496  [118900/467875]\n",
            "loss: 0.473296  [121800/467875]\n",
            "loss: 0.538082  [124700/467875]\n",
            "loss: 0.470831  [127600/467875]\n",
            "loss: 0.548693  [130500/467875]\n",
            "loss: 0.569051  [133400/467875]\n",
            "loss: 0.561364  [136300/467875]\n",
            "loss: 0.468396  [139200/467875]\n",
            "loss: 0.518724  [142100/467875]\n",
            "loss: 0.456692  [145000/467875]\n",
            "loss: 0.629351  [147900/467875]\n",
            "loss: 0.537944  [150800/467875]\n",
            "loss: 0.483215  [153700/467875]\n",
            "loss: 0.476788  [156600/467875]\n",
            "loss: 0.529326  [159500/467875]\n",
            "loss: 0.481244  [162400/467875]\n",
            "loss: 0.503062  [165300/467875]\n",
            "loss: 0.452582  [168200/467875]\n",
            "loss: 0.452793  [171100/467875]\n",
            "loss: 0.524982  [174000/467875]\n",
            "loss: 0.534508  [176900/467875]\n",
            "loss: 0.577719  [179800/467875]\n",
            "loss: 0.539683  [182700/467875]\n",
            "loss: 0.512153  [185600/467875]\n",
            "loss: 0.486686  [188500/467875]\n",
            "loss: 0.559024  [191400/467875]\n",
            "loss: 0.475765  [194300/467875]\n",
            "loss: 0.544466  [197200/467875]\n",
            "loss: 0.459926  [200100/467875]\n",
            "loss: 0.457064  [203000/467875]\n",
            "loss: 0.449910  [205900/467875]\n",
            "loss: 0.541507  [208800/467875]\n",
            "loss: 0.550428  [211700/467875]\n",
            "loss: 0.508289  [214600/467875]\n",
            "loss: 0.515114  [217500/467875]\n",
            "loss: 0.625139  [220400/467875]\n",
            "loss: 0.525485  [223300/467875]\n",
            "loss: 0.577159  [226200/467875]\n",
            "loss: 0.549271  [229100/467875]\n",
            "loss: 0.489058  [232000/467875]\n",
            "loss: 0.373863  [234900/467875]\n",
            "loss: 0.560219  [237800/467875]\n",
            "loss: 0.496174  [240700/467875]\n",
            "loss: 0.558738  [243600/467875]\n",
            "loss: 0.414750  [246500/467875]\n",
            "loss: 0.387755  [249400/467875]\n",
            "loss: 0.639544  [252300/467875]\n",
            "loss: 0.445352  [255200/467875]\n",
            "loss: 0.538440  [258100/467875]\n",
            "loss: 0.522043  [261000/467875]\n",
            "loss: 0.548097  [263900/467875]\n",
            "loss: 0.523851  [266800/467875]\n",
            "loss: 0.571878  [269700/467875]\n",
            "loss: 0.473006  [272600/467875]\n",
            "loss: 0.414311  [275500/467875]\n",
            "loss: 0.563658  [278400/467875]\n",
            "loss: 0.474698  [281300/467875]\n",
            "loss: 0.468771  [284200/467875]\n",
            "loss: 0.445009  [287100/467875]\n",
            "loss: 0.610262  [290000/467875]\n",
            "loss: 0.595385  [292900/467875]\n",
            "loss: 0.517796  [295800/467875]\n",
            "loss: 0.692217  [298700/467875]\n",
            "loss: 0.563599  [301600/467875]\n",
            "loss: 0.567486  [304500/467875]\n",
            "loss: 0.549091  [307400/467875]\n",
            "loss: 0.552158  [310300/467875]\n",
            "loss: 0.604064  [313200/467875]\n",
            "loss: 0.479012  [316100/467875]\n",
            "loss: 0.592718  [319000/467875]\n",
            "loss: 0.526087  [321900/467875]\n",
            "loss: 0.475696  [324800/467875]\n",
            "loss: 0.496821  [327700/467875]\n",
            "loss: 0.483458  [330600/467875]\n",
            "loss: 0.486816  [333500/467875]\n",
            "loss: 0.510935  [336400/467875]\n",
            "loss: 0.410443  [339300/467875]\n",
            "loss: 0.525463  [342200/467875]\n",
            "loss: 0.612142  [345100/467875]\n",
            "loss: 0.549302  [348000/467875]\n",
            "loss: 0.560727  [350900/467875]\n",
            "loss: 0.528452  [353800/467875]\n",
            "loss: 0.524571  [356700/467875]\n",
            "loss: 0.484789  [359600/467875]\n",
            "loss: 0.497962  [362500/467875]\n",
            "loss: 0.613169  [365400/467875]\n",
            "loss: 0.509342  [368300/467875]\n",
            "loss: 0.468040  [371200/467875]\n",
            "loss: 0.477765  [374100/467875]\n",
            "loss: 0.533780  [377000/467875]\n",
            "loss: 0.503994  [379900/467875]\n",
            "loss: 0.491820  [382800/467875]\n",
            "loss: 0.477138  [385700/467875]\n",
            "loss: 0.570630  [388600/467875]\n",
            "loss: 0.671113  [391500/467875]\n",
            "loss: 0.469751  [394400/467875]\n",
            "loss: 0.587166  [397300/467875]\n",
            "loss: 0.533746  [400200/467875]\n",
            "loss: 0.603284  [403100/467875]\n",
            "loss: 0.514707  [406000/467875]\n",
            "loss: 0.566449  [408900/467875]\n",
            "loss: 0.498829  [411800/467875]\n",
            "loss: 0.469432  [414700/467875]\n",
            "loss: 0.450369  [417600/467875]\n",
            "loss: 0.460300  [420500/467875]\n",
            "loss: 0.622078  [423400/467875]\n",
            "loss: 0.565072  [426300/467875]\n",
            "loss: 0.418344  [429200/467875]\n",
            "loss: 0.458740  [432100/467875]\n",
            "loss: 0.458867  [435000/467875]\n",
            "loss: 0.568305  [437900/467875]\n",
            "loss: 0.497078  [440800/467875]\n",
            "loss: 0.480267  [443700/467875]\n",
            "loss: 0.508538  [446600/467875]\n",
            "loss: 0.554576  [449500/467875]\n",
            "loss: 0.483412  [452400/467875]\n",
            "loss: 0.548712  [455300/467875]\n",
            "loss: 0.541634  [458200/467875]\n",
            "loss: 0.538315  [461100/467875]\n",
            "loss: 0.570062  [464000/467875]\n",
            "loss: 0.498288  [466900/467875]\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522944 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 5 s\n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.601022  [    0/467875]\n",
            "loss: 0.468817  [ 2900/467875]\n",
            "loss: 0.451477  [ 5800/467875]\n",
            "loss: 0.458998  [ 8700/467875]\n",
            "loss: 0.561526  [11600/467875]\n",
            "loss: 0.464455  [14500/467875]\n",
            "loss: 0.522256  [17400/467875]\n",
            "loss: 0.549080  [20300/467875]\n",
            "loss: 0.564503  [23200/467875]\n",
            "loss: 0.604951  [26100/467875]\n",
            "loss: 0.538126  [29000/467875]\n",
            "loss: 0.522964  [31900/467875]\n",
            "loss: 0.467742  [34800/467875]\n",
            "loss: 0.498918  [37700/467875]\n",
            "loss: 0.436132  [40600/467875]\n",
            "loss: 0.545985  [43500/467875]\n",
            "loss: 0.659096  [46400/467875]\n",
            "loss: 0.496134  [49300/467875]\n",
            "loss: 0.445075  [52200/467875]\n",
            "loss: 0.671419  [55100/467875]\n",
            "loss: 0.595521  [58000/467875]\n",
            "loss: 0.578035  [60900/467875]\n",
            "loss: 0.545499  [63800/467875]\n",
            "loss: 0.576944  [66700/467875]\n",
            "loss: 0.532213  [69600/467875]\n",
            "loss: 0.545666  [72500/467875]\n",
            "loss: 0.428254  [75400/467875]\n",
            "loss: 0.567979  [78300/467875]\n",
            "loss: 0.435659  [81200/467875]\n",
            "loss: 0.402876  [84100/467875]\n",
            "loss: 0.600922  [87000/467875]\n",
            "loss: 0.404983  [89900/467875]\n",
            "loss: 0.495646  [92800/467875]\n",
            "loss: 0.651178  [95700/467875]\n",
            "loss: 0.445986  [98600/467875]\n",
            "loss: 0.529929  [101500/467875]\n",
            "loss: 0.710461  [104400/467875]\n",
            "loss: 0.562395  [107300/467875]\n",
            "loss: 0.553913  [110200/467875]\n",
            "loss: 0.604259  [113100/467875]\n",
            "loss: 0.517953  [116000/467875]\n",
            "loss: 0.415727  [118900/467875]\n",
            "loss: 0.577145  [121800/467875]\n",
            "loss: 0.675890  [124700/467875]\n",
            "loss: 0.457831  [127600/467875]\n",
            "loss: 0.519468  [130500/467875]\n",
            "loss: 0.435903  [133400/467875]\n",
            "loss: 0.608106  [136300/467875]\n",
            "loss: 0.564015  [139200/467875]\n",
            "loss: 0.521395  [142100/467875]\n",
            "loss: 0.541734  [145000/467875]\n",
            "loss: 0.507581  [147900/467875]\n",
            "loss: 0.518608  [150800/467875]\n",
            "loss: 0.393326  [153700/467875]\n",
            "loss: 0.516013  [156600/467875]\n",
            "loss: 0.491761  [159500/467875]\n",
            "loss: 0.571814  [162400/467875]\n",
            "loss: 0.396272  [165300/467875]\n",
            "loss: 0.521339  [168200/467875]\n",
            "loss: 0.501588  [171100/467875]\n",
            "loss: 0.440397  [174000/467875]\n",
            "loss: 0.455503  [176900/467875]\n",
            "loss: 0.458101  [179800/467875]\n",
            "loss: 0.577345  [182700/467875]\n",
            "loss: 0.600452  [185600/467875]\n",
            "loss: 0.623662  [188500/467875]\n",
            "loss: 0.528125  [191400/467875]\n",
            "loss: 0.573476  [194300/467875]\n",
            "loss: 0.581442  [197200/467875]\n",
            "loss: 0.508734  [200100/467875]\n",
            "loss: 0.423114  [203000/467875]\n",
            "loss: 0.413613  [205900/467875]\n",
            "loss: 0.379936  [208800/467875]\n",
            "loss: 0.430118  [211700/467875]\n",
            "loss: 0.481110  [214600/467875]\n",
            "loss: 0.552374  [217500/467875]\n",
            "loss: 0.526801  [220400/467875]\n",
            "loss: 0.519795  [223300/467875]\n",
            "loss: 0.501305  [226200/467875]\n",
            "loss: 0.523348  [229100/467875]\n",
            "loss: 0.508573  [232000/467875]\n",
            "loss: 0.588818  [234900/467875]\n",
            "loss: 0.497739  [237800/467875]\n",
            "loss: 0.563916  [240700/467875]\n",
            "loss: 0.515949  [243600/467875]\n",
            "loss: 0.589297  [246500/467875]\n",
            "loss: 0.636209  [249400/467875]\n",
            "loss: 0.658372  [252300/467875]\n",
            "loss: 0.567667  [255200/467875]\n",
            "loss: 0.511832  [258100/467875]\n",
            "loss: 0.538032  [261000/467875]\n",
            "loss: 0.523015  [263900/467875]\n",
            "loss: 0.457449  [266800/467875]\n",
            "loss: 0.619658  [269700/467875]\n",
            "loss: 0.538070  [272600/467875]\n",
            "loss: 0.485627  [275500/467875]\n",
            "loss: 0.496924  [278400/467875]\n",
            "loss: 0.418020  [281300/467875]\n",
            "loss: 0.439963  [284200/467875]\n",
            "loss: 0.520873  [287100/467875]\n",
            "loss: 0.571686  [290000/467875]\n",
            "loss: 0.477536  [292900/467875]\n",
            "loss: 0.464387  [295800/467875]\n",
            "loss: 0.563054  [298700/467875]\n",
            "loss: 0.523215  [301600/467875]\n",
            "loss: 0.381946  [304500/467875]\n",
            "loss: 0.518653  [307400/467875]\n",
            "loss: 0.449385  [310300/467875]\n",
            "loss: 0.627095  [313200/467875]\n",
            "loss: 0.513309  [316100/467875]\n",
            "loss: 0.511593  [319000/467875]\n",
            "loss: 0.538372  [321900/467875]\n",
            "loss: 0.462574  [324800/467875]\n",
            "loss: 0.623022  [327700/467875]\n",
            "loss: 0.666539  [330600/467875]\n",
            "loss: 0.414585  [333500/467875]\n",
            "loss: 0.555933  [336400/467875]\n",
            "loss: 0.601463  [339300/467875]\n",
            "loss: 0.412914  [342200/467875]\n",
            "loss: 0.538918  [345100/467875]\n",
            "loss: 0.566455  [348000/467875]\n",
            "loss: 0.537175  [350900/467875]\n",
            "loss: 0.422745  [353800/467875]\n",
            "loss: 0.501014  [356700/467875]\n",
            "loss: 0.539137  [359600/467875]\n",
            "loss: 0.453515  [362500/467875]\n",
            "loss: 0.621519  [365400/467875]\n",
            "loss: 0.472829  [368300/467875]\n",
            "loss: 0.466893  [371200/467875]\n",
            "loss: 0.605463  [374100/467875]\n",
            "loss: 0.530181  [377000/467875]\n",
            "loss: 0.591977  [379900/467875]\n",
            "loss: 0.580289  [382800/467875]\n",
            "loss: 0.482137  [385700/467875]\n",
            "loss: 0.460966  [388600/467875]\n",
            "loss: 0.543981  [391500/467875]\n",
            "loss: 0.498835  [394400/467875]\n",
            "loss: 0.583412  [397300/467875]\n",
            "loss: 0.482532  [400200/467875]\n",
            "loss: 0.544511  [403100/467875]\n",
            "loss: 0.487228  [406000/467875]\n",
            "loss: 0.511956  [408900/467875]\n",
            "loss: 0.507227  [411800/467875]\n",
            "loss: 0.545243  [414700/467875]\n",
            "loss: 0.534343  [417600/467875]\n",
            "loss: 0.446625  [420500/467875]\n",
            "loss: 0.626914  [423400/467875]\n",
            "loss: 0.522808  [426300/467875]\n",
            "loss: 0.512138  [429200/467875]\n",
            "loss: 0.453653  [432100/467875]\n",
            "loss: 0.513332  [435000/467875]\n",
            "loss: 0.447805  [437900/467875]\n",
            "loss: 0.541365  [440800/467875]\n",
            "loss: 0.505716  [443700/467875]\n",
            "loss: 0.498853  [446600/467875]\n",
            "loss: 0.593634  [449500/467875]\n",
            "loss: 0.549034  [452400/467875]\n",
            "loss: 0.364604  [455300/467875]\n",
            "loss: 0.402185  [458200/467875]\n",
            "loss: 0.530501  [461100/467875]\n",
            "loss: 0.494036  [464000/467875]\n",
            "loss: 0.513236  [466900/467875]\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.524714 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 5 s\n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.531307  [    0/467875]\n",
            "loss: 0.474941  [ 2900/467875]\n",
            "loss: 0.519034  [ 5800/467875]\n",
            "loss: 0.522685  [ 8700/467875]\n",
            "loss: 0.552526  [11600/467875]\n",
            "loss: 0.571282  [14500/467875]\n",
            "loss: 0.576626  [17400/467875]\n",
            "loss: 0.474118  [20300/467875]\n",
            "loss: 0.485213  [23200/467875]\n",
            "loss: 0.549371  [26100/467875]\n",
            "loss: 0.476676  [29000/467875]\n",
            "loss: 0.590479  [31900/467875]\n",
            "loss: 0.476675  [34800/467875]\n",
            "loss: 0.519413  [37700/467875]\n",
            "loss: 0.477167  [40600/467875]\n",
            "loss: 0.614908  [43500/467875]\n",
            "loss: 0.584148  [46400/467875]\n",
            "loss: 0.387888  [49300/467875]\n",
            "loss: 0.522332  [52200/467875]\n",
            "loss: 0.514197  [55100/467875]\n",
            "loss: 0.616341  [58000/467875]\n",
            "loss: 0.488137  [60900/467875]\n",
            "loss: 0.535921  [63800/467875]\n",
            "loss: 0.598033  [66700/467875]\n",
            "loss: 0.531468  [69600/467875]\n",
            "loss: 0.535538  [72500/467875]\n",
            "loss: 0.584249  [75400/467875]\n",
            "loss: 0.519223  [78300/467875]\n",
            "loss: 0.502578  [81200/467875]\n",
            "loss: 0.481228  [84100/467875]\n",
            "loss: 0.554904  [87000/467875]\n",
            "loss: 0.554400  [89900/467875]\n",
            "loss: 0.480973  [92800/467875]\n",
            "loss: 0.577304  [95700/467875]\n",
            "loss: 0.551079  [98600/467875]\n",
            "loss: 0.549571  [101500/467875]\n",
            "loss: 0.489427  [104400/467875]\n",
            "loss: 0.641683  [107300/467875]\n",
            "loss: 0.556534  [110200/467875]\n",
            "loss: 0.565971  [113100/467875]\n",
            "loss: 0.456384  [116000/467875]\n",
            "loss: 0.567197  [118900/467875]\n",
            "loss: 0.486999  [121800/467875]\n",
            "loss: 0.564842  [124700/467875]\n",
            "loss: 0.549800  [127600/467875]\n",
            "loss: 0.571744  [130500/467875]\n",
            "loss: 0.402121  [133400/467875]\n",
            "loss: 0.455691  [136300/467875]\n",
            "loss: 0.526757  [139200/467875]\n",
            "loss: 0.433017  [142100/467875]\n",
            "loss: 0.586852  [145000/467875]\n",
            "loss: 0.581350  [147900/467875]\n",
            "loss: 0.460732  [150800/467875]\n",
            "loss: 0.580624  [153700/467875]\n",
            "loss: 0.643608  [156600/467875]\n",
            "loss: 0.501133  [159500/467875]\n",
            "loss: 0.636324  [162400/467875]\n",
            "loss: 0.449338  [165300/467875]\n",
            "loss: 0.597001  [168200/467875]\n",
            "loss: 0.412242  [171100/467875]\n",
            "loss: 0.587863  [174000/467875]\n",
            "loss: 0.485661  [176900/467875]\n",
            "loss: 0.494053  [179800/467875]\n",
            "loss: 0.532556  [182700/467875]\n",
            "loss: 0.383064  [185600/467875]\n",
            "loss: 0.397122  [188500/467875]\n",
            "loss: 0.521300  [191400/467875]\n",
            "loss: 0.652738  [194300/467875]\n",
            "loss: 0.480695  [197200/467875]\n",
            "loss: 0.478302  [200100/467875]\n",
            "loss: 0.598075  [203000/467875]\n",
            "loss: 0.476922  [205900/467875]\n",
            "loss: 0.543337  [208800/467875]\n",
            "loss: 0.445930  [211700/467875]\n",
            "loss: 0.570677  [214600/467875]\n",
            "loss: 0.479122  [217500/467875]\n",
            "loss: 0.493558  [220400/467875]\n",
            "loss: 0.528460  [223300/467875]\n",
            "loss: 0.495785  [226200/467875]\n",
            "loss: 0.568131  [229100/467875]\n",
            "loss: 0.550834  [232000/467875]\n",
            "loss: 0.520732  [234900/467875]\n",
            "loss: 0.599682  [237800/467875]\n",
            "loss: 0.584639  [240700/467875]\n",
            "loss: 0.523634  [243600/467875]\n",
            "loss: 0.483953  [246500/467875]\n",
            "loss: 0.489520  [249400/467875]\n",
            "loss: 0.653601  [252300/467875]\n",
            "loss: 0.576746  [255200/467875]\n",
            "loss: 0.492121  [258100/467875]\n",
            "loss: 0.524263  [261000/467875]\n",
            "loss: 0.474335  [263900/467875]\n",
            "loss: 0.463905  [266800/467875]\n",
            "loss: 0.584586  [269700/467875]\n",
            "loss: 0.520434  [272600/467875]\n",
            "loss: 0.454793  [275500/467875]\n",
            "loss: 0.481788  [278400/467875]\n",
            "loss: 0.478921  [281300/467875]\n",
            "loss: 0.566610  [284200/467875]\n",
            "loss: 0.503112  [287100/467875]\n",
            "loss: 0.469202  [290000/467875]\n",
            "loss: 0.477430  [292900/467875]\n",
            "loss: 0.564771  [295800/467875]\n",
            "loss: 0.539925  [298700/467875]\n",
            "loss: 0.628427  [301600/467875]\n",
            "loss: 0.434325  [304500/467875]\n",
            "loss: 0.563193  [307400/467875]\n",
            "loss: 0.705785  [310300/467875]\n",
            "loss: 0.536190  [313200/467875]\n",
            "loss: 0.592216  [316100/467875]\n",
            "loss: 0.448132  [319000/467875]\n",
            "loss: 0.365267  [321900/467875]\n",
            "loss: 0.491554  [324800/467875]\n",
            "loss: 0.495102  [327700/467875]\n",
            "loss: 0.633033  [330600/467875]\n",
            "loss: 0.667066  [333500/467875]\n",
            "loss: 0.441775  [336400/467875]\n",
            "loss: 0.504379  [339300/467875]\n",
            "loss: 0.665126  [342200/467875]\n",
            "loss: 0.542278  [345100/467875]\n",
            "loss: 0.531253  [348000/467875]\n",
            "loss: 0.356129  [350900/467875]\n",
            "loss: 0.516531  [353800/467875]\n",
            "loss: 0.557153  [356700/467875]\n",
            "loss: 0.503230  [359600/467875]\n",
            "loss: 0.497243  [362500/467875]\n",
            "loss: 0.509288  [365400/467875]\n",
            "loss: 0.467436  [368300/467875]\n",
            "loss: 0.549959  [371200/467875]\n",
            "loss: 0.435209  [374100/467875]\n",
            "loss: 0.563265  [377000/467875]\n",
            "loss: 0.518543  [379900/467875]\n",
            "loss: 0.377012  [382800/467875]\n",
            "loss: 0.508476  [385700/467875]\n",
            "loss: 0.609122  [388600/467875]\n",
            "loss: 0.592853  [391500/467875]\n",
            "loss: 0.607358  [394400/467875]\n",
            "loss: 0.436510  [397300/467875]\n",
            "loss: 0.553346  [400200/467875]\n",
            "loss: 0.452549  [403100/467875]\n",
            "loss: 0.520692  [406000/467875]\n",
            "loss: 0.590421  [408900/467875]\n",
            "loss: 0.496188  [411800/467875]\n",
            "loss: 0.416805  [414700/467875]\n",
            "loss: 0.506687  [417600/467875]\n",
            "loss: 0.459946  [420500/467875]\n",
            "loss: 0.563739  [423400/467875]\n",
            "loss: 0.677909  [426300/467875]\n",
            "loss: 0.445992  [429200/467875]\n",
            "loss: 0.480831  [432100/467875]\n",
            "loss: 0.431105  [435000/467875]\n",
            "loss: 0.524549  [437900/467875]\n",
            "loss: 0.550027  [440800/467875]\n",
            "loss: 0.459241  [443700/467875]\n",
            "loss: 0.536255  [446600/467875]\n",
            "loss: 0.408587  [449500/467875]\n",
            "loss: 0.437424  [452400/467875]\n",
            "loss: 0.566664  [455300/467875]\n",
            "loss: 0.516310  [458200/467875]\n",
            "loss: 0.499930  [461100/467875]\n",
            "loss: 0.451157  [464000/467875]\n",
            "loss: 0.549496  [466900/467875]\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.522609 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 5 s\n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.507630  [    0/467875]\n",
            "loss: 0.533777  [ 2900/467875]\n",
            "loss: 0.472676  [ 5800/467875]\n",
            "loss: 0.470919  [ 8700/467875]\n",
            "loss: 0.493202  [11600/467875]\n",
            "loss: 0.462326  [14500/467875]\n",
            "loss: 0.544925  [17400/467875]\n",
            "loss: 0.629716  [20300/467875]\n",
            "loss: 0.446243  [23200/467875]\n",
            "loss: 0.534191  [26100/467875]\n",
            "loss: 0.612175  [29000/467875]\n",
            "loss: 0.571047  [31900/467875]\n",
            "loss: 0.546982  [34800/467875]\n",
            "loss: 0.458132  [37700/467875]\n",
            "loss: 0.406850  [40600/467875]\n",
            "loss: 0.514510  [43500/467875]\n",
            "loss: 0.490775  [46400/467875]\n",
            "loss: 0.628620  [49300/467875]\n",
            "loss: 0.738774  [52200/467875]\n",
            "loss: 0.444729  [55100/467875]\n",
            "loss: 0.362838  [58000/467875]\n",
            "loss: 0.395698  [60900/467875]\n",
            "loss: 0.392919  [63800/467875]\n",
            "loss: 0.456813  [66700/467875]\n",
            "loss: 0.592084  [69600/467875]\n",
            "loss: 0.554766  [72500/467875]\n",
            "loss: 0.629324  [75400/467875]\n",
            "loss: 0.507722  [78300/467875]\n",
            "loss: 0.591028  [81200/467875]\n",
            "loss: 0.457067  [84100/467875]\n",
            "loss: 0.550099  [87000/467875]\n",
            "loss: 0.504390  [89900/467875]\n",
            "loss: 0.624637  [92800/467875]\n",
            "loss: 0.558174  [95700/467875]\n",
            "loss: 0.512401  [98600/467875]\n",
            "loss: 0.496266  [101500/467875]\n",
            "loss: 0.680041  [104400/467875]\n",
            "loss: 0.412660  [107300/467875]\n",
            "loss: 0.478285  [110200/467875]\n",
            "loss: 0.562800  [113100/467875]\n",
            "loss: 0.487493  [116000/467875]\n",
            "loss: 0.497995  [118900/467875]\n",
            "loss: 0.512411  [121800/467875]\n",
            "loss: 0.582574  [124700/467875]\n",
            "loss: 0.433926  [127600/467875]\n",
            "loss: 0.418011  [130500/467875]\n",
            "loss: 0.588647  [133400/467875]\n",
            "loss: 0.487986  [136300/467875]\n",
            "loss: 0.564672  [139200/467875]\n",
            "loss: 0.504964  [142100/467875]\n",
            "loss: 0.630717  [145000/467875]\n",
            "loss: 0.512029  [147900/467875]\n",
            "loss: 0.579070  [150800/467875]\n",
            "loss: 0.480612  [153700/467875]\n",
            "loss: 0.435539  [156600/467875]\n",
            "loss: 0.504036  [159500/467875]\n",
            "loss: 0.582775  [162400/467875]\n",
            "loss: 0.480615  [165300/467875]\n",
            "loss: 0.505543  [168200/467875]\n",
            "loss: 0.578140  [171100/467875]\n",
            "loss: 0.506310  [174000/467875]\n",
            "loss: 0.492565  [176900/467875]\n",
            "loss: 0.556577  [179800/467875]\n",
            "loss: 0.544169  [182700/467875]\n",
            "loss: 0.583302  [185600/467875]\n",
            "loss: 0.603384  [188500/467875]\n",
            "loss: 0.441250  [191400/467875]\n",
            "loss: 0.631806  [194300/467875]\n",
            "loss: 0.628804  [197200/467875]\n",
            "loss: 0.455885  [200100/467875]\n",
            "loss: 0.496021  [203000/467875]\n",
            "loss: 0.459970  [205900/467875]\n",
            "loss: 0.402881  [208800/467875]\n",
            "loss: 0.446825  [211700/467875]\n",
            "loss: 0.384337  [214600/467875]\n",
            "loss: 0.483072  [217500/467875]\n",
            "loss: 0.519562  [220400/467875]\n",
            "loss: 0.571856  [223300/467875]\n",
            "loss: 0.530706  [226200/467875]\n",
            "loss: 0.501406  [229100/467875]\n",
            "loss: 0.480584  [232000/467875]\n",
            "loss: 0.408170  [234900/467875]\n",
            "loss: 0.449492  [237800/467875]\n",
            "loss: 0.511806  [240700/467875]\n",
            "loss: 0.598290  [243600/467875]\n",
            "loss: 0.380157  [246500/467875]\n",
            "loss: 0.489863  [249400/467875]\n",
            "loss: 0.497518  [252300/467875]\n",
            "loss: 0.561279  [255200/467875]\n",
            "loss: 0.584176  [258100/467875]\n",
            "loss: 0.553207  [261000/467875]\n",
            "loss: 0.503566  [263900/467875]\n",
            "loss: 0.517210  [266800/467875]\n",
            "loss: 0.416653  [269700/467875]\n",
            "loss: 0.409045  [272600/467875]\n",
            "loss: 0.531512  [275500/467875]\n",
            "loss: 0.561901  [278400/467875]\n",
            "loss: 0.448683  [281300/467875]\n",
            "loss: 0.425344  [284200/467875]\n",
            "loss: 0.474243  [287100/467875]\n",
            "loss: 0.450971  [290000/467875]\n",
            "loss: 0.540683  [292900/467875]\n",
            "loss: 0.498448  [295800/467875]\n",
            "loss: 0.542289  [298700/467875]\n",
            "loss: 0.329467  [301600/467875]\n",
            "loss: 0.545635  [304500/467875]\n",
            "loss: 0.561367  [307400/467875]\n",
            "loss: 0.372807  [310300/467875]\n",
            "loss: 0.548862  [313200/467875]\n",
            "loss: 0.451319  [316100/467875]\n",
            "loss: 0.666018  [319000/467875]\n",
            "loss: 0.426335  [321900/467875]\n",
            "loss: 0.522563  [324800/467875]\n",
            "loss: 0.500739  [327700/467875]\n",
            "loss: 0.586274  [330600/467875]\n",
            "loss: 0.522087  [333500/467875]\n",
            "loss: 0.565504  [336400/467875]\n",
            "loss: 0.530310  [339300/467875]\n",
            "loss: 0.482670  [342200/467875]\n",
            "loss: 0.592881  [345100/467875]\n",
            "loss: 0.439952  [348000/467875]\n",
            "loss: 0.566931  [350900/467875]\n",
            "loss: 0.569710  [353800/467875]\n",
            "loss: 0.456790  [356700/467875]\n",
            "loss: 0.493192  [359600/467875]\n",
            "loss: 0.496846  [362500/467875]\n",
            "loss: 0.592769  [365400/467875]\n",
            "loss: 0.456571  [368300/467875]\n",
            "loss: 0.582044  [371200/467875]\n",
            "loss: 0.532069  [374100/467875]\n",
            "loss: 0.676121  [377000/467875]\n",
            "loss: 0.535067  [379900/467875]\n",
            "loss: 0.558627  [382800/467875]\n",
            "loss: 0.583278  [385700/467875]\n",
            "loss: 0.568300  [388600/467875]\n",
            "loss: 0.589877  [391500/467875]\n",
            "loss: 0.409019  [394400/467875]\n",
            "loss: 0.578870  [397300/467875]\n",
            "loss: 0.598572  [400200/467875]\n",
            "loss: 0.513808  [403100/467875]\n",
            "loss: 0.571396  [406000/467875]\n",
            "loss: 0.525338  [408900/467875]\n",
            "loss: 0.568939  [411800/467875]\n",
            "loss: 0.583410  [414700/467875]\n",
            "loss: 0.650197  [417600/467875]\n",
            "loss: 0.514141  [420500/467875]\n",
            "loss: 0.563975  [423400/467875]\n",
            "loss: 0.505376  [426300/467875]\n",
            "loss: 0.543789  [429200/467875]\n",
            "loss: 0.547966  [432100/467875]\n",
            "loss: 0.532361  [435000/467875]\n",
            "loss: 0.574623  [437900/467875]\n",
            "loss: 0.553122  [440800/467875]\n",
            "loss: 0.659614  [443700/467875]\n",
            "loss: 0.670064  [446600/467875]\n",
            "loss: 0.499682  [449500/467875]\n",
            "loss: 0.569047  [452400/467875]\n",
            "loss: 0.476758  [455300/467875]\n",
            "loss: 0.553648  [458200/467875]\n",
            "loss: 0.572983  [461100/467875]\n",
            "loss: 0.481096  [464000/467875]\n",
            "loss: 0.585003  [466900/467875]\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.522312 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 3 s\n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.587106  [    0/467875]\n",
            "loss: 0.458995  [ 2900/467875]\n",
            "loss: 0.572939  [ 5800/467875]\n",
            "loss: 0.522747  [ 8700/467875]\n",
            "loss: 0.502625  [11600/467875]\n",
            "loss: 0.529025  [14500/467875]\n",
            "loss: 0.599739  [17400/467875]\n",
            "loss: 0.426753  [20300/467875]\n",
            "loss: 0.438252  [23200/467875]\n",
            "loss: 0.426362  [26100/467875]\n",
            "loss: 0.513029  [29000/467875]\n",
            "loss: 0.500754  [31900/467875]\n",
            "loss: 0.345505  [34800/467875]\n",
            "loss: 0.526972  [37700/467875]\n",
            "loss: 0.485513  [40600/467875]\n",
            "loss: 0.560878  [43500/467875]\n",
            "loss: 0.579576  [46400/467875]\n",
            "loss: 0.515531  [49300/467875]\n",
            "loss: 0.559522  [52200/467875]\n",
            "loss: 0.498453  [55100/467875]\n",
            "loss: 0.378778  [58000/467875]\n",
            "loss: 0.672229  [60900/467875]\n",
            "loss: 0.402115  [63800/467875]\n",
            "loss: 0.438849  [66700/467875]\n",
            "loss: 0.531922  [69600/467875]\n",
            "loss: 0.408629  [72500/467875]\n",
            "loss: 0.479561  [75400/467875]\n",
            "loss: 0.555304  [78300/467875]\n",
            "loss: 0.440612  [81200/467875]\n",
            "loss: 0.558285  [84100/467875]\n",
            "loss: 0.494760  [87000/467875]\n",
            "loss: 0.499423  [89900/467875]\n",
            "loss: 0.573122  [92800/467875]\n",
            "loss: 0.455406  [95700/467875]\n",
            "loss: 0.530609  [98600/467875]\n",
            "loss: 0.516298  [101500/467875]\n",
            "loss: 0.511546  [104400/467875]\n",
            "loss: 0.513655  [107300/467875]\n",
            "loss: 0.542269  [110200/467875]\n",
            "loss: 0.453623  [113100/467875]\n",
            "loss: 0.524949  [116000/467875]\n",
            "loss: 0.525518  [118900/467875]\n",
            "loss: 0.381270  [121800/467875]\n",
            "loss: 0.478274  [124700/467875]\n",
            "loss: 0.528026  [127600/467875]\n",
            "loss: 0.470447  [130500/467875]\n",
            "loss: 0.509330  [133400/467875]\n",
            "loss: 0.592588  [136300/467875]\n",
            "loss: 0.525144  [139200/467875]\n",
            "loss: 0.543111  [142100/467875]\n",
            "loss: 0.452477  [145000/467875]\n",
            "loss: 0.484074  [147900/467875]\n",
            "loss: 0.537547  [150800/467875]\n",
            "loss: 0.459636  [153700/467875]\n",
            "loss: 0.580126  [156600/467875]\n",
            "loss: 0.568753  [159500/467875]\n",
            "loss: 0.472772  [162400/467875]\n",
            "loss: 0.480605  [165300/467875]\n",
            "loss: 0.489235  [168200/467875]\n",
            "loss: 0.486443  [171100/467875]\n",
            "loss: 0.510753  [174000/467875]\n",
            "loss: 0.442829  [176900/467875]\n",
            "loss: 0.431000  [179800/467875]\n",
            "loss: 0.506244  [182700/467875]\n",
            "loss: 0.517906  [185600/467875]\n",
            "loss: 0.619715  [188500/467875]\n",
            "loss: 0.544060  [191400/467875]\n",
            "loss: 0.509474  [194300/467875]\n",
            "loss: 0.479981  [197200/467875]\n",
            "loss: 0.529974  [200100/467875]\n",
            "loss: 0.457245  [203000/467875]\n",
            "loss: 0.490255  [205900/467875]\n",
            "loss: 0.637114  [208800/467875]\n",
            "loss: 0.519133  [211700/467875]\n",
            "loss: 0.570580  [214600/467875]\n",
            "loss: 0.618099  [217500/467875]\n",
            "loss: 0.540703  [220400/467875]\n",
            "loss: 0.502242  [223300/467875]\n",
            "loss: 0.618133  [226200/467875]\n",
            "loss: 0.653342  [229100/467875]\n",
            "loss: 0.458610  [232000/467875]\n",
            "loss: 0.578039  [234900/467875]\n",
            "loss: 0.493199  [237800/467875]\n",
            "loss: 0.545047  [240700/467875]\n",
            "loss: 0.605442  [243600/467875]\n",
            "loss: 0.488687  [246500/467875]\n",
            "loss: 0.526637  [249400/467875]\n",
            "loss: 0.506447  [252300/467875]\n",
            "loss: 0.454614  [255200/467875]\n",
            "loss: 0.571821  [258100/467875]\n",
            "loss: 0.528013  [261000/467875]\n",
            "loss: 0.510565  [263900/467875]\n",
            "loss: 0.451887  [266800/467875]\n",
            "loss: 0.455820  [269700/467875]\n",
            "loss: 0.505481  [272600/467875]\n",
            "loss: 0.475414  [275500/467875]\n",
            "loss: 0.479965  [278400/467875]\n",
            "loss: 0.525920  [281300/467875]\n",
            "loss: 0.482807  [284200/467875]\n",
            "loss: 0.515862  [287100/467875]\n",
            "loss: 0.489786  [290000/467875]\n",
            "loss: 0.471737  [292900/467875]\n",
            "loss: 0.530624  [295800/467875]\n",
            "loss: 0.563400  [298700/467875]\n",
            "loss: 0.450508  [301600/467875]\n",
            "loss: 0.523468  [304500/467875]\n",
            "loss: 0.407439  [307400/467875]\n",
            "loss: 0.591341  [310300/467875]\n",
            "loss: 0.401289  [313200/467875]\n",
            "loss: 0.550947  [316100/467875]\n",
            "loss: 0.460575  [319000/467875]\n",
            "loss: 0.504810  [321900/467875]\n",
            "loss: 0.543628  [324800/467875]\n",
            "loss: 0.468103  [327700/467875]\n",
            "loss: 0.456613  [330600/467875]\n",
            "loss: 0.597672  [333500/467875]\n",
            "loss: 0.559349  [336400/467875]\n",
            "loss: 0.441982  [339300/467875]\n",
            "loss: 0.664273  [342200/467875]\n",
            "loss: 0.509915  [345100/467875]\n",
            "loss: 0.551341  [348000/467875]\n",
            "loss: 0.406624  [350900/467875]\n",
            "loss: 0.538887  [353800/467875]\n",
            "loss: 0.462027  [356700/467875]\n",
            "loss: 0.509674  [359600/467875]\n",
            "loss: 0.584177  [362500/467875]\n",
            "loss: 0.510900  [365400/467875]\n",
            "loss: 0.449839  [368300/467875]\n",
            "loss: 0.417043  [371200/467875]\n",
            "loss: 0.539725  [374100/467875]\n",
            "loss: 0.443334  [377000/467875]\n",
            "loss: 0.510536  [379900/467875]\n",
            "loss: 0.478154  [382800/467875]\n",
            "loss: 0.462851  [385700/467875]\n",
            "loss: 0.465882  [388600/467875]\n",
            "loss: 0.613337  [391500/467875]\n",
            "loss: 0.469357  [394400/467875]\n",
            "loss: 0.579409  [397300/467875]\n",
            "loss: 0.558348  [400200/467875]\n",
            "loss: 0.475197  [403100/467875]\n",
            "loss: 0.564702  [406000/467875]\n",
            "loss: 0.556985  [408900/467875]\n",
            "loss: 0.551162  [411800/467875]\n",
            "loss: 0.458745  [414700/467875]\n",
            "loss: 0.467819  [417600/467875]\n",
            "loss: 0.585681  [420500/467875]\n",
            "loss: 0.468666  [423400/467875]\n",
            "loss: 0.582444  [426300/467875]\n",
            "loss: 0.589942  [429200/467875]\n",
            "loss: 0.426052  [432100/467875]\n",
            "loss: 0.625717  [435000/467875]\n",
            "loss: 0.670076  [437900/467875]\n",
            "loss: 0.496795  [440800/467875]\n",
            "loss: 0.547626  [443700/467875]\n",
            "loss: 0.629446  [446600/467875]\n",
            "loss: 0.490461  [449500/467875]\n",
            "loss: 0.538700  [452400/467875]\n",
            "loss: 0.585990  [455300/467875]\n",
            "loss: 0.589618  [458200/467875]\n",
            "loss: 0.592129  [461100/467875]\n",
            "loss: 0.536654  [464000/467875]\n",
            "loss: 0.533303  [466900/467875]\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.523163 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 4 s\n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.615602  [    0/467875]\n",
            "loss: 0.436774  [ 2900/467875]\n",
            "loss: 0.523327  [ 5800/467875]\n",
            "loss: 0.486550  [ 8700/467875]\n",
            "loss: 0.518714  [11600/467875]\n",
            "loss: 0.556217  [14500/467875]\n",
            "loss: 0.495019  [17400/467875]\n",
            "loss: 0.538826  [20300/467875]\n",
            "loss: 0.543920  [23200/467875]\n",
            "loss: 0.588069  [26100/467875]\n",
            "loss: 0.500701  [29000/467875]\n",
            "loss: 0.605502  [31900/467875]\n",
            "loss: 0.404964  [34800/467875]\n",
            "loss: 0.594571  [37700/467875]\n",
            "loss: 0.536287  [40600/467875]\n",
            "loss: 0.518986  [43500/467875]\n",
            "loss: 0.592857  [46400/467875]\n",
            "loss: 0.573079  [49300/467875]\n",
            "loss: 0.612697  [52200/467875]\n",
            "loss: 0.428439  [55100/467875]\n",
            "loss: 0.457279  [58000/467875]\n",
            "loss: 0.670656  [60900/467875]\n",
            "loss: 0.491708  [63800/467875]\n",
            "loss: 0.431895  [66700/467875]\n",
            "loss: 0.702278  [69600/467875]\n",
            "loss: 0.505189  [72500/467875]\n",
            "loss: 0.499233  [75400/467875]\n",
            "loss: 0.484378  [78300/467875]\n",
            "loss: 0.421454  [81200/467875]\n",
            "loss: 0.504413  [84100/467875]\n",
            "loss: 0.572345  [87000/467875]\n",
            "loss: 0.540798  [89900/467875]\n",
            "loss: 0.606152  [92800/467875]\n",
            "loss: 0.409496  [95700/467875]\n",
            "loss: 0.495772  [98600/467875]\n",
            "loss: 0.590070  [101500/467875]\n",
            "loss: 0.555698  [104400/467875]\n",
            "loss: 0.541728  [107300/467875]\n",
            "loss: 0.451499  [110200/467875]\n",
            "loss: 0.531102  [113100/467875]\n",
            "loss: 0.476481  [116000/467875]\n",
            "loss: 0.509690  [118900/467875]\n",
            "loss: 0.532573  [121800/467875]\n",
            "loss: 0.529351  [124700/467875]\n",
            "loss: 0.608667  [127600/467875]\n",
            "loss: 0.539266  [130500/467875]\n",
            "loss: 0.495811  [133400/467875]\n",
            "loss: 0.581619  [136300/467875]\n",
            "loss: 0.514943  [139200/467875]\n",
            "loss: 0.424620  [142100/467875]\n",
            "loss: 0.542990  [145000/467875]\n",
            "loss: 0.605484  [147900/467875]\n",
            "loss: 0.553410  [150800/467875]\n",
            "loss: 0.422737  [153700/467875]\n",
            "loss: 0.519505  [156600/467875]\n",
            "loss: 0.493534  [159500/467875]\n",
            "loss: 0.460513  [162400/467875]\n",
            "loss: 0.465052  [165300/467875]\n",
            "loss: 0.531494  [168200/467875]\n",
            "loss: 0.480477  [171100/467875]\n",
            "loss: 0.457206  [174000/467875]\n",
            "loss: 0.486850  [176900/467875]\n",
            "loss: 0.479127  [179800/467875]\n",
            "loss: 0.486426  [182700/467875]\n",
            "loss: 0.588115  [185600/467875]\n",
            "loss: 0.439934  [188500/467875]\n",
            "loss: 0.608709  [191400/467875]\n",
            "loss: 0.605273  [194300/467875]\n",
            "loss: 0.529419  [197200/467875]\n",
            "loss: 0.523918  [200100/467875]\n",
            "loss: 0.399224  [203000/467875]\n",
            "loss: 0.527045  [205900/467875]\n",
            "loss: 0.518095  [208800/467875]\n",
            "loss: 0.608012  [211700/467875]\n",
            "loss: 0.500673  [214600/467875]\n",
            "loss: 0.544900  [217500/467875]\n",
            "loss: 0.552485  [220400/467875]\n",
            "loss: 0.542564  [223300/467875]\n",
            "loss: 0.487574  [226200/467875]\n",
            "loss: 0.460141  [229100/467875]\n",
            "loss: 0.511146  [232000/467875]\n",
            "loss: 0.529755  [234900/467875]\n",
            "loss: 0.496296  [237800/467875]\n",
            "loss: 0.490408  [240700/467875]\n",
            "loss: 0.572694  [243600/467875]\n",
            "loss: 0.552894  [246500/467875]\n",
            "loss: 0.424502  [249400/467875]\n",
            "loss: 0.551273  [252300/467875]\n",
            "loss: 0.455673  [255200/467875]\n",
            "loss: 0.507663  [258100/467875]\n",
            "loss: 0.532939  [261000/467875]\n",
            "loss: 0.571840  [263900/467875]\n",
            "loss: 0.550329  [266800/467875]\n",
            "loss: 0.469951  [269700/467875]\n",
            "loss: 0.455512  [272600/467875]\n",
            "loss: 0.434746  [275500/467875]\n",
            "loss: 0.469775  [278400/467875]\n",
            "loss: 0.512604  [281300/467875]\n",
            "loss: 0.475669  [284200/467875]\n",
            "loss: 0.537576  [287100/467875]\n",
            "loss: 0.613876  [290000/467875]\n",
            "loss: 0.390381  [292900/467875]\n",
            "loss: 0.555928  [295800/467875]\n",
            "loss: 0.542349  [298700/467875]\n",
            "loss: 0.560567  [301600/467875]\n",
            "loss: 0.457974  [304500/467875]\n",
            "loss: 0.490258  [307400/467875]\n",
            "loss: 0.451833  [310300/467875]\n",
            "loss: 0.423613  [313200/467875]\n",
            "loss: 0.580512  [316100/467875]\n",
            "loss: 0.517353  [319000/467875]\n",
            "loss: 0.402535  [321900/467875]\n",
            "loss: 0.450867  [324800/467875]\n",
            "loss: 0.642784  [327700/467875]\n",
            "loss: 0.410085  [330600/467875]\n",
            "loss: 0.555453  [333500/467875]\n",
            "loss: 0.492134  [336400/467875]\n",
            "loss: 0.474423  [339300/467875]\n",
            "loss: 0.570804  [342200/467875]\n",
            "loss: 0.528011  [345100/467875]\n",
            "loss: 0.454330  [348000/467875]\n",
            "loss: 0.543878  [350900/467875]\n",
            "loss: 0.543389  [353800/467875]\n",
            "loss: 0.552418  [356700/467875]\n",
            "loss: 0.518957  [359600/467875]\n",
            "loss: 0.446888  [362500/467875]\n",
            "loss: 0.718171  [365400/467875]\n",
            "loss: 0.522403  [368300/467875]\n",
            "loss: 0.431283  [371200/467875]\n",
            "loss: 0.486193  [374100/467875]\n",
            "loss: 0.487738  [377000/467875]\n",
            "loss: 0.518377  [379900/467875]\n",
            "loss: 0.585292  [382800/467875]\n",
            "loss: 0.419041  [385700/467875]\n",
            "loss: 0.491824  [388600/467875]\n",
            "loss: 0.502441  [391500/467875]\n",
            "loss: 0.509059  [394400/467875]\n",
            "loss: 0.561597  [397300/467875]\n",
            "loss: 0.586580  [400200/467875]\n",
            "loss: 0.537857  [403100/467875]\n",
            "loss: 0.567544  [406000/467875]\n",
            "loss: 0.515676  [408900/467875]\n",
            "loss: 0.545190  [411800/467875]\n",
            "loss: 0.464784  [414700/467875]\n",
            "loss: 0.486989  [417600/467875]\n",
            "loss: 0.559215  [420500/467875]\n",
            "loss: 0.448162  [423400/467875]\n",
            "loss: 0.590925  [426300/467875]\n",
            "loss: 0.493806  [429200/467875]\n",
            "loss: 0.389182  [432100/467875]\n",
            "loss: 0.373419  [435000/467875]\n",
            "loss: 0.495827  [437900/467875]\n",
            "loss: 0.485817  [440800/467875]\n",
            "loss: 0.574430  [443700/467875]\n",
            "loss: 0.492742  [446600/467875]\n",
            "loss: 0.611463  [449500/467875]\n",
            "loss: 0.492733  [452400/467875]\n",
            "loss: 0.495606  [455300/467875]\n",
            "loss: 0.556741  [458200/467875]\n",
            "loss: 0.480530  [461100/467875]\n",
            "loss: 0.513237  [464000/467875]\n",
            "loss: 0.552727  [466900/467875]\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.521936 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 3 s\n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.478264  [    0/467875]\n",
            "loss: 0.541307  [ 2900/467875]\n",
            "loss: 0.727845  [ 5800/467875]\n",
            "loss: 0.513669  [ 8700/467875]\n",
            "loss: 0.502691  [11600/467875]\n",
            "loss: 0.438125  [14500/467875]\n",
            "loss: 0.548919  [17400/467875]\n",
            "loss: 0.527926  [20300/467875]\n",
            "loss: 0.647483  [23200/467875]\n",
            "loss: 0.455195  [26100/467875]\n",
            "loss: 0.569869  [29000/467875]\n",
            "loss: 0.564047  [31900/467875]\n",
            "loss: 0.449924  [34800/467875]\n",
            "loss: 0.458069  [37700/467875]\n",
            "loss: 0.580440  [40600/467875]\n",
            "loss: 0.579779  [43500/467875]\n",
            "loss: 0.554947  [46400/467875]\n",
            "loss: 0.552332  [49300/467875]\n",
            "loss: 0.516292  [52200/467875]\n",
            "loss: 0.444844  [55100/467875]\n",
            "loss: 0.548162  [58000/467875]\n",
            "loss: 0.522556  [60900/467875]\n",
            "loss: 0.510948  [63800/467875]\n",
            "loss: 0.696402  [66700/467875]\n",
            "loss: 0.466901  [69600/467875]\n",
            "loss: 0.601267  [72500/467875]\n",
            "loss: 0.520060  [75400/467875]\n",
            "loss: 0.506718  [78300/467875]\n",
            "loss: 0.510555  [81200/467875]\n",
            "loss: 0.561184  [84100/467875]\n",
            "loss: 0.480286  [87000/467875]\n",
            "loss: 0.573281  [89900/467875]\n",
            "loss: 0.504823  [92800/467875]\n",
            "loss: 0.588980  [95700/467875]\n",
            "loss: 0.537856  [98600/467875]\n",
            "loss: 0.465308  [101500/467875]\n",
            "loss: 0.464536  [104400/467875]\n",
            "loss: 0.535165  [107300/467875]\n",
            "loss: 0.465931  [110200/467875]\n",
            "loss: 0.475514  [113100/467875]\n",
            "loss: 0.466045  [116000/467875]\n",
            "loss: 0.450473  [118900/467875]\n",
            "loss: 0.552267  [121800/467875]\n",
            "loss: 0.454859  [124700/467875]\n",
            "loss: 0.536961  [127600/467875]\n",
            "loss: 0.534444  [130500/467875]\n",
            "loss: 0.652204  [133400/467875]\n",
            "loss: 0.497183  [136300/467875]\n",
            "loss: 0.614386  [139200/467875]\n",
            "loss: 0.535407  [142100/467875]\n",
            "loss: 0.441689  [145000/467875]\n",
            "loss: 0.497339  [147900/467875]\n",
            "loss: 0.428715  [150800/467875]\n",
            "loss: 0.488212  [153700/467875]\n",
            "loss: 0.628976  [156600/467875]\n",
            "loss: 0.466804  [159500/467875]\n",
            "loss: 0.580842  [162400/467875]\n",
            "loss: 0.392700  [165300/467875]\n",
            "loss: 0.547758  [168200/467875]\n",
            "loss: 0.455419  [171100/467875]\n",
            "loss: 0.503619  [174000/467875]\n",
            "loss: 0.550554  [176900/467875]\n",
            "loss: 0.480947  [179800/467875]\n",
            "loss: 0.600398  [182700/467875]\n",
            "loss: 0.481473  [185600/467875]\n",
            "loss: 0.544163  [188500/467875]\n",
            "loss: 0.543199  [191400/467875]\n",
            "loss: 0.574478  [194300/467875]\n",
            "loss: 0.481313  [197200/467875]\n",
            "loss: 0.468340  [200100/467875]\n",
            "loss: 0.628649  [203000/467875]\n",
            "loss: 0.506702  [205900/467875]\n",
            "loss: 0.599508  [208800/467875]\n",
            "loss: 0.691986  [211700/467875]\n",
            "loss: 0.408868  [214600/467875]\n",
            "loss: 0.629137  [217500/467875]\n",
            "loss: 0.480641  [220400/467875]\n",
            "loss: 0.564464  [223300/467875]\n",
            "loss: 0.532615  [226200/467875]\n",
            "loss: 0.535387  [229100/467875]\n",
            "loss: 0.649155  [232000/467875]\n",
            "loss: 0.384453  [234900/467875]\n",
            "loss: 0.458947  [237800/467875]\n",
            "loss: 0.449191  [240700/467875]\n",
            "loss: 0.527826  [243600/467875]\n",
            "loss: 0.552215  [246500/467875]\n",
            "loss: 0.420118  [249400/467875]\n",
            "loss: 0.552032  [252300/467875]\n",
            "loss: 0.534464  [255200/467875]\n",
            "loss: 0.414602  [258100/467875]\n",
            "loss: 0.523436  [261000/467875]\n",
            "loss: 0.480559  [263900/467875]\n",
            "loss: 0.499954  [266800/467875]\n",
            "loss: 0.493610  [269700/467875]\n",
            "loss: 0.522078  [272600/467875]\n",
            "loss: 0.670644  [275500/467875]\n",
            "loss: 0.577704  [278400/467875]\n",
            "loss: 0.528549  [281300/467875]\n",
            "loss: 0.496958  [284200/467875]\n",
            "loss: 0.503816  [287100/467875]\n",
            "loss: 0.519220  [290000/467875]\n",
            "loss: 0.440487  [292900/467875]\n",
            "loss: 0.487424  [295800/467875]\n",
            "loss: 0.513336  [298700/467875]\n",
            "loss: 0.460106  [301600/467875]\n",
            "loss: 0.530471  [304500/467875]\n",
            "loss: 0.515069  [307400/467875]\n",
            "loss: 0.465118  [310300/467875]\n",
            "loss: 0.504484  [313200/467875]\n",
            "loss: 0.500150  [316100/467875]\n",
            "loss: 0.552216  [319000/467875]\n",
            "loss: 0.506707  [321900/467875]\n",
            "loss: 0.498192  [324800/467875]\n",
            "loss: 0.537579  [327700/467875]\n",
            "loss: 0.514252  [330600/467875]\n",
            "loss: 0.499201  [333500/467875]\n",
            "loss: 0.478835  [336400/467875]\n",
            "loss: 0.458438  [339300/467875]\n",
            "loss: 0.438193  [342200/467875]\n",
            "loss: 0.528181  [345100/467875]\n",
            "loss: 0.480028  [348000/467875]\n",
            "loss: 0.440952  [350900/467875]\n",
            "loss: 0.592879  [353800/467875]\n",
            "loss: 0.484484  [356700/467875]\n",
            "loss: 0.578534  [359600/467875]\n",
            "loss: 0.460303  [362500/467875]\n",
            "loss: 0.445875  [365400/467875]\n",
            "loss: 0.422034  [368300/467875]\n",
            "loss: 0.523446  [371200/467875]\n",
            "loss: 0.542365  [374100/467875]\n",
            "loss: 0.556767  [377000/467875]\n",
            "loss: 0.501051  [379900/467875]\n",
            "loss: 0.558633  [382800/467875]\n",
            "loss: 0.636793  [385700/467875]\n",
            "loss: 0.483934  [388600/467875]\n",
            "loss: 0.577480  [391500/467875]\n",
            "loss: 0.484204  [394400/467875]\n",
            "loss: 0.461971  [397300/467875]\n",
            "loss: 0.530774  [400200/467875]\n",
            "loss: 0.535316  [403100/467875]\n",
            "loss: 0.464040  [406000/467875]\n",
            "loss: 0.520102  [408900/467875]\n",
            "loss: 0.575890  [411800/467875]\n",
            "loss: 0.582514  [414700/467875]\n",
            "loss: 0.472199  [417600/467875]\n",
            "loss: 0.559164  [420500/467875]\n",
            "loss: 0.493383  [423400/467875]\n",
            "loss: 0.556352  [426300/467875]\n",
            "loss: 0.511961  [429200/467875]\n",
            "loss: 0.630657  [432100/467875]\n",
            "loss: 0.688987  [435000/467875]\n",
            "loss: 0.525475  [437900/467875]\n",
            "loss: 0.568039  [440800/467875]\n",
            "loss: 0.411979  [443700/467875]\n",
            "loss: 0.513135  [446600/467875]\n",
            "loss: 0.497834  [449500/467875]\n",
            "loss: 0.458505  [452400/467875]\n",
            "loss: 0.576336  [455300/467875]\n",
            "loss: 0.451061  [458200/467875]\n",
            "loss: 0.465160  [461100/467875]\n",
            "loss: 0.531385  [464000/467875]\n",
            "loss: 0.504199  [466900/467875]\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.521892 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 6 s\n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.550497  [    0/467875]\n",
            "loss: 0.500235  [ 2900/467875]\n",
            "loss: 0.585123  [ 5800/467875]\n",
            "loss: 0.461839  [ 8700/467875]\n",
            "loss: 0.462710  [11600/467875]\n",
            "loss: 0.726071  [14500/467875]\n",
            "loss: 0.507662  [17400/467875]\n",
            "loss: 0.465826  [20300/467875]\n",
            "loss: 0.465615  [23200/467875]\n",
            "loss: 0.600424  [26100/467875]\n",
            "loss: 0.513915  [29000/467875]\n",
            "loss: 0.523531  [31900/467875]\n",
            "loss: 0.431146  [34800/467875]\n",
            "loss: 0.544306  [37700/467875]\n",
            "loss: 0.447614  [40600/467875]\n",
            "loss: 0.450583  [43500/467875]\n",
            "loss: 0.613004  [46400/467875]\n",
            "loss: 0.521176  [49300/467875]\n",
            "loss: 0.524130  [52200/467875]\n",
            "loss: 0.607872  [55100/467875]\n",
            "loss: 0.559031  [58000/467875]\n",
            "loss: 0.580816  [60900/467875]\n",
            "loss: 0.512247  [63800/467875]\n",
            "loss: 0.445403  [66700/467875]\n",
            "loss: 0.469229  [69600/467875]\n",
            "loss: 0.477478  [72500/467875]\n",
            "loss: 0.527210  [75400/467875]\n",
            "loss: 0.641999  [78300/467875]\n",
            "loss: 0.443124  [81200/467875]\n",
            "loss: 0.534234  [84100/467875]\n",
            "loss: 0.566884  [87000/467875]\n",
            "loss: 0.495407  [89900/467875]\n",
            "loss: 0.562296  [92800/467875]\n",
            "loss: 0.528490  [95700/467875]\n",
            "loss: 0.580505  [98600/467875]\n",
            "loss: 0.549905  [101500/467875]\n",
            "loss: 0.652622  [104400/467875]\n",
            "loss: 0.415977  [107300/467875]\n",
            "loss: 0.524766  [110200/467875]\n",
            "loss: 0.455834  [113100/467875]\n",
            "loss: 0.564264  [116000/467875]\n",
            "loss: 0.465141  [118900/467875]\n",
            "loss: 0.435107  [121800/467875]\n",
            "loss: 0.503895  [124700/467875]\n",
            "loss: 0.407168  [127600/467875]\n",
            "loss: 0.455667  [130500/467875]\n",
            "loss: 0.558574  [133400/467875]\n",
            "loss: 0.488953  [136300/467875]\n",
            "loss: 0.491651  [139200/467875]\n",
            "loss: 0.568651  [142100/467875]\n",
            "loss: 0.498460  [145000/467875]\n",
            "loss: 0.572885  [147900/467875]\n",
            "loss: 0.561863  [150800/467875]\n",
            "loss: 0.532703  [153700/467875]\n",
            "loss: 0.465193  [156600/467875]\n",
            "loss: 0.621423  [159500/467875]\n",
            "loss: 0.572589  [162400/467875]\n",
            "loss: 0.560731  [165300/467875]\n",
            "loss: 0.388529  [168200/467875]\n",
            "loss: 0.488225  [171100/467875]\n",
            "loss: 0.586290  [174000/467875]\n",
            "loss: 0.527037  [176900/467875]\n",
            "loss: 0.478704  [179800/467875]\n",
            "loss: 0.607462  [182700/467875]\n",
            "loss: 0.561621  [185600/467875]\n",
            "loss: 0.465358  [188500/467875]\n",
            "loss: 0.541587  [191400/467875]\n",
            "loss: 0.511999  [194300/467875]\n",
            "loss: 0.485192  [197200/467875]\n",
            "loss: 0.433115  [200100/467875]\n",
            "loss: 0.528026  [203000/467875]\n",
            "loss: 0.474728  [205900/467875]\n",
            "loss: 0.522547  [208800/467875]\n",
            "loss: 0.498827  [211700/467875]\n",
            "loss: 0.458926  [214600/467875]\n",
            "loss: 0.446739  [217500/467875]\n",
            "loss: 0.511069  [220400/467875]\n",
            "loss: 0.521448  [223300/467875]\n",
            "loss: 0.629562  [226200/467875]\n",
            "loss: 0.561828  [229100/467875]\n",
            "loss: 0.611035  [232000/467875]\n",
            "loss: 0.600092  [234900/467875]\n",
            "loss: 0.548452  [237800/467875]\n",
            "loss: 0.449966  [240700/467875]\n",
            "loss: 0.592792  [243600/467875]\n",
            "loss: 0.507606  [246500/467875]\n",
            "loss: 0.527193  [249400/467875]\n",
            "loss: 0.495542  [252300/467875]\n",
            "loss: 0.530311  [255200/467875]\n",
            "loss: 0.382020  [258100/467875]\n",
            "loss: 0.486749  [261000/467875]\n",
            "loss: 0.513000  [263900/467875]\n",
            "loss: 0.540961  [266800/467875]\n",
            "loss: 0.511492  [269700/467875]\n",
            "loss: 0.638238  [272600/467875]\n",
            "loss: 0.411354  [275500/467875]\n",
            "loss: 0.543105  [278400/467875]\n",
            "loss: 0.499082  [281300/467875]\n",
            "loss: 0.573967  [284200/467875]\n",
            "loss: 0.575143  [287100/467875]\n",
            "loss: 0.535559  [290000/467875]\n",
            "loss: 0.522515  [292900/467875]\n",
            "loss: 0.524176  [295800/467875]\n",
            "loss: 0.520575  [298700/467875]\n",
            "loss: 0.460506  [301600/467875]\n",
            "loss: 0.500667  [304500/467875]\n",
            "loss: 0.536110  [307400/467875]\n",
            "loss: 0.540947  [310300/467875]\n",
            "loss: 0.577028  [313200/467875]\n",
            "loss: 0.600155  [316100/467875]\n",
            "loss: 0.466050  [319000/467875]\n",
            "loss: 0.753462  [321900/467875]\n",
            "loss: 0.488809  [324800/467875]\n",
            "loss: 0.408247  [327700/467875]\n",
            "loss: 0.417261  [330600/467875]\n",
            "loss: 0.610519  [333500/467875]\n",
            "loss: 0.398481  [336400/467875]\n",
            "loss: 0.719273  [339300/467875]\n",
            "loss: 0.423257  [342200/467875]\n",
            "loss: 0.493951  [345100/467875]\n",
            "loss: 0.510260  [348000/467875]\n",
            "loss: 0.576947  [350900/467875]\n",
            "loss: 0.510287  [353800/467875]\n",
            "loss: 0.599036  [356700/467875]\n",
            "loss: 0.440019  [359600/467875]\n",
            "loss: 0.528223  [362500/467875]\n",
            "loss: 0.611531  [365400/467875]\n",
            "loss: 0.634407  [368300/467875]\n",
            "loss: 0.578746  [371200/467875]\n",
            "loss: 0.541267  [374100/467875]\n",
            "loss: 0.545530  [377000/467875]\n",
            "loss: 0.503618  [379900/467875]\n",
            "loss: 0.622803  [382800/467875]\n",
            "loss: 0.531422  [385700/467875]\n",
            "loss: 0.569816  [388600/467875]\n",
            "loss: 0.451441  [391500/467875]\n",
            "loss: 0.567379  [394400/467875]\n",
            "loss: 0.550512  [397300/467875]\n",
            "loss: 0.456971  [400200/467875]\n",
            "loss: 0.560373  [403100/467875]\n",
            "loss: 0.568567  [406000/467875]\n",
            "loss: 0.559592  [408900/467875]\n",
            "loss: 0.498424  [411800/467875]\n",
            "loss: 0.504427  [414700/467875]\n",
            "loss: 0.554177  [417600/467875]\n",
            "loss: 0.518967  [420500/467875]\n",
            "loss: 0.516768  [423400/467875]\n",
            "loss: 0.511955  [426300/467875]\n",
            "loss: 0.449250  [429200/467875]\n",
            "loss: 0.571063  [432100/467875]\n",
            "loss: 0.484419  [435000/467875]\n",
            "loss: 0.414400  [437900/467875]\n",
            "loss: 0.513510  [440800/467875]\n",
            "loss: 0.480375  [443700/467875]\n",
            "loss: 0.543833  [446600/467875]\n",
            "loss: 0.455040  [449500/467875]\n",
            "loss: 0.540172  [452400/467875]\n",
            "loss: 0.721336  [455300/467875]\n",
            "loss: 0.550871  [458200/467875]\n",
            "loss: 0.485503  [461100/467875]\n",
            "loss: 0.465791  [464000/467875]\n",
            "loss: 0.578487  [466900/467875]\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.524764 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 4 s\n",
            "\n",
            "\n",
            "SPEND 1 h 53 m 41 s IN TOTAL.\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "epochs = 100\n",
        "start = time()\n",
        "for t in range(epochs):\n",
        "    begin = time()\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, criterion, optimizer, t)\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    test_loop(val_dataloader, model, criterion, t)\n",
        "    time_1 = time() - begin\n",
        "    print(\"\\nTime spend to epoch\", '%.0f' % (time_1//3600), \"h\", '%.0f' % (time_1%3600//60), \"m\", '%.0f' % (time_1%3600%60), \"s\\n\")\n",
        "\n",
        "time = time() - start\n",
        "print(\"\\nSPEND\", '%.0f' % (time//3600), \"h\", '%.0f' % (time%3600//60), \"m\", '%.0f' % (time%3600%60), \"s\", \"IN TOTAL.\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc= np.array(val_acc)*100\n",
        "train_acc = np.array(train_acc)*100"
      ],
      "metadata": {
        "id": "d6yH3z0TU7NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize= (7,5))\n",
        "plt.plot(train_acc, color = 'k', linestyle = ':', label = 'Точность на обучающей выборке')\n",
        "plt.plot(val_acc, color = 'k', linestyle = '-', label = 'Точность на тестовой выборке')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Точность, %')\n",
        "plt.title('Точность обучения с использованием оптимизатора Adam')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "cB4b8zFJU0vZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "aeaa6749-0d1a-4353-dc26-80d72302ba63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4f9d0c1810>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize= (7,5))\n",
        "plt.plot(train_loss, color = 'k', linestyle = ':', label = 'Потери на обучающей выборке')\n",
        "plt.plot(val_loss, color = 'k', linestyle = '-', label = 'Потери на тестовой выборке')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Потери, %')\n",
        "plt.title('Функция потерь с использованием оптимизатора Adam')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "z71DJ4YlVZeS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "b1f7d16d-d67d-45b0-dc79-d51d2f8611b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4f9d1118a0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAHWCAYAAAA2Of5hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB6H0lEQVR4nO3dd3gUVdsG8HvTe+8hhZAECCWhG4qgROnFF0GKEhDBV4ogRUGkiRobRQEVUQEBX4oCgiKCCAghEgiEGkICKRDSSe/ZPd8fXDsfwyaQQEIGuH/XNRfkzJmZZ+bM7j575sysSgghQEREREQNSq+hAyAiIiIiJmVEREREisCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERE1OAWLlwIlUrV0GE0KCZlDWDMmDGwsLBo6DCIiIjq3Ntvvw2VSoWXXnqpoUN55Bg0dABPiuzsbGzatAlHjhzBP//8g5KSEvTu3Rtt2rTBsGHD0KZNm4YOkYiI6IEIIfC///0P3t7e2L17NwoKCmBpadnQYT0y2FP2EGzevBne3t6YOnUqTp48CUNDQ6hUKmRnZ2PJkiVo27YtxowZg/Ly8oYOlYiI6L4dOnQI169fxw8//IDKykps3769oUN6pDApq2fh4eF4+eWX4eLigvDwcCQkJCAkJAQmJiY4ceIEbty4gREjRmD9+vV46623ANz6puHt7Y1BgwbprK+0tBTW1tZ4/fXXAdx6AahUKvz88886dS0sLDBmzBjp73Xr1kGlUiExMVEqu3DhAmxtbdG/f39UVlbK6p08eVK2vqysLKhUKixcuFBWXlXZZ599BpVKhR49esjKr169iqFDh8LNzQ16enpQqVRQqVRo2bLl3Q6jtJ3qJm9vb1ndoqIizJgxAx4eHjA2NkbTpk3x+eefQwhRo/XdGXtZWRkWLFgAX19fGBsbw8PDA2+//TbKysp0Ypw8eTI2bdqEpk2bwsTEBO3atcM///wjq6cdO5GVlXXP/a5KSkoKxo0bBzc3NxgbG6Nx48Z444037prYJyYmQqVSYd26dbLySZMmQaVSyc4VAMjNzcVbb70Fb29vGBsbo1GjRhg9erQUs/bcq266c33atrezs4OZmRmeeuop/P7771XGOmbMmCrXeft5VtNhAIMGDYK3tzdMTEzg5OSEgQMH4ty5c7I6lZWVWLx4MZo0aQJjY2N4e3vj3Xff1Wlfb29vKRY9PT24uLjgpZdeQnJysqze559/js6dO8Pe3h6mpqZo165dla9R7flyp/79+8vO6ftpu2nTpknnv6+vLz755BNoNBqddapUKuzcuVO2fGlpKWxtbaFSqfD555/rxHenjIwMjBs3Ds7OzjAxMUFgYCDWr18vq3P79u72ervXeXX7eVDVGKTCwkK4uLhApVLh0KFDUnmPHj2gUqkwePBgnfhff/11nfeh2hzz7du3o2PHjrCzs4OpqSmaNWuGTz75RPZ+k5SUhIkTJ6Jp06YwNTWFvb09hg4dKns/1r733m26PZ6///4b3bp1g7m5OWxsbDBo0CDExMTI4tUeo0uXLmHYsGGwsrKCvb09pk6ditLSUlndtWvX4tlnn4WTkxOMjY0REBCAr7/+Wud43c2mTZsQEBCAZ555BiEhIdi0aVOV9Y4ePYoOHTrAxMQETZo0werVq6usV9OYvL290b9/fxw6dAjt27eHqakpWrVqJZ0D27dvR6tWraT35NOnT9dqvx4WXr6sZx9//DE0Gg02b96Mdu3a6cx3cHDAjz/+iIsXL2L16tVYsGABnJyc8PLLL+PTTz/FzZs3YWdnJ9XfvXs38vPz8fLLLz9wbNeuXUPv3r3RrFkzbN26FQYGdXM65ObmIiwsTKdcrVZj4MCBSEpKwrRp0+Dv7w+VSoUPP/ywxut+7rnnMHr0aFnZkiVLkJOTI/0thMDAgQNx8OBBjBs3DkFBQfjzzz8xa9YspKSkYNmyZQCADRs2SMscOXIE3377LZYtWwYHBwcAgLOzMwBAo9Fg4MCBOHr0KCZMmIDmzZvj3LlzWLZsGS5fvqzzgXb48GFs2bIFb775JoyNjfHVV1+hd+/eiIyMrFHyeS83btxAx44dkZubiwkTJqBZs2ZISUnBzz//jOLiYhgZGdV4XfHx8VizZo1OeWFhIbp164aYmBi8+uqraNu2LbKysrBr1y5cv35dOkYA8Oabb6JDhw6y5V977TXZ3+np6ejcuTOKi4vx5ptvwt7eHuvXr8fAgQPx888/44UXXtCJwcHBQWorAHjllVdqvF93mjBhAlxcXHDjxg2sXLkSISEhSEhIgJmZmRTv+vXr8eKLL2LGjBk4fvw4wsLCEBMTgx07dsjW1a1bN0yYMAEajQbnz5/H8uXLcePGDRw5ckSq88UXX2DgwIEYNWoUysvLsXnzZgwdOhS//fYb+vXrd9/7cbvq2q64uBjdu3dHSkoKXn/9dXh6euLYsWOYM2cOUlNTsXz5cll9ExMTrF27VpasbN++XecDuzolJSXo0aMH4uPjMXnyZDRu3Bjbtm3DmDFjkJubi6lTp8rqjxgxAn379pWVzZkzR/p/8+bNZa/Nb7/9FjExMbJzoXXr1tXGs2TJEqSnp1c5z8TEBL///jsyMjLg5OQkxb9lyxaYmJjcc1+rO+b5+fno1KkTQkNDYWhoiL1792L27NkwMDDAjBkzAAAnTpzAsWPHMHz4cDRq1AiJiYn4+uuv0aNHD1y8eBFmZmZ4+umnZfuufW+cO3euVNa5c2cAwF9//YU+ffrAx8cHCxcuRElJCVasWIEuXbrg1KlTOl9Uhw0bBm9vb4SFheHff//Fl19+iZycHPz4449Sna+//hotWrTAwIEDYWBggN27d2PixInQaDSYNGnSPY9PWVkZfvnlF2mfR4wYgbFjxyItLQ0uLi5SvXPnzuH555+Ho6MjFi5ciMrKSixYsEB6z71dbWKKj4/HyJEj8frrr+Pll1/G559/jgEDBuCbb77Bu+++i4kTJwIAwsLCMGzYMMTGxkJPT2F9U4LqlZ2dnfDy8pKVhYaGCnNzc1nZvHnzBACxe/duIYQQsbGxAoD4+uuvZfUGDhwovL29hUajEUIIcfDgQQFAbNu2TWfb5ubmIjQ0VPp77dq1AoBISEgQN2/eFAEBAaJp06YiKytLtpy23okTJ2TlmZmZAoBYsGCBrPzOsrfffls4OTmJdu3aie7du0vl2n0KCwuTLd+9e3fRokULnfjvBEBMmjRJp7xfv36yY7xz504BQHzwwQeyei+++KJQqVQiPj5eZx23H5s7bdiwQejp6YkjR47Iyr/55hsBQISHh8tiBCBOnjwplSUlJQkTExPxwgsvSGULFiwQAERmZuY99/tOo0ePFnp6ejrtI4SQzouqJCQkCABi7dq1UtmwYcNEy5YthYeHh+xcmT9/vgAgtm/fXu02anPuTZs2TQCQHcOCggLRuHFj4e3tLdRqtWz5UaNGicaNG8vK7jzPqnod1cTWrVtlbRQdHS0AiNdee01Wb+bMmQKA+Pvvv6UyLy8v2X4JIcTIkSOFmZmZrKy4uFj2d3l5uWjZsqV49tlndfapJud0bdpu8eLFwtzcXFy+fFm2ztmzZwt9fX2RnJwsW+eIESOEgYGBSEtLk+r27NlTjBw5UgAQn332mU58t1u+fLkAIDZu3Cjb3+DgYGFhYSHy8/Nl26tqfS1atJC9V9wuNDRU5z1US/s60srIyBCWlpaiT58+AoA4ePCgNE/7PtO6dWvx+eefS+UbNmwQjRo1Et26dZO9D9XmmFclICBA9O/fX/r7znNCCCEiIiIEAPHjjz9WuY7u3btXe1yCgoKEk5OTyM7OlsrOnDkj9PT0xOjRo6Uy7TEaOHCgbPmJEycKAOLMmTN3jbFXr17Cx8en6p28w88//ywAiLi4OCGEEPn5+cLExEQsW7ZMVm/w4MHCxMREJCUlSWUXL14U+vr6svasTUxeXl4CgDh27JhU9ueffwoAwtTUVLat1atX65wfSqGwFPHxU1BQIH0juxvtN4T8/HwAgL+/Pzp16iTr+r158yb++OMPjBo1SqfLvqCgAFlZWbKpOqWlpRg4cCAyMzOxd+9e2Nvb38+uVSklJQUrVqzAvHnzdC4tFRQUAECdbq8qe/bsgb6+Pt58801Z+YwZMyCEwB9//FGr9W3btg3NmzdHs2bNZMf32WefBQAcPHhQVj84OFjWK+rp6YlBgwbhzz//hFqtltW9efMmsrKyUFRUVKNYNBoNdu7ciQEDBqB9+/Y682tzO3lUVBS2bduGsLAwnW+Lv/zyCwIDA6vswbqfW9b37NmDjh07omvXrlKZhYUFJkyYgMTERFy8eFFWv7y8HMbGxjVat7Y97tazU1xcjKysLERHR2PNmjVwdnaGv7+/FBsATJ8+XbaM9tv+nZdYy8rKkJWVhYyMDOzfvx9///03evbsKatjamoq/T8nJwd5eXno1q0bTp06pRNbaWmpzmu3oqLirvt8t7bbtm0bunXrBltbW9k6Q0JCoFardS6lt23bFi1atJB6aJKSknDw4EGdS6LV2bNnD1xcXDBixAipzNDQEG+++SYKCwtx+PDhGq2nLixevBjW1tY6r/3bjR07FmvXrpX+Xrt2LUJDQ+/ZY3K3Y66VlZWF69evY926dYiPj8fTTz8tzbv9nKioqEB2djZ8fX1hY2NT5XlxN6mpqYiOjsaYMWNkV1Jat26N5557Tjqnb3dnr9KUKVMAQFb39hjz8vKQlZWF7t274+rVq8jLy7tnXJs2bUL79u3h6+sLALC0tES/fv1kn2NqtRp//vknBg8eDE9PT6m8efPm6NWrl846axNTQEAAgoODpb87deoEAHj22Wdl29KWX7169Z779LAxKatnbm5uuHLlyj3rxcfHAwDc3d2lstGjRyM8PBxJSUkAbr3ZVlRUVHkZ59VXX4Wjo6Nsqu6DfuzYsTh69CgKCgqkcWR1ZcGCBXBzc5PGvN2uadOmsLW1xZIlSxAeHo7MzMwafQDVVlJSEtzc3HTu+GnevLk0vzbi4uJw4cIFneOr/VDPyMiQ1ffz89NZh7+/P4qLi5GZmSkrb9q0KRwdHWFhYQFnZ2e89957Oonb7TIzM5Gfn18nl0Fnz56Nbt26oX///jrzrly5Uifb0EpKSkLTpk11yqtrk9zc3BqNFysqKpLaw9TUFJ6envjiiy906r3//vtwdHREmzZtkJiYiEOHDknnR1JSEvT09KQPEi0XFxfY2NjoxLZ582Y4OjrC2dkZzz//PDw8PPDdd9/J6vz222946qmnYGJiAjs7Ozg6OuLrr7+u8oPt+++/1zm39u3bd9f9vlvbxcXFYe/evTrrDAkJAaB7vgLyRGXdunXo3LlzledxVZKSkuDn56eTqNzv6+1+JSQkYPXq1Vi0aNFdL0WOGjUKly9fRmRkpHQu1CQBvdsxB24l146OjvDw8MCrr76KWbNmYdasWdL8kpISzJ8/Xxrn5+DgAEdHR+Tm5tYo4bmd9phW95qq6ovene3ZpEkT6Onpyca0hYeHIyQkRBqj5ujoiHfffRcA7hljbm4u9uzZg+7duyM+Pl6aunTpgpMnT+Ly5csAbr2HlZSUVHl+VbU/tYnp9sQLAKytrQEAHh4eVZbfPuxFKTimrJ71798fq1atwvfff49x48ZVWSc9PR3r16+Ho6MjnnrqKal8+PDheOutt7Bp0ya8++672LhxI9q3b1/liTt//nx069ZNVjZgwIAqt3fq1Cn8+uuvmDx5MiZMmIC///77Afbw/8XExGDdunXYuHEjDA0NdeZbWFhgy5YtePXVV2U9JgDQokWLOomhPmg0GrRq1QpLly6tcv6dL/ja+OWXX2BlZYXi4mLs2LEDH374IaysrPD222/f9zprYt++ffjrr78QERFRr9u5X2lpafDy8rpnPRMTE+zevRvArZ7YH374AdOmTYOrqyuGDRsm1XvttdfQs2dPXL9+HcuWLcOQIUNw7Ngx6c0ZqHkP4PPPPy992F6/fh2ffPIJnnnmGZw8eRKmpqY4cuQIBg4ciKeffhpfffUVXF1dYWhoiLVr1+Knn37SWd+gQYN0Bvu/9957SEtLq3L792o7jUaD5557rtpzSPtl4nYvv/wy3n77bfz7779Yv3493nvvvbseAyWaO3cu/Pz8EBoaKhvfdydHR0cMGDAAa9euhbOzM7p06aKTkN+pJq8XIyMj7N+/H8XFxThy5Ag++eQTeHh4SF9Qp0yZgrVr12LatGkIDg6GtbU1VCoVhg8fLrsB42G583y/cuUKevbsiWbNmmHp0qXw8PCAkZER9uzZg2XLlt0zxm3btqGsrAxLlizBkiVLdOZv2rQJixYtqlWMtY1JX1+/yvVUVy5uuxFDKZiU1bP33nsPO3fuxBtvvIFLly5h5MiRUk9IcnIyDhw4gPnz5yMnJwc//fST7JKNnZ2d1PU7atQohIeH6wzS1WrVqpX0TViruhPxu+++w8CBA6Gvr4/+/fvfNWGsjTlz5iAoKOiuDwx87rnn8Omnn2LUqFH45ptv4OPjgxkzZty1d6i2vLy88Ndff+k8H+fSpUvS/Npo0qQJzpw5g549e9bogzsuLk6n7PLlyzAzM4Ojo6Os/Omnn5YGzQ8cOBDh4eHYu3dvtR+ojo6OsLKywvnz52u1D7cTQmD27Nl44YUXZF8CbtekSZMH2sadvLy8EBsbq1NeVZtUVFQgPj4evXv3vud69fX1Zed9v379YGdnh71798qSMl9fX+mDNyQkBJ6envjpp5/wxhtvwMvLCxqNBnFxcVLvDnDry1Jubq7O+eLq6irbZtOmTdG5c2fs3LkTI0aMwC+//AITExP8+eefstfz7ZfMbteoUSOd1+7y5curTMpq2naFhYU667wbe3t7DBw4EK+//joyMjIwbNiwGt8Z7OXlhbNnz0Kj0ch6y+739XY/Tp8+jc2bN2Pnzp3Vvu/d7tVXX8WoUaNgbW2tc+f4nWpyzAFAT09POuYDBw7EzZs3MX/+fCkp+/nnnxEaGipLWEpLS5Gbm3vvHbyD9phW95pycHCAubm5rDwuLg6NGzeW/o6Pj4dGo5FuCNi9ezfKysqwa9cuWY/TncMzqrNp0ya0bNkSCxYs0Jm3evVq/PTTT1i0aJHUq13V++Sd+/OgMT2KePmynrm4uCAiIgJ9+vSRnkm2ceNGFBUVwcvLC6+++ipMTU2xe/du2ZgMrVdeeQUXL17ErFmzoK+vj+HDhz9wTNoetX79+mH48OGYNWtWtXcr1VRERAR+/fVXfPzxx3dNXK5du4aJEyfizTffxIQJExASEgJbW9sH2vad+vbtC7VajZUrV8rKly1bBpVKhT59+tRqfcOGDUNKSkqVd12VlJToXCaIiIiQjRG5du0afv31Vzz//PN3/cAQQkAIcdc6enp6GDx4MHbv3q3zyBLtOu5l8+bNOHv2bJV3yGoNGTIEZ86c0bnzsKbbuFPfvn0RGRkp62koKirCt99+C29vbwQEBEjlv/76K0pKSqQxe7Whje1ux1CbbGgfd6G9E/DOLzzantF73S1ZUlIiW5++vj5UKpXsi0ZiYqLOXbr3oyZtN2zYMERERODPP//UmZebm1vtkIVXX30VZ8+exdChQ2v1iyN9+/ZFWloatmzZIpVVVlZixYoVsLCwQPfu3Wu8rvs1e/ZsdOnSBQMHDqxR/d69e8Pc3Bw3b96UJe9Vqckxr0pWVpbskSr6+vo6r50VK1bc1xdSV1dXBAUFYf369bKk7vz589i3b5/O3a0AsGrVKp1tA5DeD7WvmdtjzMvLq/bLxO2uXbuGf/75B8OGDcOLL76oM40dOxbx8fE4fvw49PX10atXL+zcuVP2KJmYmBidc/ZBYnpUsafsIfDw8MCvv/6K1NRUhIeH47PPPkN0dDS++eYbBAUFISgoqNpEpl+/frC3t8e2bdvQp0+fGt00UBtffPEFmjdvjilTpmDr1q2yeREREbJvy9qbEOLj4xEZGYmOHTtK8/bt24fnnnvurt/ONRoNXnnlFTRq1Agff/xxne7H7QYMGIBnnnkGc+fORWJiIgIDA7Fv3z78+uuvmDZtGpo0aVKr9b3yyivYunUr/vvf/+LgwYPo0qUL1Go1Ll26hK1bt+LPP/+UDbpv2bIlevXqJXskBoAqu+7//vtv2eXL+Ph4TJs27a7xfPTRR9i3bx+6d+8uPaIjNTUV27Ztw9GjR2FjY3PX5fft24fx48dXeRlca9asWfj5558xdOhQvPrqq2jXrh1u3ryJXbt24ZtvvkFgYOBdt3Gn2bNn43//+x/69OmDN998E3Z2dli/fj0SEhLwyy+/QE9PD8XFxViwYAG++uordO7cGc8///w916tWq7F3714Aty5frl27FkVFRdLjHfbs2YPvvvsOnTt3hp2dHa5evYo1a9bA3NxcuokhMDAQoaGh+Pbbb5Gbm4vu3bsjMjIS69evx+DBg/HMM8/Itnn16lVs3LgRwK0bW1auXAkrKytpsH+/fv2wdOlS9O7dGyNHjkRGRgZWrVoFX19fnD17tlbH7U41bbtdu3ahf//+GDNmDNq1a4eioiKcO3cOP//8MxITE2WPNNHq3bs3MjMza/0TcBMmTMDq1asxZswYREVFwdvbGz///LPUs/8wnua+b98+hIeH17i+vr4+YmJiIITQ6VGqat33OuZDhgyBr68vmjRpgvLycuzduxe///677LJ0//79sWHDBlhbWyMgIAARERH466+/7vvGp88++wx9+vRBcHAwxo0bJz0So7rev4SEBAwcOBC9e/dGREQENm7ciJEjR0qv5eeffx5GRkYYMGAAXn/9dRQWFmLNmjVwcnJCamrqXWP56aefpEcRVaVv374wMDDApk2b0KlTJyxatAh79+5Ft27dMHHiRCmJb9Gihew18iAxPbIa4I7PJ15tb+XX3rr8008/6cy730di3G79+vUCgNi1a5es3t2m22/TBiBUKpWIioqSrffO27k/+ugjYWxsLLsFW1uvLh+JIcStxy289dZbws3NTRgaGgo/Pz/x2WefVfvIiLs9EkOIW7f4f/LJJ6JFixbC2NhY2Nrainbt2olFixaJvLw8nRg3btwo/Pz8hLGxsWjTpo3Ordfa29S1k6mpqQgICNC5dbw6SUlJYvTo0cLR0VEYGxsLHx8fMWnSJFFWVlbtMtpb/E1NTUVKSopsXlWPesjOzhaTJ08W7u7uwsjISDRq1EiEhoZKj1CpzbknhBBXrlwRL774orCxsREmJiaiY8eO4rfffpPmX79+XXh4eIhp06bJjqkWqngkxu3H0MLCQrRt21Zs2LBBqnP+/Hnx/PPPC3t7e2FkZCQ8PDzE8OHDxdmzZ2XrrqioEIsWLRKNGzcWhoaGwsPDQ8yZM0eUlpbqHKfbt+ng4CCef/55ERERIav3/fffS+3frFkzsXbtWp3HN2j3qTaPxKhp2xUUFIg5c+YIX19fYWRkJBwcHETnzp3F559/LsrLy2XrrO6RF/eaf7v09HQxduxY4eDgIIyMjESrVq1kj5K41/oe9JEYgwYNkpVrz82qHolRnTvn1+aYL1y4UDRt2lSYmpoKKysrERQUJL744gtRUVEh1cnJyZGOkYWFhejVq5e4dOlSle13e0zVHRchhPjrr79Ely5dpO0OGDBAXLx4UVZHe4wuXrwoXnzxRWFpaSlsbW3F5MmTRUlJiazurl27ROvWrYWJiYnw9vYWn3zyifjhhx/u+t4ohBCtWrUSnp6e1c4XQogePXoIJycn6ZgcPnxYtGvXThgZGQkfHx/xzTffVPkaqWlMXl5eol+/fjrbreo1Vptz+2FTCaHAkW4k89Zbb+H7779HWlqa9LDLhrRw4UIcOnRI9rRsukWlUmHSpEk6l06JiBrCwoULsWjRImRmZlbZQ0rKwjFlCldaWoqNGzdiyJAhikjIiIiIqH5wTJlCZWRk4K+//sLPP/+M7OxsnZ8qaUi+vr4oLi5u6DCIiIgeK0zKFOrixYsYNWoUnJyc8OWXXyIoKKihQ5LUxe9uEhERkRzHlBEREREpAMeUERERESkAkzIiIiIiBXjixpRpNBrcuHEDlpaWNf6tOyIiIqL7JYRAQUEB3NzcZD9HdqcnLim7cePGA/2ANBEREdH9uHbtGho1alTt/CcuKdP+5Me1a9dgZWXVwNEQERHR4y4/Px8eHh73/NmxJy4p016ytLKyYlJGRERED829hk1xoD8RERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESmAQUMH8LgpLi5GQEAA3N3d4ebmJvvX2toahoaGssnOzg4eHh4wNTVt6NCJiIioATEpq2M3btxAUlISkpKSarWcg4MDPD094enpCVtbWxgZGcHY2FiaDA0NYWBgIJuEEKisrJQmtVoNS0tLODs7yyZtMqhSqaTtFRUV4fz58zh79izOnDmDCxcuwMzMDN7e3rLJw8MDTk5O0NNruE7VGzdu4Pjx42jVqhV8fX0bLA4iIqL6pBJCiIYO4mHKz8+HtbU18vLyYGVlVefrLykpQXR0NFJSUnDjxg3Zv4WFhaioqJCm8vJyZGZmoqioqM7juJNKpYKpqSlMTExgZGSE9PR01LTpDQwM4OrqKvX4OTs7w8HBQZrs7e1hYHArvxdCSFNJSQmKi4tRVFSEoqIilJSUwMbGBq6urtL6nJ2dYWhoKNteRUUFIiIi8Mcff+CPP/7AmTNnpHlt27bFsGHDMHToUPj4+Ej1ExMTcfnyZSQnJ8PLywuBgYFwc3OTJaKPgvLycpw7dw7Jycno1KkT3NzcGjokIiJ6QDXNPZiUNTAhBHJzc5GcnIxr164hOTkZ+fn5KCsrQ3l5OcrKylBWVobKykpUVFRIvWIVFRXQ09OT9Zzp6+sjPz8faWlpSE9PR3p6OrKzs6vdtrOzMwIDAxEYGIiWLVuivLwciYmJ0pSQkIC0tDRoNJp6PQb6+vrQ09OTJu3+aalUKjRr1gyXL1+GWq2Wylu3bo2ysjJcuXIFlZWVOut1cHCQ9g0ACgoKUFhYiMLCQpSUlMDS0hL29vawt7eHnZ0drKyscPPmTaSlpUlTTk4OGjdujFatWkmTv78/CgsLkZGRIU03b96U9kU7aS9POzo6SgmsmZkZiouLkZ2djezsbNy8eRPXr1/HyZMnceLECURHR6OsrEzah8DAQPTu3Ru9e/dGly5ddBLYR4lGo8GFCxeQmZmJoKAg2NnZNXRIREQPBZOyaigtKatvFRUVKC4uRklJCUpLS6V/tb1U91JZWYm0tDSkpKRIU2ZmJrKzs5GVlYWsrCxkZ2dDrVZLvVIqlQoqlQomJiYwNzeHubk5zMzMYGJigpycHKSmpiI1NRVpaWlVJlMAYG9vj169eqFv3754/vnn4ejoiKysLGzfvh1bt27FwYMHZcmiqakp/Pz84OnpiYSEBFy6dEmWwCmFvr7+PeOytbVFo0aNcP78eVlvppGREaysrGBqaiqbtMf3zmOt7RnV9o7eqbKyEkVFRVKiWlBQgIqKClhaWsLKykqaTExMUFZWhpKSEun80SaOt8dnaGgIe3t7ODg4SIloSUkJjh07hvDwcERERCAvL0+q7+Pjgw4dOqBDhw5o2rQp9PX1ZfFVVFSgoKBANmk0GtjY2MDW1hZ2dnbSv9rE+vaxmYWFhbhy5Qri4+MRHx+PiooK+Pn5wd/fH35+frCwsJBtTwiB4uJi6Ovrw8TEpAateUt5ebmUnOvr68PLyws2NjY1Xr46Go1Gei1R/aisrERSUhLc3Nweu3G92qsVZmZmDR1KjVRWViI6OhrZ2dno3LkzLC0tH8p2hRCIjIzE/v378d5779XbdpiUVeNJS8qUTKPR4ObNmygvL4dGo5EmlUqFRo0a6XxI3y4jIwOHDh2Cvb09/P394e7uLhv3VlpaigsXLiA6OhqXLl2CoaEhLCwsYGlpCQsLC5iYmCA/P1/qrcrOzkZ+fj7s7Ozg6uoKFxcXuLi4wMrKCnFxcTh37pw0aRMLOzs7ODk5wcnJCXZ2dlCpVFCr1dJUXl4uJa+ZmZkoLy+X4tMmMPb29nB0dERQUBA6duyIjh07wsfHByqVCpmZmdi/fz/++OMP/Pnnn8jMzKy/xnhILCws4OjoiISEhHpZv6mpKezt7aUvE3fj5uYGW1tbFBQUID8/H/n5+VKib2VlJZ0DLi4uMDU1lS7Ha//NyclBeno6cnJydNZtZWUFLy8vKUG7fdxnZWUlSktLpYRYe3m/tLRUNrxBCAEzMzN4eXnB09NTtj49PT1Zr2x5ebmsJ7iwsBBCCBgZGcHQ0FD6V61Wo7S0VDYZGBjAwsJCSurNzc2lHtnbk8Ls7Gxcv34dKSkpuH79OlJTU2FpaSkbg+rl5QUzMzPZMAa1Wo309HQkJydLU0pKCkxNTeHi4gJnZ2fpXz09PZSXl8smQ0NDGBsbS18wTExMYG1tDTs7O2mytbWFqakpjIyMpEmlUknjbrXrKigowKlTp/Dvv//i+PHjOHHiBIqKiqCvr4+WLVuiQ4cOaN++Pdq1awc3NzfY2NjA1NRUOgZCCBQUFEjDUjIyMmBkZCR9KTIzM4OZmRmMjIxgYGAg3dBlZGQES0tLnfc0jUaDhIQE6b0lMTERjRo1gq+vL3x9feHn5wd7e/saJ+YVFRU4evQodu3ahV27duHq1avw9fVF165d0a1bN3Tt2hV+fn73XJ/2C8qd54r2qs3tU0VFhXQOaychBDw9PdG4cWM0btxYtg/aZDE3NxdXr17FP//8g3/++Qfh4eEoLCwEcOv9sVu3bujbty/69u2LZs2aVRuzRqNBWloakpOTUV5ejmbNmsHJyemex+rmzZvYuHEj1qxZg/PnzwMAzp49i1atWtXoWNcWk7JqMCmjByGEwM2bN2FlZVWrS4lCCBQVFSEvLw/W1tYwNzevVQ+IRqNBcnKyNDbvzgRB+2ao/b+2Z+v2N9M7t6enpyclqdqE1dDQUJak5Ofno6SkRPow1PbOGRsby3pGAaCsrAzZ2dnIzMyUElEA6NSpE7p06YKuXbuiVatWMDAwQE5ODqKionDixAmcOHGiyhtjDAwMYGlpKfXcWVpaQqVSITc3Fzdv3kROTg5ycnKkxLqqHkh7e3vpA87Q0BBxcXG4fPlynSe4+vr6cHJyQkVFBbKysup03XR/DAwMqu2Jr009AwMD2NjYwNzcHFlZWQ80BtjS0hLW1tawsbGBoaEhLl++fM/1WVlZwdHRUfoSZ29vLyXmt8vIyMCff/5Z5ZeE29na2sLNzU02LtjU1BTp6elSsnnjxg0UFxff937eSftlrKCgALm5udUeb2tra9ja2iIxMVFWbm9vD2tra1nyW1lZiWvXruH69es663NwcECLFi3QokULeHt7y4bHqFQqHD9+HL/88ovU429iYoJhw4Zh7ty58Pf3r7P9vh2TsmowKSN6/AghpJ7P7OxsqFQqNGnSBLa2tlXWz8nJQVxcHPLz82WXaq2srFBZWYn09HTZ2MKysjKYmZlJSamZmRmsrKykO5zt7OykD8mioiIkJycjMTERSUlJKCoqksZ8av/VXtq/vYfK1NRUusta28OSk5Mj3c2dnJyMpKQkFBYWQqPRyHpljY2NpeRau05tr9PtNxZpt62djI2NZZextcl9ZWWlrLdLCAE7Ozs0atQI7u7u0qN+8vPzZeNQk5KSpC8AKpVK+hB0cnKCh4eHdId5o0aNUFJSIo19TUtLQ0ZGhtS7p50MDQ2lnsWysjJpCEZeXh5u3rwpm2oyXEFPTw8BAQF46qmnpKlZs2a4ceOG9AXh5MmTOHPmzF3XaW1tDTc3Nzg5OUGtVktfiLRfirTHXNubdDdGRkYICAhA69at4ePjg5SUFMTHxyMuLg7Xr1+/5z7dycHBAf3798fAgQPRqVMnnDlzBkeOHMHRo0cRGRkpG7NaEwYGBrKbxLTnqPZ8NjQ0lPWympubQ6PRICkpCQkJCbhx40aV69XT04OzszO6dOmCp59+Gk8//TRatmwJfX19xMXFYc+ePdizZw8OHz58z5j19fXh7u4OfX19JCYm1vgmtqCgIIwfPx4jR46skyEHd8OkrBpMyoiIHj/aIQN3Xvq8PcnT19evcQ+1tnc7NzcXubm5KCgogIODA9zc3GBubl7juIQQKC8vR35+PvLy8pCbm4u8vDwUFxdLlyi1d6/fqaSkBElJSdLYXe2Um5sr1dHuj4mJCZ555hk89dRT1Q79KCsrQ2xsrDQeWDsVFhbCxcUFbm5u0uTs7AxTU9NqY6up0tJSJCUlITs7G1ZWVrCxsYG1tTUsLCxq1BZFRUWIj4+XXQkoLi6GSqWSknwXFxcpzqKiIly6dAkXLlzAhQsXcOPGDemLhUajgRACzs7OeOWVV9CuXbuHNmaTSVk1mJQRERHRw1TT3IM/s0RERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpACKSMpWrVoFb29vmJiYoFOnToiMjKy27rp166TfVdNOJiYmDzFaIiIiorrX4EnZli1bMH36dCxYsACnTp1CYGAgevXqhYyMjGqXsbKyQmpqqjQlJSU9xIiJiIiI6l6DJ2VLly7F+PHjMXbsWAQEBOCbb76BmZkZfvjhh2qXUalUcHFxkSZnZ+eHGDERERFR3WvQpKy8vBxRUVEICQmRyvT09BASEoKIiIhqlyssLISXlxc8PDwwaNAgXLhwodq6ZWVlyM/Pl01EREREStOgSVlWVhbUarVOT5ezszPS0tKqXKZp06b44Ycf8Ouvv2Ljxo3QaDTo3Lkzrl+/XmX9sLAwWFtbS5OHh0ed7wcRERHRg2rwy5e1FRwcjNGjRyMoKAjdu3fH9u3b4ejoiNWrV1dZf86cOcjLy5Oma9euPeSIiYiIiO7NoCE37uDgAH19faSnp8vK09PT4eLiUqN1GBoaok2bNoiPj69yvrGxMYyNjR84ViIiIqL61KA9ZUZGRmjXrh0OHDgglWk0Ghw4cADBwcE1Wodarca5c+fg6upaX2ESERER1bsG7SkDgOnTpyM0NBTt27dHx44dsXz5chQVFWHs2LEAgNGjR8Pd3R1hYWEAgPfffx9PPfUUfH19kZubi88++wxJSUl47bXXGnI3iIiIiB5IgydlL730EjIzMzF//nykpaUhKCgIe/fulQb/JycnQ0/v/zv0cnJyMH78eKSlpcHW1hbt2rXDsWPHEBAQ0FC7QERERPTAVEII0dBBPEz5+fmwtrZGXl4erKysGjocIiIieszVNPd45O6+JCIiInocMSkjIiIiUgAmZUREREQKwKSMiIiISAGYlBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBWBSRkRERKQATMqIiIiIFIBJGREREZECMCkjIiIiUgAmZUREREQKwKSMiIiISAGYlBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBWBSRkRERKQATMqIiIiIFIBJGREREZECMCkjIiIiUgAmZUREREQKwKSMiIiISAGYlBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAigiKVu1ahW8vb1hYmKCTp06ITIyskbLbd68GSqVCoMHD67fAImIiIjqWYMnZVu2bMH06dOxYMECnDp1CoGBgejVqxcyMjLuulxiYiJmzpyJbt26PaRIiYiIiOpPgydlS5cuxfjx4zF27FgEBATgm2++gZmZGX744Ydql1Gr1Rg1ahQWLVoEHx+fhxgtERERUf1o0KSsvLwcUVFRCAkJkcr09PQQEhKCiIiIapd7//334eTkhHHjxt1zG2VlZcjPz5dNRERERErToElZVlYW1Go1nJ2dZeXOzs5IS0urcpmjR4/i+++/x5o1a2q0jbCwMFhbW0uTh4fHA8dNREREVNca/PJlbRQUFOCVV17BmjVr4ODgUKNl5syZg7y8PGm6du1aPUdJREREVHsGDblxBwcH6OvrIz09XVaenp4OFxcXnfpXrlxBYmIiBgwYIJVpNBoAgIGBAWJjY9GkSRPZMsbGxjA2Nq6H6ImIiIjqToP2lBkZGaFdu3Y4cOCAVKbRaHDgwAEEBwfr1G/WrBnOnTuH6OhoaRo4cCCeeeYZREdH89IkERERPbIatKcMAKZPn47Q0FC0b98eHTt2xPLly1FUVISxY8cCAEaPHg13d3eEhYXBxMQELVu2lC1vY2MDADrlRERERI+SBk/KXnrpJWRmZmL+/PlIS0tDUFAQ9u7dKw3+T05Ohp7eIzX0jYiIiKjWVEII0dBBPEz5+fmwtrZGXl4erKysGjocIiIieszVNPdgFxQRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBWBSRkRERKQATMqIiIiIFIBJGREREZECMCkjIiIiUgAmZUREREQKwKSMiIiISAGYlBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBWBSRkRERKQATMqIiIiIFIBJGREREZECMCkjIiIiUgCDB1n4/PnzOHz4MNRqNbp06YJ27drVVVxERERET5T77ilbtWoVevbsicOHD+PgwYN49tln8eGHH9ZlbERERERPDJUQQtSk4rVr1+Dh4SH93bx5cxw5cgQODg4AgIiICAwcOBCZmZn1E2kdyc/Ph7W1NfLy8mBlZdXQ4RAREdFjrqa5R417ykJCQvDFF19Am8PZ29tj7969KCsrQ0FBAf766y84Ojo+eORERERET6AaJ2UnTpxAbGwsOnXqhOjoaHz77bdYtmwZTE1NYWNjgy1btmD9+vX1GSsRERHRY6vGA/2trKzw1Vdf4dixYxgzZgyeffZZHDlyBGq1Gmq1GjY2NvUYJhEREdHjrdYD/Tt37oyTJ0/C1tYWbdq0wT///MOEjIiIiOgB1Xigf2VlJb799lvExMQgMDAQY8eOxZUrV/Df//4X9vb2WLlyJZydnes73gfGgf5ERET0MNX5QP9x48Zh5cqVMDc3x9q1a/HWW2/B398ff//9N3r37o3g4GB8/fXXdRI8ERER0ZOmxj1lNjY2iIiIQPPmzVFcXIxWrVrhypUr0vyMjAxMmzYNP/30U70FWxfYU0ZEREQPU533lDk7O2Pfvn0oLy/H33//DXt7e9l8JycnxSdkREREREpV47svV65ciVGjRmH69OlwdXXF1q1b6zMuIiIioidKjZOy5557Dunp6cjKyuJDYonokaZWq1FRUdHQYRDRY8LQ0BD6+voPvJ5a/SC5SqViQkZEjywhBNLS0pCbm9vQoRDRY8bGxgYuLi5QqVT3vY5aJWVERI8ybULm5OQEMzOzB3rzJCICbn3ZKy4uRkZGBgDA1dX1vteliKRs1apV+Oyzz5CWlobAwECsWLECHTt2rLLu9u3b8dFHHyE+Ph4VFRXw8/PDjBkz8MorrzzkqInoUaJWq6WE7M4blYiIHoSpqSmAW0+icHJyuu9LmbV+on9d27JlC6ZPn44FCxbg1KlTCAwMRK9evaSM8052dnaYO3cuIiIicPbsWYwdOxZjx47Fn3/++ZAjJ6JHiXYMmZmZWQNHQkSPI+17y4OMV23wpGzp0qUYP348xo4di4CAAHzzzTcwMzPDDz/8UGX9Hj164IUXXkDz5s3RpEkTTJ06Fa1bt8bRo0cfcuRE9CjiJUsiqg918d5Sp0nZjz/+KHug7L2Ul5cjKioKISEh/x+Qnh5CQkIQERFxz+WFEDhw4ABiY2Px9NNPV1mnrKwM+fn5somIiOhueHeucjxJbVGnSdmYMWMQEBCAKVOm1Kh+VlYW1Gq1zm9mOjs7Iy0trdrl8vLyYGFhASMjI/Tr1w8rVqzAc889V2XdsLAwWFtbS5OHh0fNd4iIiB57lZWVWLp0Kbp06QJ3d3eYmJhg3rx5DR1Wgzh37hz69OmD8vJynD17ttrx3fUpOjoaoaGh8Pf3h62tLaysrJCXl/fQ42gIdTrQX6PRICEhAX/88UddrlaHpaUloqOjUVhYiAMHDmD69Onw8fFBjx49dOrOmTMH06dPl/7Oz89nYkZEj4wxY8Zg/fr11c7PycmBjY3NwwvoMSOEwIABA5CSkoJFixahRYsW0NPTg7u7e0OH1iBatmwJQ0NDmJubQ19fH+vWrXuo2z906BD69++PSZMmYfPmzbCysoKpqSmsra0fahwNpc7vvmzcuDEmTpxYo7oODg7Q19dHenq6rDw9PR0uLi7VLqenpwdfX18AQFBQEGJiYhAWFlZlUmZsbAxjY+Oa7wARkcL07t0ba9eulZUdO3YMQ4YMaaCIHh8bN25EYmIiTpw4AQsLi4YOp8GpVCrs2rULGRkZsLCweKg3xgghMH78eCxfvhyvvfbaQ9uuktT68uWd47MeZLyWkZER2rVrhwMHDkhlGo0GBw4cQHBwcI3Xo9FoUFZWVqttExE9KoyNjeHi4iKb7OzsdOr98ssvaNGiBYyNjeHt7Y0lS5ZI83r06AGVSlXltHDhQgC3xuDOnDkT7u7uMDc3R6dOnXDo0CFpHevWrYONjQ127twJPz8/mJiYoFevXrh27ZpUZ+HChQgKCpL+Li8vh6+vL1Qq1V0f2qtSqbBz505ZWY8ePTBt2jTp7w0bNqB9+/awtLSEi4sLRo4cWe2d+lo5OTkYPXo0bG1tYWZmhj59+iAuLk6a/9tvvyEgIAD9+vWDpaUlnJ2d8dZbb6G8vBzArbHS9vb2Op8xgwcPlh7FdGeciYmJUKlUiI6OBnDrcSzjxo1D48aNYWpqiqZNm+KLL76QrW/MmDEYPHiw9Pe6devQokULmJqawtfXF2vWrLnr8Zo2bZqsY+LOGAAgNzcXr732GhwdHWFlZYVnn30WZ86ckebf3nZOTk4wMDCocdtpJysrKzz33HOy8eXe3t5Yvnx5lcsOHjwYY8aMAQBcunQJSUlJiI+Ph5eXF0xMTPDUU0/p3Mh3+PBhdOzYEcbGxnB1dcXs2bNRWVkpze/RowcmT56MyZMnw9raGg4ODpg3bx6EENXG9N5776FRo0ZITEyUyo4ePYpu3brB1NQUHh4eePPNN1FUVFTtcagLtU7KbG1tq5xsbGxga2tb6wCmT5+ONWvWYP369YiJicEbb7yBoqIijB07FgAwevRozJkzR6ofFhaG/fv34+rVq4iJicGSJUuwYcMGvPzyy7XeNhERABQVFaGoqEj2pl1eXo6ioiKdD2NtXY1GI5VVVFSgqKgIpaWl96xbX6KiojBs2DAMHz4c586dw8KFCzFv3jzp8tP27duRmpqK1NRUBAcHY8aMGdLfM2fOBABMnjwZERER2Lx5M86ePYuhQ4eid+/esiSmuLgYH374IX788UeEh4cjNzcXw4cPrzaulStX6lwNuV8VFRVYvHgxzpw5g507dyIxMVH6QK/OmDFjcPLkSezatQsREREQQqBv377S4PHMzExs374dLVq0QGRkJH744Qds3rxZ+twZOnQo1Go1du3aJa0zIyMDv//+O1599dUaxa3RaNCoUSNs27YNFy9exPz58/Huu+9W+xvSmzdvxrhx4zBu3DicOXMGM2bMwKRJk7B79+4aba86Q4cORUZGBv744w9ERUWhbdu26NmzJ27evFll/dq03dq1a5Gamop//vkHGRkZePfdd2sdX2ZmJioqKrBhwwZ8/fXXOH36NIKCgtC7d2+kpqYCAFJSUtC3b1906NABZ86cwddff43vv/8eH3zwgWxd69evh4GBASIjI/HFF19g6dKl+O6776rc7pIlS7B69Wrs378f3t7eAIArV66gd+/eGDJkCM6ePYstW7bg6NGjmDx5cq33q1ZELTVp0kRYWlqKDz/8UBw6dEhnuh8rVqwQnp6ewsjISHTs2FH8+++/0rzu3buL0NBQ6e+5c+cKX19fYWJiImxtbUVwcLDYvHlzjbeVl5cnAIi8vLz7ipWIHk0lJSXi4sWLoqSkRGceAAFAZGRkSGUffPCBACBee+01WV0zMzMBQCQkJEhly5YtEwDEyJEjZXUdHBwEAHH+/Pn7jjs0NFQMGjRIp/zgwYMCgMjJyRFCCDFy5Ejx3HPPyerMmjVLBAQE6CzbvXt3sWDBAllZUlKS0NfXFykpKbLynj17ijlz5gghhFi7dq0AIHuPjomJEQDE8ePHhRBCLFiwQAQGBgohhMjOzha2trZi8eLFslirAkDs2LFDJ86pU6dWu8yJEycEAFFQUFDl/MuXLwsAIjw8XCrLysoSpqamYuvWrdI2mjZtKjQajVRnw4YNwsjISBQVFQkhhHjjjTdEnz59pPlLliwRPj4+0jK9e/cWEyZMkOYnJCQIAOL06dPVxj5p0iQxZMgQ6e/b27ljx45i6NChsvrjx48XXbp0kf6+83hNnTpVdO/evdoYjhw5IqysrERpaalsvU2aNBGrV68WQtRN2+Xm5oouXbqI8ePHS/O9vLzEsmXLqlx20KBB0me89pzetGmTNF+tVgs/Pz8xd+5cIYQQ7777rk57rVq1SlhYWAi1Wi2EuNWmzZs3l9V55513RPPmzXViWrNmjbCyshInT56UxTVu3DhZmwpx6xjq6elV+R4ixN3fY2qae9S6pywmJgYLFy7EkiVLsHLlSnh6eqJ79+7SdD8mT56MpKQklJWV4fjx4+jUqZM079ChQ7KBhh988AHi4uJQUlKCmzdv4tixY3jppZfua7tERI+LmJgYdOnSRVbWpUsXxMXFQa1W33P5c+fOQa1Ww9/fHxYWFtJ0+PBh2aUoAwMDdOjQQfq7WbNmsLGxQUxMjM4633//fTzzzDPo2rVrjfZhxIgRsm0fOXJENj8qKgoDBgyAp6cnLC0tpc+c5OTkKtcXExMDAwMD2WeKvb09mjZtKos3ODhY9oyprl27ory8HPHx8QCA8ePHY9++fUhJSQFw69LimDFjpGVatmyJ/fv3IzMzs9p9W7VqFdq1awdHR0dYWFjg22+/1Yn7t99+g4WFBSIjI6tsy4sXL1a7/ns5c+YMCgsLYW9vLzvGCQkJVT7K6n7bztbWFgUFBQgLC5PNf+edd2BhYQEnJyf06NED4eHh1a7r9n3X09ND586dpX2PiYnRaa8uXbqgsLAQ169fl8qeeuopWZ3g4GCd18Kvv/6K119/HW5ubmjZsqUshjNnzmDdunWyY9WrVy/phsb6UuukzNDQENOnT0dcXBzc3d3RunVrzJgxgz/wS0SPrMLCQhQWFsLBwUEqmzVrFgoLC7Fy5UpZ3YyMDBQWFsLT01MqmzRpEgoLC/H999/L6iYmJqKwsBDNmzev3x2oA4WFhdDX10dUVBSio6OlKSYmRmf8U03ExcXhu+++wyeffFLjZZYtWybbdvv27aV5RUVF6NWrF6ysrLBp0yacOHECO3bsAABp/Nf9uNuwG+2Heps2bRAYGIgff/wRUVFRuHDhguyy6cyZM2FtbQ0XFxdYWFigRYsWsvVs3rwZM2fOxLhx47Bv3z5ER0dj7NixOnE/88wziI6OrvbOzwd5OGlhYSFcXV1lxzc6OhqxsbGYNWuWrO6DtF1kZCRcXFx0LivPmjUL0dHR2L9/Pxo1aoQBAwbo7H9N2qIuhYeHY8uWLbJxlVqFhYV4/fXXZcfqzJkziIuLQ5MmTeo8Fq37vvvSzs4Oy5cvx+TJk/HOO+/A19cX7733nmywIxHRo8Dc3FynzMjICEZGRjWqa2hoCENDwxrVrS/NmzfX6X0IDw+Hv79/jX6Hr02bNlCr1cjIyEC3bt2qrVdZWYmTJ09Kz6+KjY1Fbm6uTuL5zjvv4LXXXoOvr6+sB+NuXFxcpDvrgf//PUHg1iDw7OxsfPzxx9JjjU6ePHnX9TVv3hyVlZU4fvw4OnfuDADIzs5GbGwsAgICANzq6duxYweEENIH/9GjR2FkZCT78H3ttdewfPlypKSkICQkRPZoJWdnZ5w+fRopKSkoKSlBSkqKbNB9eHg4OnfuLHsyQVW9U+bm5vD19UVAQADCw8MxdepU2Tq0Md+Ptm3bIi0tDQYGBtK4qeo8aNtNmTIFAwcOREVFhfS6cHBwkObPmTMHmzZt0ukpbNKkCQwMDBAeHg4vLy8At8bj3X5FrHnz5vjll19k7RUeHg5LS0s0atRIWtfx48dl6/7333/h5+cney3Mnj0bL774Ijw9PfH000/jP//5j9QL3LZtW1y8eFF2Pj4Mte4pa9OmDdq2bStNw4YNw9WrV1FWVoYZM2bUR4xERHQPM2bMwIEDB7B48WJcvnwZ69evx8qVK6VB/Pfi7++PUaNGYfTo0di+fTsSEhIQGRmJsLAw/P7771I9Q0NDTJkyBcePH0dUVBTGjBmDp556SvaQ0fj4eBw6dAjz58+vs/3z9PSEkZERVqxYgatXr2LXrl1YvHjxXZfx8/PDoEGDMH78eBw9ehRnzpzByy+/DHd3dwwaNAgA8MYbbyAxMRGTJk1CTEwM9uzZg1mzZmHy5Mmyx0GMHDkS169fx5o1a6od4O/u7g5fX18pobg9jpMnT+LPP//E5cuXMW/ePJw4caLauKdPn45ffvkFS5cuRVxcHL7++musW7cOb7/9tqxeRUUFSktLUVpaCrVaDY1GI/2tvUGlvLwcQgiEhIQgODgYgwcPxr59+5CYmIhjx45h7ty5suT2ftsuNzcXaWlpiI2Nxffffw8fHx/ZF5XKykqUlpYiOzsbP/zwQ5UPc7ewsMD48eMxa9Ys7NmzBzExMZg4cSJu3LghJbQTJ07EtWvXMGXKFFy6dAm//vorFixYgOnTp0NP7/9TmuTkZEyfPh2xsbH43//+hxUrVsiSXADSHcwdO3bEtGnTZL2X77zzDo4dO4bJkycjOjoacXFx+PXXX+t9oH+te8puv2WXiIiUoW3btti6dSvmz5+PxYsXw9XVFe+///4970683dq1a/HBBx9gxowZSElJgYODA5566in0799fqmNmZoZ33nkHI0eOREpKCrp166Zz2baoqAiLFi2q8rEd98vR0RHr1q3Du+++iy+//BJt27bF559/joEDB95zn6ZOnYr+/fujvLwcTz/9NPbs2SMlDJ6envjtt98we/ZsBAYGwtbWFqNGjdIZE2VtbY0hQ4bg999/r/Xn4Ouvv47Tp0/jpZdegkqlwogRIzBx4sRqH7Teu3dvrF69Gh9//DFmz54NLy8vrFq1CgMGDJDVGzZsmM6yt/cuAkCnTp2QkJAAb29v7NmzB3PnzsXYsWORmZkJFxcXPP3007Jf1bnfttM+McHS0hJt27bFzz//LJs/a9YszJo1C6ampmjZsiV27NhR5TNEP//8c6hUKoSGhiI/Px9t27bFn3/+CVdXVwC3El9t4hwYGAg7OzuMGzcO7733nmw9o0ePRklJCTp27Ah9fX1MnToVEyZMqDb+RYsWYdeuXVi4cCE++ugjtG7dGocPH8bcuXPRrVs3CCHQpEmTeh/DrhLitnvAnwD5+fmwtrZGXl4erKysGjocInpISktLkZCQgMaNG8PExKShw3kkrVu3DtOmTXtixxD37NkTLVq0wJdfftnQodRYUFAQdu7cec9Llo+THj16ICgoqNpno9WXu73H1DT3uO8xZSdPnpTuXgkICEC7du3ud1VERESKlZOTg0OHDuHQoUP46quvGjqcWjE2Nq6XQfJUP2qdlF2/fh0jRoxAeHi49Htrubm56Ny5MzZv3iwbaEdERPSoa9OmDXJycvDJJ5+gadOmDR1Ordw54J2UrdaXL3v37o3c3FysX79eOjljY2MxduxYWFlZYe/evfUSaF3h5UuiJxMvXxJRfWqQy5eHDx/GsWPHZN8WmjZtihUrVtz1NmoiIiIiql6tH4nh4eEh/WbY7dRqNdzc3OokKCIiIqInTa2Tss8++wxTpkyRPdfk5MmTmDp1Kj7//PM6DY6IiIjoSVHrMWW2trYoLi5GZWUlDAxuXf3U/v/Op1dX98vzDYljyoieTBxTRkT1qUHGlD3s534QERERPQlqnZSFhobWRxxERET0GLn9ty+pZmo9pgy49UOq7733HkaMGIGMjAwAwB9//IELFy7UaXBERER0b7t378b48eOh0Wjw+++/48UXX3zoMRw6dAgvvvgimjRpAmtra3h5eeEJ+9GgB1brpOzw4cNo1aoVjh8/ju3bt6OwsBAAcObMGSxYsKDOAyQiepKNGTMGKpWq2ulJ/ckjkgsJCUF0dDSMjY0xcuRITJky5aFuf9OmTRgwYADat2+PHTt2ICoqCqdPn+avCdRSrS9fzp49Gx988AGmT58OS0tLqfzZZ5/FypUr6zQ4IiK69dDutWvXysqOHTuGIUOGNFBEpDSmpqaIjIxEWloa7Ozsqvyx7/pSWFiIyZMnY/v27Xjuuece2nYfR7XuKTt37hxeeOEFnXInJydkZWXVSVBERPT/jI2N4eLiIpvs7Ox06v3yyy9o0aIFjI2N4e3tjSVLlkjzevToUW1v28KFCwEAZWVlmDlzJtzd3WFubo5OnTrh0KFD0jrWrVsHGxsb7Ny5E35+fjAxMUGvXr1w7do1qc7ChQsRFBQk/V1eXg5fX9979uqpVCrs3LlTVtajRw9MmzZN+nvDhg1o3749LC0t4eLigpEjR0pDaKpSF/sMAOHh4ejRowfMzMxga2uLXr16IScn5669mGPGjJHW/+abb8LJyQkmJibo2rUrTpw4Ia370KFD0jJ6enpwcnLCuHHjUFpaKtU5d+4cnn32WZiamsLe3h4TJkyQrlIBt3pTBw8eDJVKBVdXVxQWFsLW1lb6KcSqJCYmyuK1s7PDf/7zH2RnZ9+1TbSCgoKkY3jkyBGYmppix44dcHNzg5mZGUJCQnSGNN3t/AQAb29vLF68GCNGjIC5uTnc3d2xatUqWZ3bYxJCYPTo0WjdujVycnKkOr/++ivatm0LExMT+Pj4YNGiRaisrKz2WChJrZMyGxsbpKam6pSfPn0a7u7udRIUEdHDIIRAUVHRQ5/qY5xNVFQUhg0bhuHDh+PcuXNYuHAh5s2bh3Xr1gEAtm/fjtTUVKSmpiI4OBgzZsyQ/p45cyYAYPLkyYiIiMDmzZtx9uxZDB06FL1790ZcXJy0neLiYnz44Yf48ccfER4ejtzcXAwfPrzauFauXIn09PQ62ceKigosXrwYZ86cwc6dO5GYmCglP1Wpi32Ojo5Gz549ERAQgIiICBw9ehQDBgyAWq3GF198Ia1v2LBhGDZsmPT3F198AQB4++238csvv2D9+vU4deoUfH190atXL51HRsXGxiIlJQUbN27Eli1bpJ7RoqIi9OrVC7a2tjhx4gS2bduGv/76C5MnT652v2uThPz1119ITU3F77//jsjISHz66ac1Wu52mZmZSE1NxaFDh7B582YcP34clpaW6N27N0pKSgDc+/zU+uyzzxAYGIjTp09j9uzZmDp1Kvbv31/ldt98800cO3YM+/btg62tLYBbCeLo0aMxdepUXLx4EatXr8a6devw4Ycf1nq/GoSopRkzZoiuXbuK1NRUYWlpKeLi4sTRo0eFj4+PWLhwYW1X99Dl5eUJACIvL6+hQyGih6ikpERcvHhRlJSUSGWFhYUCwEOfCgsLaxx3aGioGDRokE75wYMHBQCRk5MjhBBi5MiR4rnnnpPVmTVrlggICNBZtnv37mLBggWysqSkJKGvry9SUlJk5T179hRz5swRQgixdu1aAUD8+++/0vyYmBgBQBw/flwIIcSCBQtEYGCgEEKI7OxsYWtrKxYvXiyLtSoAxI4dO3TinDp1arXLnDhxQgAQBQUF1da5fV33s88jRowQXbp0uef6Q0NDRWhoqKyssLBQGBoaik2bNkll5eXlws3NTXz66adCCN12jIuLE7a2ttIy3377rbC1tZWdM7///rvQ09MTaWlp0ra150hsbKwwNzcX8+bNE9bW1tXGm5CQIACI06dPCyGESE1NFb6+vuLDDz+U6lTVJlqBgYHS8dSeF+Hh4dL8/Px8YWNjI9asWSOEqNn56eXlJXr37i2r89JLL4k+ffroxDR37lzh7u4uEhISZPV79uwpPvroI1nZhg0bhKura7XHoq5U9R6jVdPco9Y9ZR999BGaNWsGDw8PFBYWIiAgAE8//TQ6d+6M9957r7arIyKiOhATE4MuXbrIyrp06YK4uDio1ep7Ln/u3Dmo1Wr4+/vDwsJCmg4fPowrV65I9QwMDNChQwfp72bNmsHGxgYxMTE663z//ffxzDPPoGvXrjXahxEjRsi2feTIEdn8qKgoDBgwAJ6enrC0tET37t0BAMnJyTVa/51qss/anrL7ceXKFVRUVMjaxdDQEB07dtQ5Xo0aNYK5uTn8/PzQt29fjBgxAsCtdg0MDJQ9nL1Lly7QaDSIjY3V2ebbb7+N119/HT4+PjWKsXPnzrCwsICrqys8PDwwY8YM2Xxtm7i6uqJfv364ePFilesxMDBAp06dpL8tLS0RGBgo1a/p+RkcHCyrExwcrHOsVq5ciQ8//BBNmzaFt7e3bN6ZM2fw/vvvy9pz/PjxSE1NRXFxcY2OSUOq9UB/IyMjrFmzBvPnz8e5c+dQWFiINm3awM/Prz7iIyKqN2ZmZrKxOQ9zu0pTWFgIfX19REVFQV9fXzbPwsKi1uuLi4vDd999h+joaFy/fr1GyyxbtgwhISHS36NGjZL+r72M16tXL2zatAmOjo5ITk5Gr169UF5eXuv4gJrts6mp6X2tu7aOHDkCS0tLJCQkYMKECVi6dKlOgnQvhw8fxpEjR7B27Vr8+uuvNVpmy5YtaN68OdLS0jB16lTMnDkTK1askOZr2yQ3Nxfvvvsuhg0bhvPnz8vWob10WJX6uPsyMjISe/bswZgxY7B69Wq8/vrr0rzCwkIsWrQI//nPf3SWexR+yaPWSdn777+PmTNnwsPDAx4eHvURExHRQ6FSqXR+Hu5R1bx5c4SHh8vKwsPD4e/vr5NwVKVNmzZQq9XIyMhAt27dqq1XWVmJkydPomPHjgBujYXKzc1F8+bNZfXeeecdvPbaa/D19a1xUubi4gJfX1/p79sTokuXLiE7Oxsff/yx9Nlz+28w34+a7HPr1q1x4MABLFq0qNbrb9KkCYyMjBAeHg4vLy8At8bFnThxQnYDAwA0btwYNjY28PX1xZAhQ7Bjxw7MmDEDzZs3x7p161BUVCSdq+Hh4dDT00PTpk2l5YUQmDFjBubNm3fXJOlOHh4e8PX1ha+vL8aOHYuPP/5YlpTd3iZTp07FgAEDUFFRIVtHs2bNUFlZiePHj6Nz584AgIKCApw5cwavvPIKgJqfn//++6+szr///qtzbi1fvhx9+vTBV199hbFjx6JPnz7w9PQEALRt2xaxsbGy8+hRUuvLl4sWLWqQb5ZERFS9GTNm4MCBA1i8eDEuX76M9evXY+XKldKA9nvx9/fHqFGjMHr0aGzfvh0JCQmIjIxEWFgYfv/9d6meoaEhpkyZguPHjyMqKgpjxozBU089JSVpABAfH49Dhw5h/vz5dbZ/np6eMDIywooVK3D16lXs2rULixcvfqB11mSf58yZgxMnTmDixIk4e/YsLl26hK+//rpGTxswNzfHG2+8gVmzZmHv3r24ePEixo8fj+LiYowbN05WNyMjA2lpaTh+/Dh2796NZs2aAbjVW2hiYoLQ0FCcP38eBw8exJQpU/DKK6/A2dlZWv7AgQPIy8vDpEmTanUMsrOzkZaWhrNnz+J///uftF2tiooKlJaWIi0tDRs3boS/v7/OU/qbNm2KPn364LXXXsORI0dw7tw5jB49GhYWFhg5ciSAmp+f4eHh+PTTT3H58mWsWrUK27Ztw9SpU2V1tHceDxkyBH379sVrr70mzZs/fz5+/PFHLFq0CBcuXEBMTAw2b9786Ayvqu1ANpVKJdLT02u7mGJwoD/Rk+lug3CVrKYD/YUQ4ueffxYBAQHC0NBQeHp6is8++6zKdVY16F2IW4PQ58+fL7y9vYWhoaFwdXUVL7zwgjh79qwQ4taAbmtra/HLL78IHx8fYWxsLEJCQkRSUpK0jgULFggA4vPPP79rrHdCDQb6//TTT8Lb21sYGxuL4OBgsWvXLtlg9bu5330WQohDhw6Jzp07C2NjY2FjYyN69eqlsy9VDfQX4tZ5N2XKFOHg4CCMjY1Fly5dRGRkpDRfe2y0k4ODgxg5cqTIzs6W6pw9e1Y888wzwsTERNjZ2Ynx48fLbm4IDQ0VAMTPP/8slWnbqjragf7aycbGRvTv3182cP72+ZaWlqJ79+4iOjpaCCEf6C+EEJmZmWLkyJHC2tpamJqaipCQEHHhwgXZNu91fnp5eYlFixaJoUOHCjMzM+Hi4iK++OILWZ07z5PMzEzh5OQkVq9eLZXt3btXdO7cWZiamgorKyvRsWNH8e2331Z7LOpKXQz0VwlRu3uz9fT0MHPmzGrHGNTlN6P6UNNfaieix0tpaSkSEhLQuHHjR2JsiRKtW7cO06ZN468IUL3w9vbGtGnTdC7tPiru9h5T09yj1mPKgFvdi0ZGRjrlKpVK8UkZERERkRLdV1K2Y8cOODk51XUsRERERE+s+0rKiIjoyTNmzJi7PkGf6EEkJiY2dAgNrtZ3X3bv3r3KS5dEREREdP9q3VN28OBB6f/aewTq4+FwRERERE+SWveUAcCPP/6IVq1awdTUFKampmjdujU2bNhQ17EREdU5jUbT0CEQ0WOoLt5bat1TtnTpUsybNw+TJ0+Wfsfq6NGj+O9//4usrCy89dZbDxwUEVFdMzIygp6eHm7cuAFHR0cYGRmxl5+IHpgQAuXl5cjMzISent4DDfGq9XPKGjdujEWLFmH06NGy8vXr12PhwoVISEi472AeBj6njOjJVV5e/sj8MDERPVrMzMzg6upaZVJWb88pS01NlX7b6nadO3dGampqbVdHRPTQGBkZwdPTE5WVlVCr1Q0dDhE9JvT19WFgYPDAve+1Tsp8fX2xdetWvPvuu7LyLVu2wM/P74GCISKqbyqVCoaGhjq/30dE1NBqnZQtWrQIL730Ev755x9pTFl4eDgOHDiArVu31nmARERERE+CWt99OWTIEBw/fhwODg7YuXMndu7cCQcHB0RGRuKFF16ojxiJiIiIHnu1Huj/qONAfyIiInqY6nygf35+fo3qMdEhIiIiqr0aJ2U2NjZ3vatACAGVSsU7moiIiIjuQ60G+v/888+ws7Orr1iIiIiInli1Ssq6dOkCJyen+oqFiIiI6Il1X799SURERER1i0kZERERkQLUOClTqVT88V4iIiKielLjMWVCCIwZMwbGxsZ3rbd9+/YHDoqIiIjoSVPjpCw0NLQ+4yAiIiJ6otU4KVu7dm19xkFERET0RONAfyIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESmAIpKyVatWwdvbGyYmJujUqRMiIyOrrbtmzRp069YNtra2sLW1RUhIyF3rExERET0KGjwp27JlC6ZPn44FCxbg1KlTCAwMRK9evZCRkVFl/UOHDmHEiBE4ePAgIiIi4OHhgeeffx4pKSkPOXIiIiKiuqMSQoiGDKBTp07o0KEDVq5cCQDQaDTw8PDAlClTMHv27Hsur1arYWtri5UrV2L06NH3rJ+fnw9ra2vk5eXBysrqgeMnIiIiupua5h4N2lNWXl6OqKgohISESGV6enoICQlBREREjdZRXFyMiooK2NnZVTm/rKwM+fn5somIiIhIaRo0KcvKyoJarYazs7Os3NnZGWlpaTVaxzvvvAM3NzdZYne7sLAwWFtbS5OHh8cDx01ERERU1xp8TNmD+Pjjj7F582bs2LEDJiYmVdaZM2cO8vLypOnatWsPOUoiIiKiezNoyI07ODhAX18f6enpsvL09HS4uLjcddnPP/8cH3/8Mf766y+0bt262nrGxsYwNjauk3iJiIiI6kuD9pQZGRmhXbt2OHDggFSm0Whw4MABBAcHV7vcp59+isWLF2Pv3r1o3779wwiViIiIqF41aE8ZAEyfPh2hoaFo3749OnbsiOXLl6OoqAhjx44FAIwePRru7u4ICwsDAHzyySeYP38+fvrpJ3h7e0tjzywsLGBhYdFg+0FERET0IBo8KXvppZeQmZmJ+fPnIy0tDUFBQdi7d680+D85ORl6ev/foff111+jvLwcL774omw9CxYswMKFCx9m6ERERER1psGfU/aw8TllRERE9DA9Es8pIyIiIqJbmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBWBSRkRERKQATMqIiIiIFIBJGREREZECMCkjIiIiUgAmZUREREQKwKSMiIiISAGYlBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBWBSRkRERKQATMqIiIiIFIBJGREREZECMCkjIiIiUgAmZUREREQKwKSMiIiISAGYlBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIARo8KVu1ahW8vb1hYmKCTp06ITIystq6Fy5cwJAhQ+Dt7Q2VSoXly5c/vECJiIiI6lGDJmVbtmzB9OnTsWDBApw6dQqBgYHo1asXMjIyqqxfXFwMHx8ffPzxx3BxcXnI0RIRERHVnwZNypYuXYrx48dj7NixCAgIwDfffAMzMzP88MMPVdbv0KEDPvvsMwwfPhzGxsYPOVoiIiKi+tNgSVl5eTmioqIQEhLy/8Ho6SEkJAQRERF1tp2ysjLk5+fLJiIiIiKlabCkLCsrC2q1Gs7OzrJyZ2dnpKWl1dl2wsLCYG1tLU0eHh51tm4iIiKiutLgA/3r25w5c5CXlydN165da+iQiIiIiHQYNNSGHRwcoK+vj/T0dFl5enp6nQ7iNzY25vgzIiIiUrwG6ykzMjJCu3btcODAAalMo9HgwIEDCA4ObqiwiIiIiBpEg/WUAcD06dMRGhqK9u3bo2PHjli+fDmKioowduxYAMDo0aPh7u6OsLAwALduDrh48aL0/5SUFERHR8PCwgK+vr4Nth9ERERED6pBk7KXXnoJmZmZmD9/PtLS0hAUFIS9e/dKg/+Tk5Ohp/f/nXk3btxAmzZtpL8///xzfP755+jevTsOHTr0sMMnIiIiqjMqIYRo6CAepvz8fFhbWyMvLw9WVlYNHQ4RERE95mqaezz2d18SERERPQqYlBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBWBSRkRERKQATMqIiIiIFIBJGREREZECMCkjIiIiUgAmZUREREQKwKSMiIiISAGYlBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBWBSRkRERKQATMqIiIiIFIBJGREREZECMCkjIiIiUgCDhg7gcaRWqxEZGQkTExO0bt0a+vr6AICCggIUFxfD3NwcFhYWAAAhBHJzcwEANjY2UKlUAIDS0lKUl5fDyMgIJiYm0rqLiooAAKamptDTu5VTV1ZWorKyEvr6+jA0NJTqlpeXAwAMDQ2l9Wo0Gmg0Gujp6UnLa8sBQKVSSXWFENJ8bRkRERHVD/aU1YPCwkJ07twZbdu2RWVlpVS+aNEiuLi44P3335fKysvLYWdnBzs7O+Tn50vlYWFhsLa2xsyZM2Xrtra2hoWFBdLS0qSypUuXwtTUFBMmTJDVdXR0hLGxMa5cuSKVffXVVzA0NMTw4cNldRs1agR9fX2cPXtWKlu3bh309PTQv39/WV0/Pz+oVCocO3ZMKtu2bRv09fXx7LPPyuq2bdsWBgYG+Ouvv6SyPXv2wNDQEMHBwbK6Tz/9NIyMjLBr1y6p7PDhwzAxMUHbtm1ldfv27QsTExNs2bJFKjt58iTMzMzQvHlzWd1hw4bB3Nwca9eulcouXrwICwsLeHt7y+q++uqrsLS0xFdffSWVJSUlwcrKCq6urrK6b775JmxsbLBkyRKpLDMzE7a2trC1tZXVnTNnDuzs7PDBBx9IZUVFRbC3t4e9vT2Ki4ul8g8++AAODg547733pDIhBBwdHeHo6Ijs7GypfNmyZXBycsKMGTNk2/P29oazszOuXbsmla1evRouLi6YOHGirG6LFi3g4uKC2NhYqWzDhg1wdXXFq6++KqvbsWNHuLm5ITo6Wirbvn073N3dMWLECFndHj16wN3dHREREVLZH3/8AXd3dwwePFhWt2/fvmjUqBH+/vtvqezw4cPw8PBA7969ZXVffPFFeHh44Pfff5fKTpw4AQ8PDzz99NOyuqGhofD09MS2bduksvPnz8PLywsdO3aU1X3jjTfg5eWFH3/8USq7evUqvLy80KpVK1ndGTNmwNvbG6tXr5bK0tLS4O3tDT8/P1ndefPmoXHjxvjiiy+ksry8PDRu3BiNGzeWvUeEhYXBx8cHH3/8sVRWXl4OHx8f+Pj4yN4jli9fDh8fHyxYsEC2vaZNm6JJkyZIT0+Xyr799ls0adIEb7/9tqxuUFAQfH19kZiYKJVt3LgRvr6+mDJliqxu586d4evri5iYGKnsl19+gZ+fH8aPHy+rGxISAj8/P5w6dUoq27NnD/z8/PDyyy/L6g4cOBD+/v4IDw+Xyg4dOgR/f38MGTJEVvell16Cv7+/7P3k33//RdOmTdGvXz9Z3bFjx6Jp06bYvXu3VHb27Fk0a9YMPXv2lNWdOHEimjVrhq1bt0plcXFxaNasGTp37iyrO3PmTDRr1kx2nly/fh3NmzfXeZ+aN28emjdvLjtPsrOz0bx5c533qbCwMAQEBGD58uVSWXFxMQICAhAQEICSkhKpfNmyZQgICJCdJ0IIqe7Nmzel8tWrV6NFixY650m7du3QokUL3LhxQypbv349WrRooXOedO3aFS1atJB9lmzbtg0tW7bE1KlTZXWfe+45tGzZEufPn5fKfvvtN7Rs2VLnPBk0aBBatWqFkydPSmV//fUXWrZsqXOeDB8+HK1atcKRI0eksmPHjqFVq1YYOnSorO6YMWPQqlUr7Nu3Tyo7ffo0WrdujQEDBsjqTpw4Ea1bt0ZqaiqUgD1l9UCtVqNx48YoKyuDkZGRzvxHrdfpznhv70G7vUyj0ejMU6vVUKvVOnUrKyt1yisrK1FRUSFbh0ajQVlZmdTrp1VeXo6ysjLZOjQaDUpKSlBaWiqrW1ZWhuLiYp26RUVFMDMzk9UtLS1FYWEhKioqZHULCgqk3kSt4uJi5OXloaysTLZv2p7PO+vm5OTIYhNCyN48b6+bnZ0t9YpqZWVlSctpFRUVITMzEwUFBbK6GRkZKCkpkcVcXFyM9PR02Qe7tm5WVpbs+JSUlCAtLQ05OTmyuunp6UhNTZUlEiUlJbhx44YsWdSu98aNG7K2Ky0txY0bN5CZmalTNyUlRXZ8ysrKcP36dTg4OMjqZmZm4vr167IPqfLycly/fh2mpqY6da9duyZLeisqKpCcnCzbB+DW8U1OTkZhYaFUVllZieTkZNjY2OjUTUpKkh1LtVqNpKQkWW81cOtDODExUXZeaDQaWSKkdfPmTSQkJOicFwkJCQDkbZ+Tk4OEhATpvNC6evWqzusrNzcXV69e1TnuCQkJyM/Pl53veXl5uHLliiypA4DExESkpqbK2jM/Px/x8fHw9/eX1U1KSkJ8fLysPQsLCxEfH49GjRrp1I2Li5O1Z1FREeLi4nSO+7Vr1xAXFyd7bZSUlODy5cswMJB/nF2/fh2XL1+WvTZKS0sRGxsre80CQEpKCmJjY5GXlyeVlZWVITY2Fo6OjrK6N27cQGxsrOy1UVFRgUuXLsHc3FxWNy0tDZcuXZK9NtRqNS5duoQ7paWlISYmRtZGQggpCb697TMzMxETE6PTRtq6t7d9ZmYmLl68qJN0xMTEoKSkRNb2N2/exMWLFxEUFCSrGxsbi6ysLNlxy8nJwYULF3S+hFy+fBnJycmy9szLy8OFCxfg5uYmqxsXF4eYmBhZexYUFODChQuwtraW1b1y5QrOnz8va8/CwkKcP39ep+0TEhJw/vx52euzqKgI586d0/l8SExMxLlz52THoSGpRFWfsI+x/Px8WFtbIy8vD1ZWVg0SgxBCdolQ+wLS19eXyrVvqnp6erI3ee2Hi4mJiXT5UZugGBgYyD6U8vLyIISApaWldAm1tLQUxcXFMDIyki6hArdejEIIWFtbSyd4aWkpCgoKYGRkJHuBZGdnQ61Ww8bGRko6S0pKkJeXByMjI9jZ2Ul1MzMzUVlZCTs7OxgbG0vrvXnzJgwNDWVveBkZGSgvL4e9vb20H6WlpcjMzISBgYGspyo9PR1lZWWwt7eX3gjLysqQlpYGfX192Rt/eno6SkpKYG9vD0tLS+mYpaSkQE9PD15eXrK62h4s7T5XVFQgKSkJKpUKTZo0kdXNz8+Hvb29tM+VlZW4evUqAMg+qNLT05Gbmws7OztpnzUaDS5fvgwhBJo2bSq1pzZJsrOzg4uLi3SeaN9w/f39pTbKzMxERkYGbGxs4O7uLm3vwoULEELA399faqOsrCykpqbCxsYGHh4esrpqtRr+/v7SpfLs7GykpKTAyspK1pt48eJFVFRUwM/PT0pob968ieTkZFhaWsqOT0xMDMrKyuDr6yuda7m5uUhISIC5ubns+Fy6dAklJSXw8fGRjrv2A9/U1FTWq3D58mUUFhaicePGUo9kYWEhYmNjYWxsjJYtW0p14+LikJ+fDy8vLym5Ky4uxsWLF2FoaIjAwECp7pUrV5CTkwNPT084OTkBuHX+nTt3Dvr6+rJekKtXryI7OxuNGjWSzsvy8nJER0dDpVKhQ4cOUt3ExERkZGTA3d1daqPKykqpF6lDhw7S6z45ORmpqalwdXWFp6endJ6cOHECwK3eDW3bX7t2DSkpKXB2dkbjxo2l7R0/fhxCCLRt21Zq+xs3biA5ORkODg7w9fWV6kZGRkKtViMoKEh6zaWlpSEhIQH29vayNjpx4gQqKioQGBgoveYyMjIQHx8PGxsbBAQESHVPnTqF0tJStGzZUnqfzcrKQmxsLKysrGQ9j6dPn0ZxcTFatGghJWHa5MDCwkKWIJw5cwYFBQVo3rw57O3tAdw6p86fPw8zMzNZG50/fx65ubnw9/eX2rOgoADR0dEwNjaW9ZReuHABN2/ehK+vr9SeRUVFOHXqFAwMDGS9+trEqUmTJlJ7lpaWIjIyEnp6eujatatUNzY2VupB1b7PlJeXS73H3bt3l+rGx8fj+vXr8PT0hI+PD4BbyZW2Z6hbt27S+/jVq1eRnJyMRo0aSe0phMDhw4cB3OrV1LZ9UlISEhIS4OrqiqZNm0rbO3z4MDQaDYKDg6XX/bVr1xAfHw9nZ2dZex49ehQVFRXo2LGj1Pba5NTBwUHWnseOHUNZWRnatWsntX1aWhouXrwIOzs7WXv++++/KC4uRps2baTXcmZmJs6dOwdra2u0a9dOqnvixAkUFBSgdevW0ms5OzsbZ86cgYWFhaw9o6KikJubi1atWkltn5ubi6ioKJiamsp6P0+fPo3s7Gx07dpVNlSortU092BSRkRERFSPapp7cEwZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIiIiIiBVBEUrZq1Sp4e3vDxMQEnTp1QmRk5F3rb9u2Dc2aNYOJiQlatWqFPXv2PKRIiYiIiOpHgydlW7ZswfTp07FgwQKcOnUKgYGB6NWrFzIyMqqsf+zYMYwYMQLjxo3D6dOnMXjwYAwePFj2kw5EREREj5oGf3hsp06d0KFDB6xcuRLAradXe3h4YMqUKZg9e7ZO/ZdeeglFRUX47bffpLKnnnoKQUFB+Oabb3Tql5WVyX4aIj8/Hx4eHnx4LBERET0Uj8TDY8vLyxEVFYWQkBCpTE9PDyEhIbIfMb5dRESErD4A9OrVq9r62h/21k63/7wMERERkVI0aFKm/RFkZ2dnWbmzszPS0tKqXCYtLa1W9efMmYO8vDxpunbtWt0ET0RERFSHDO5d5dFmbGws/RA2ERERkVI1aE+Zg4MD9PX1kZ6eLitPT0+Hi4tLlcu4uLjUqj4RERHRo6BBkzIjIyO0a9cOBw4ckMo0Gg0OHDiA4ODgKpcJDg6W1QeA/fv3V1ufiIiI6FHQ4Jcvp0+fjtDQULRv3x4dO3bE8uXLUVRUhLFjxwIARo8eDXd3d4SFhQEApk6diu7du2PJkiXo168fNm/ejJMnT+Lbb7+t0fa0N5vm5+fXzw4RERER3Uabc9zzgRdCAVasWCE8PT2FkZGR6Nixo/j333+led27dxehoaGy+lu3bhX+/v7CyMhItGjRQvz+++813ta1a9cEAE6cOHHixIkTp4c6Xbt27a45SoM/p+xh02g0uHHjBiwtLaFSqeplG9pnoV27do3PQlMAtodysC2Uhe2hHGwLZanr9hBCoKCgAG5ubtDTq37kWINfvnzY9PT00KhRo4eyLSsrK764FITtoRxsC2VheygH20JZ6rI9rK2t71mnwX9miYiIiIiYlBEREREpApOyemBsbIwFCxbwobUKwfZQDraFsrA9lINtoSwN1R5P3EB/IiIiIiViTxkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJTVsVWrVsHb2xsmJibo1KkTIiMjGzqkJ0JYWBg6dOgAS0tLODk5YfDgwYiNjZXVKS0txaRJk2Bvbw8LCwsMGTIE6enpDRTxk+Pjjz+GSqXCtGnTpDK2xcOVkpKCl19+Gfb29jA1NUWrVq1w8uRJab4QAvPnz4erqytMTU0REhKCuLi4Boz48aVWqzFv3jw0btwYpqamaNKkCRYvXiz7TUS2R/34559/MGDAALi5uUGlUmHnzp2y+TU57jdv3sSoUaNgZWUFGxsbjBs3DoWFhXUWI5OyOrRlyxZMnz4dCxYswKlTpxAYGIhevXohIyOjoUN77B0+fBiTJk3Cv//+i/3796OiogLPP/88ioqKpDpvvfUWdu/ejW3btuHw4cO4ceMG/vOf/zRg1I+/EydOYPXq1WjdurWsnG3x8OTk5KBLly4wNDTEH3/8gYsXL2LJkiWwtbWV6nz66af48ssv8c033+D48eMwNzdHr169UFpa2oCRP54++eQTfP3111i5ciViYmLwySef4NNPP8WKFSukOmyP+lFUVITAwECsWrWqyvk1Oe6jRo3ChQsXsH//fvz222/4559/MGHChLoLssa/5E331LFjRzFp0iTpb7VaLdzc3ERYWFgDRvVkysjIEADE4cOHhRBC5ObmCkNDQ7Ft2zapTkxMjAAgIiIiGirMx1pBQYHw8/MT+/fvF927dxdTp04VQrAtHrZ33nlHdO3atdr5Go1GuLi4iM8++0wqy83NFcbGxuJ///vfwwjxidKvXz/x6quvysr+85//iFGjRgkh2B4PCwCxY8cO6e+aHPeLFy8KAOLEiRNSnT/++EOoVCqRkpJSJ3Gxp6yOlJeXIyoqCiEhIVKZnp4eQkJCEBER0YCRPZny8vIAAHZ2dgCAqKgoVFRUyNqnWbNm8PT0ZPvUk0mTJqFfv36yYw6wLR62Xbt2oX379hg6dCicnJzQpk0brFmzRpqfkJCAtLQ0WXtYW1ujU6dObI960LlzZxw4cACXL18GAJw5cwZHjx5Fnz59ALA9GkpNjntERARsbGzQvn17qU5ISAj09PRw/PjxOonjiftB8vqSlZUFtVoNZ2dnWbmzszMuXbrUQFE9mTQaDaZNm4YuXbqgZcuWAIC0tDQYGRnBxsZGVtfZ2RlpaWkNEOXjbfPmzTh16hROnDihM49t8XBdvXoVX3/9NaZPn453330XJ06cwJtvvgkjIyOEhoZKx7yq9y62R92bPXs28vPz0axZM+jr60OtVuPDDz/EqFGjAIDt0UBqctzT0tLg5OQkm29gYAA7O7s6axsmZfTYmTRpEs6fP4+jR482dChPpGvXrmHq1KnYv38/TExMGjqcJ55Go0H79u3x0UcfAQDatGmD8+fP45tvvkFoaGgDR/fk2bp1KzZt2oSffvoJLVq0QHR0NKZNmwY3Nze2B3Ggf11xcHCAvr6+zh1k6enpcHFxaaConjyTJ0/Gb7/9hoMHD6JRo0ZSuYuLC8rLy5Gbmyurz/ape1FRUcjIyEDbtm1hYGAAAwMDHD58GF9++SUMDAzg7OzMtniIXF1dERAQICtr3rw5kpOTAUA65nzvejhmzZqF2bNnY/jw4WjVqhVeeeUVvPXWWwgLCwPA9mgoNTnuLi4uOjfuVVZW4ubNm3XWNkzK6oiRkRHatWuHAwcOSGUajQYHDhxAcHBwA0b2ZBBCYPLkydixYwf+/vtvNG7cWDa/Xbt2MDQ0lLVPbGwskpOT2T51rGfPnjh37hyio6OlqX379hg1apT0f7bFw9OlSxedx8NcvnwZXl5eAIDGjRvDxcVF1h75+fk4fvw426MeFBcXQ09P/tGrr68PjUYDgO3RUGpy3IODg5Gbm4uoqCipzt9//w2NRoNOnTrVTSB1crsACSGE2Lx5szA2Nhbr1q0TFy9eFBMmTBA2NjYiLS2toUN77L3xxhvC2tpaHDp0SKSmpkpTcXGxVOe///2v8PT0FH///bc4efKkCA4OFsHBwQ0Y9ZPj9rsvhWBbPEyRkZHCwMBAfPjhhyIuLk5s2rRJmJmZiY0bN0p1Pv74Y2FjYyN+/fVXcfbsWTFo0CDRuHFjUVJS0oCRP55CQ0OFu7u7+O2330RCQoLYvn27cHBwEG+//bZUh+1RPwoKCsTp06fF6dOnBQCxdOlScfr0aZGUlCSEqNlx7927t2jTpo04fvy4OHr0qPDz8xMjRoyosxiZlNWxFStWCE9PT2FkZCQ6duwo/v3334YO6YkAoMpp7dq1Up2SkhIxceJEYWtrK8zMzMQLL7wgUlNTGy7oJ8idSRnb4uHavXu3aNmypTA2NhbNmjUT3377rWy+RqMR8+bNE87OzsLY2Fj07NlTxMbGNlC0j7f8/HwxdepU4enpKUxMTISPj4+YO3euKCsrk+qwPerHwYMHq/ycCA0NFULU7LhnZ2eLESNGCAsLC2FlZSXGjh0rCgoK6ixGlRC3PUaYiIiIiBoEx5QRERERKQCTMiIiIiIFYFJGREREpABMyoiIiIgUgEkZERERkQIwKSMiIiJSACZlRERERArApIyIiIhIAZiUERERESkAkzIieixVVFRg3bp16Nq1KxwdHWFqaorWrVvjk08+QXl5eUOHR0Skgz+zRESPpejoaMyYMQMTJ05EmzZtUFpainPnzmHhwoVwdXXFn3/+CUNDw4YOk4hIwp4yInostWzZEgcOHMCQIUPg4+ODgIAAvPTSS/jnn39w/vx5LF++HACgUqmqnKZNmyatKycnB6NHj4atrS3MzMzQp08fxMXFSfNfffVVtG7dGmVlZQCA8vJytGnTBqNHj5bqvPPOO/D394eZmRl8fHwwb948VFRUPJRjQUSPBiZlRPRYMjAwqLLc0dER//nPf7Bp0yapbO3atUhNTZWm4OBg2TJjxozByZMnsWvXLkREREAIgb59+0pJ1ZdffomioiLMnj0bADB37lzk5uZi5cqV0josLS2xbt06XLx4EV988QXWrFmDZcuW1fVuE9EjrOp3LSKix0SLFi2QlJQkK6uoqIC+vr70t42NDVxcXKS/jYyMpP/HxcVh165dCA8PR+fOnQEAmzZtgoeHB3bu3ImhQ4fCwsICGzduRPfu3WFpaYnly5fj4MGDsLKyktbz3nvvSf/39vbGzJkzsXnzZrz99tt1vs9E9GhiUkZEj7U9e/boXCb89NNPsXHjxhotHxMTAwMDA3Tq1Ekqs7e3R9OmTRETEyOVBQcHY+bMmVi8eDHeeecddO3aVbaeLVu24Msvv8SVK1dQWFiIyspKWdJGRMSkjIgea15eXjplV65cgb+/f51uR6PRIDw8HPr6+oiPj5fNi4iIwKhRo7Bo0SL06tUL1tbW2Lx5M5YsWVKnMRDRo41jyojosXTz5k0UFBTolJ88eRIHDx7EyJEja7Se5s2bo7KyEsePH5fKsrOzERsbi4CAAKnss88+w6VLl3D48GHs3bsXa9euleYdO3YMXl5emDt3Ltq3bw8/Pz+dS6pEREzKiOixlJycjKCgIHz//feIj4/H1atXsWHDBgwaNAjdunWT3V15N35+fhg0aBDGjx+Po0eP4syZM3j55Zfh7u6OQYMGAQBOnz6N+fPn47vvvkOXLl2wdOlSTJ06FVevXpXWkZycjM2bN+PKlSv48ssvsWPHjvradSJ6RDEpI6LHUsuWLbFgwQKsW7cOTz31FFq0aIFPP/0UkydPxr59+2SD+e9l7dq1aNeuHfr374/g4GAIIbBnzx4YGhqitLQUL7/8MsaMGYMBAwYAACZMmIBnnnkGr7zyCtRqNQYOHIi33noLkydPRlBQEI4dO4Z58+bV164T0SOKD48lIiIiUgD2lBEREREpAJMyIiIiIgVgUkZERESkAEzKiIiIiBSASRkRERGRAjApIyIiIlIAJmVERERECsCkjIiIiEgBmJQRERERKQCTMiIiIiIFYFJGREREpAD/B5sQ7Coe9b2lAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc"
      ],
      "metadata": {
        "id": "D5PyG40kWE5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aacfc64-88a7-43b2-d373-61d330f827d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([76.8357428 , 76.89473279, 76.97851568, 77.04690987, 77.01014799,\n",
              "       77.05289436, 77.04776479, 77.08880131, 77.10162522, 76.97680582,\n",
              "       77.06742812, 77.12128855, 77.13753217, 77.1948123 , 77.11957869,\n",
              "       77.16574477, 77.18455317, 76.98022553, 77.13838709, 77.02468175,\n",
              "       77.12898289, 77.07854218, 77.23242911, 77.11701391, 77.25294736,\n",
              "       77.19224752, 77.20079679, 77.27261069, 77.24183331, 77.11957869,\n",
              "       77.20250665, 77.26833605, 77.24952765, 77.31364721, 77.29996837,\n",
              "       77.30766271, 77.31621199, 77.27090084, 77.26491635, 77.29227402,\n",
              "       77.16232506, 77.19823201, 77.31877677, 77.18882781, 77.257222  ,\n",
              "       77.29227402, 77.19823201, 77.29654866, 77.28201489, 77.31193735,\n",
              "       77.25551214, 77.32305141, 77.24012345, 77.3102275 , 77.25807693,\n",
              "       77.24097838, 77.28457967, 77.25636707, 77.23413896, 77.25893185,\n",
              "       77.27517547, 77.24183331, 77.26919098, 77.29398388, 77.36836256,\n",
              "       77.27688533, 77.25294736, 77.27004591, 77.27090084, 77.23584881,\n",
              "       77.31279228, 77.31706692, 77.26833605, 77.29227402, 77.26235156,\n",
              "       77.26919098, 77.24012345, 77.25978678, 77.26748113, 77.21789534,\n",
              "       77.24952765, 77.2760304 , 77.31706692, 77.27346562, 77.20250665,\n",
              "       77.31193735, 77.35040908, 77.27859518, 77.21704041, 77.29654866,\n",
              "       77.31279228, 77.24097838, 77.27774026, 77.22729954, 77.2478178 ,\n",
              "       77.29056417, 77.31792184, 77.30851764, 77.33502039, 77.2760304 ])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}