{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaapetrova/Tropicall_NN/blob/main/Heart_DiseaseLMLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNMoTiYpv_zn",
        "outputId": "bfd684dd-42ae-4779-93d3-4b42eb1df8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gccwn9ewwmKG"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/heart_2020_cleaned.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2HHc0WbxsAq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# For ML models\n",
        "from sklearn.linear_model import LinearRegression ,LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC ,SVR\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wAXIDM7CgOk",
        "outputId": "8b4f39b5-362a-44d5-8285-483fded22d13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DDFNGw4x6rq"
      },
      "source": [
        "# Загрузка Датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgNl5fBjyM0E",
        "outputId": "3162867a-91fd-46a8-fd7a-d87e85254ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/heart_2020_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/heart_2020_cleaned.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2F92TLsjCz_l"
      },
      "source": [
        "## Column Descriptions\n",
        "\n",
        "- HeartDisease: Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI).\n",
        "- BMI: Body Mass Index (BMI).\n",
        "- Smoking: Have you smoked at least 100 cigarettes in your entire life?\n",
        "- AlcoholDrinking: Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week\n",
        "- Stroke(инсульт): (Ever told) (you had) a stroke?\n",
        "- PhysicalHealth: Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? (0-30 days).\n",
        "- MentalHealth: Thinking about your mental health, for how many days during the past 30 days was your mental health not good? (0-30 days).\n",
        "DiffWalking: Do you have serious difficulty walking or climbing stairs?\n",
        "- Sex: Are you male or female?\n",
        "- AgeCategory: Fourteen-level age category. (then calculated the mean)\n",
        "- Race: Imputed race/ethnicity value.\n",
        "- Diabetic: (Ever told) (you had) diabetes?\n",
        "- PhysicalActivity: Adults who reported doing physical activity or exercise during the past 30 days other than their regular job.\n",
        "- GenHealth: Would you say that in general your health is...\n",
        "- SleepTime: On average, how many hours of sleep do you get in a 24-hour period?\n",
        "- Asthma: (Ever told) (you had) asthma?\n",
        "- KidneyDisease: Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease?\n",
        "SkinCancer: (Ever told) (you had) skin cancer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdiShpiVBOfb"
      },
      "source": [
        "## Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5bh8DwN0x-bF",
        "outputId": "c859cb44-3cb7-4d16-f7e6-2fbd53af46cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  HeartDisease    BMI Smoking AlcoholDrinking Stroke  PhysicalHealth  \\\n",
              "0           No  16.60     Yes              No     No             3.0   \n",
              "1           No  20.34      No              No    Yes             0.0   \n",
              "2           No  26.58     Yes              No     No            20.0   \n",
              "3           No  24.21      No              No     No             0.0   \n",
              "4           No  23.71      No              No     No            28.0   \n",
              "\n",
              "   MentalHealth DiffWalking     Sex  AgeCategory   Race Diabetic  \\\n",
              "0          30.0          No  Female        55-59  White      Yes   \n",
              "1           0.0          No  Female  80 or older  White       No   \n",
              "2          30.0          No    Male        65-69  White      Yes   \n",
              "3           0.0          No  Female        75-79  White       No   \n",
              "4           0.0         Yes  Female        40-44  White       No   \n",
              "\n",
              "  PhysicalActivity  GenHealth  SleepTime Asthma KidneyDisease SkinCancer  \n",
              "0              Yes  Very good        5.0    Yes            No        Yes  \n",
              "1              Yes  Very good        7.0     No            No         No  \n",
              "2              Yes       Fair        8.0    Yes            No         No  \n",
              "3               No       Good        6.0     No            No        Yes  \n",
              "4              Yes  Very good        8.0     No            No         No  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa6777fb-578c-4581-b747-0a81f27d49f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeartDisease</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoking</th>\n",
              "      <th>AlcoholDrinking</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>PhysicalHealth</th>\n",
              "      <th>MentalHealth</th>\n",
              "      <th>DiffWalking</th>\n",
              "      <th>Sex</th>\n",
              "      <th>AgeCategory</th>\n",
              "      <th>Race</th>\n",
              "      <th>Diabetic</th>\n",
              "      <th>PhysicalActivity</th>\n",
              "      <th>GenHealth</th>\n",
              "      <th>SleepTime</th>\n",
              "      <th>Asthma</th>\n",
              "      <th>KidneyDisease</th>\n",
              "      <th>SkinCancer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No</td>\n",
              "      <td>16.60</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>55-59</td>\n",
              "      <td>White</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No</td>\n",
              "      <td>20.34</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>80 or older</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>7.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>No</td>\n",
              "      <td>26.58</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>20.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Male</td>\n",
              "      <td>65-69</td>\n",
              "      <td>White</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Fair</td>\n",
              "      <td>8.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>No</td>\n",
              "      <td>24.21</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>Female</td>\n",
              "      <td>75-79</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Good</td>\n",
              "      <td>6.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>No</td>\n",
              "      <td>23.71</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Female</td>\n",
              "      <td>40-44</td>\n",
              "      <td>White</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Very good</td>\n",
              "      <td>8.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa6777fb-578c-4581-b747-0a81f27d49f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa6777fb-578c-4581-b747-0a81f27d49f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa6777fb-578c-4581-b747-0a81f27d49f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df = pd.read_csv(DATASET_PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4lZqRMyBQR7"
      },
      "source": [
        "## INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jbHvbM3BQBg",
        "outputId": "cd7639c4-6cc9-42ab-cbf5-2897e1aa41df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 319795 entries, 0 to 319794\n",
            "Data columns (total 18 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   HeartDisease      319795 non-null  object \n",
            " 1   BMI               319795 non-null  float64\n",
            " 2   Smoking           319795 non-null  object \n",
            " 3   AlcoholDrinking   319795 non-null  object \n",
            " 4   Stroke            319795 non-null  object \n",
            " 5   PhysicalHealth    319795 non-null  float64\n",
            " 6   MentalHealth      319795 non-null  float64\n",
            " 7   DiffWalking       319795 non-null  object \n",
            " 8   Sex               319795 non-null  object \n",
            " 9   AgeCategory       319795 non-null  object \n",
            " 10  Race              319795 non-null  object \n",
            " 11  Diabetic          319795 non-null  object \n",
            " 12  PhysicalActivity  319795 non-null  object \n",
            " 13  GenHealth         319795 non-null  object \n",
            " 14  SleepTime         319795 non-null  float64\n",
            " 15  Asthma            319795 non-null  object \n",
            " 16  KidneyDisease     319795 non-null  object \n",
            " 17  SkinCancer        319795 non-null  object \n",
            "dtypes: float64(4), object(14)\n",
            "memory usage: 43.9+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h0il8xT-CWw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Zkvc1B-HMq"
      },
      "outputs": [],
      "source": [
        "encode_AgeCategory = {'55-59':57, '80 or older':80, '65-69':67,\n",
        "                      '75-79':77,'40-44':42,'70-74':72,'60-64':62,\n",
        "                      '50-54':52,'45-49':47,'18-24':21,'35-39':37,\n",
        "                      '30-34':32,'25-29':27}\n",
        "df['AgeCategory'] = df['AgeCategory'].apply(lambda x: encode_AgeCategory[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAIHNeMLFKbc"
      },
      "source": [
        "Определение **категориальных** данных и **непрерывных**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOfoFs_NFRtn"
      },
      "outputs": [],
      "source": [
        "CategoricalFeatures = [\"HeartDisease\", \"Smoking\", \"AlcoholDrinking\", \"Stroke\", \"DiffWalking\", \"Sex\", \"Race\", \"Diabetic\", \"PhysicalActivity\", \"GenHealth\", \"Asthma\", \"KidneyDisease\", \"SkinCancer\"]\n",
        "ContinuesFeatues = [\"BMI\", \"PhysicalHealth\", \"MentalHealth\", \"AgeCategory\", \"SleepTime\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iUKOtcqkAygF",
        "outputId": "486b9f2a-2145-4a18-8057-49ac26ba35f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb86630ee00>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_bd3a5_row0_col0 {\n",
              "  background-color: #6fb0d7;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_bd3a5_row0_col1 {\n",
              "  background-color: #b5d4e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row0_col2, #T_bd3a5_row0_col3 {\n",
              "  background-color: #539ecd;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_bd3a5_row0_col4 {\n",
              "  background-color: #74b3d8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row0_col5 {\n",
              "  background-color: #7fb9da;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row0_col6, #T_bd3a5_row3_col0, #T_bd3a5_row3_col1, #T_bd3a5_row3_col2, #T_bd3a5_row3_col3, #T_bd3a5_row3_col4, #T_bd3a5_row3_col5 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_bd3a5_row1_col0, #T_bd3a5_row1_col2, #T_bd3a5_row1_col3, #T_bd3a5_row1_col4, #T_bd3a5_row1_col5, #T_bd3a5_row2_col2, #T_bd3a5_row2_col3, #T_bd3a5_row2_col4, #T_bd3a5_row4_col1, #T_bd3a5_row4_col6 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row1_col1, #T_bd3a5_row2_col1 {\n",
              "  background-color: #94c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row1_col6, #T_bd3a5_row2_col6 {\n",
              "  background-color: #e7f0fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row2_col0 {\n",
              "  background-color: #f5fafe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row2_col5 {\n",
              "  background-color: #f5f9fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row3_col6 {\n",
              "  background-color: #1966ad;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_bd3a5_row4_col0 {\n",
              "  background-color: #e9f2fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row4_col2 {\n",
              "  background-color: #eef5fc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row4_col3 {\n",
              "  background-color: #dbe9f6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row4_col4 {\n",
              "  background-color: #dfebf7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_bd3a5_row4_col5 {\n",
              "  background-color: #e5eff9;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_bd3a5\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_bd3a5_level0_col0\" class=\"col_heading level0 col0\" >mean</th>\n",
              "      <th id=\"T_bd3a5_level0_col1\" class=\"col_heading level0 col1\" >std</th>\n",
              "      <th id=\"T_bd3a5_level0_col2\" class=\"col_heading level0 col2\" >min</th>\n",
              "      <th id=\"T_bd3a5_level0_col3\" class=\"col_heading level0 col3\" >25%</th>\n",
              "      <th id=\"T_bd3a5_level0_col4\" class=\"col_heading level0 col4\" >50%</th>\n",
              "      <th id=\"T_bd3a5_level0_col5\" class=\"col_heading level0 col5\" >75%</th>\n",
              "      <th id=\"T_bd3a5_level0_col6\" class=\"col_heading level0 col6\" >max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_bd3a5_level0_row0\" class=\"row_heading level0 row0\" >BMI</th>\n",
              "      <td id=\"T_bd3a5_row0_col0\" class=\"data row0 col0\" >28.325399</td>\n",
              "      <td id=\"T_bd3a5_row0_col1\" class=\"data row0 col1\" >6.356100</td>\n",
              "      <td id=\"T_bd3a5_row0_col2\" class=\"data row0 col2\" >12.020000</td>\n",
              "      <td id=\"T_bd3a5_row0_col3\" class=\"data row0 col3\" >24.030000</td>\n",
              "      <td id=\"T_bd3a5_row0_col4\" class=\"data row0 col4\" >27.340000</td>\n",
              "      <td id=\"T_bd3a5_row0_col5\" class=\"data row0 col5\" >31.420000</td>\n",
              "      <td id=\"T_bd3a5_row0_col6\" class=\"data row0 col6\" >94.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bd3a5_level0_row1\" class=\"row_heading level0 row1\" >PhysicalHealth</th>\n",
              "      <td id=\"T_bd3a5_row1_col0\" class=\"data row1 col0\" >3.371710</td>\n",
              "      <td id=\"T_bd3a5_row1_col1\" class=\"data row1 col1\" >7.950850</td>\n",
              "      <td id=\"T_bd3a5_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "      <td id=\"T_bd3a5_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
              "      <td id=\"T_bd3a5_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
              "      <td id=\"T_bd3a5_row1_col5\" class=\"data row1 col5\" >2.000000</td>\n",
              "      <td id=\"T_bd3a5_row1_col6\" class=\"data row1 col6\" >30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bd3a5_level0_row2\" class=\"row_heading level0 row2\" >MentalHealth</th>\n",
              "      <td id=\"T_bd3a5_row2_col0\" class=\"data row2 col0\" >3.898366</td>\n",
              "      <td id=\"T_bd3a5_row2_col1\" class=\"data row2 col1\" >7.955235</td>\n",
              "      <td id=\"T_bd3a5_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
              "      <td id=\"T_bd3a5_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
              "      <td id=\"T_bd3a5_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
              "      <td id=\"T_bd3a5_row2_col5\" class=\"data row2 col5\" >3.000000</td>\n",
              "      <td id=\"T_bd3a5_row2_col6\" class=\"data row2 col6\" >30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bd3a5_level0_row3\" class=\"row_heading level0 row3\" >AgeCategory</th>\n",
              "      <td id=\"T_bd3a5_row3_col0\" class=\"data row3 col0\" >54.355759</td>\n",
              "      <td id=\"T_bd3a5_row3_col1\" class=\"data row3 col1\" >17.720429</td>\n",
              "      <td id=\"T_bd3a5_row3_col2\" class=\"data row3 col2\" >21.000000</td>\n",
              "      <td id=\"T_bd3a5_row3_col3\" class=\"data row3 col3\" >42.000000</td>\n",
              "      <td id=\"T_bd3a5_row3_col4\" class=\"data row3 col4\" >57.000000</td>\n",
              "      <td id=\"T_bd3a5_row3_col5\" class=\"data row3 col5\" >67.000000</td>\n",
              "      <td id=\"T_bd3a5_row3_col6\" class=\"data row3 col6\" >80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_bd3a5_level0_row4\" class=\"row_heading level0 row4\" >SleepTime</th>\n",
              "      <td id=\"T_bd3a5_row4_col0\" class=\"data row4 col0\" >7.097075</td>\n",
              "      <td id=\"T_bd3a5_row4_col1\" class=\"data row4 col1\" >1.436007</td>\n",
              "      <td id=\"T_bd3a5_row4_col2\" class=\"data row4 col2\" >1.000000</td>\n",
              "      <td id=\"T_bd3a5_row4_col3\" class=\"data row4 col3\" >6.000000</td>\n",
              "      <td id=\"T_bd3a5_row4_col4\" class=\"data row4 col4\" >7.000000</td>\n",
              "      <td id=\"T_bd3a5_row4_col5\" class=\"data row4 col5\" >8.000000</td>\n",
              "      <td id=\"T_bd3a5_row4_col6\" class=\"data row4 col6\" >24.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.describe()[1:][ContinuesFeatues].T.style.background_gradient(cmap='Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gdr_kbvIBiAc",
        "outputId": "da71da2d-4b5d-442b-d91e-0d376ca443e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"c96b383e-81cf-4191-825c-5722f8d2b133\" class=\"plotly-graph-div\" style=\"height:3200px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c96b383e-81cf-4191-825c-5722f8d2b133\")) {                    Plotly.newPlot(                        \"c96b383e-81cf-4191-825c-5722f8d2b133\",                        [{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[292422,27373],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.9183673469387754,0.9999999999999999]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[187887,131908],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.9183673469387754,0.9999999999999999]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[298018,21777],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.7653061224489794,0.8469387755102039]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[307726,12069],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.7653061224489794,0.8469387755102039]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[275385,44410],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.6122448979591837,0.6938775510204082]}},{\"hole\":0.35,\"labels\":[\"Female\",\"Male\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[167805,151990],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.6122448979591837,0.6938775510204082]}},{\"hole\":0.35,\"labels\":[\"White\",\"Hispanic\",\"Black\",\"Other\",\"Asian\",\"American Indian/Alaskan Native\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[245212,27446,22939,10928,8068,5202],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.4591836734693877,0.5408163265306122]}},{\"hole\":0.35,\"labels\":[\"Yes\",\"No\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[247957,71838],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.4591836734693877,0.5408163265306122]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\",\"No, borderline diabetes\",\"Yes (during pregnancy)\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[269653,40802,6781,2559],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.30612244897959184,0.3877551020408163]}},{\"hole\":0.35,\"labels\":[\"Very good\",\"Good\",\"Excellent\",\"Fair\",\"Poor\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"textinfo\":\"label+percent\",\"values\":[113858,93129,66842,34677,11289],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.30612244897959184,0.3877551020408163]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[276923,42872],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.15306122448979592,0.2346938775510204]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[308016,11779],\"type\":\"pie\",\"domain\":{\"x\":[0.55,1.0],\"y\":[0.15306122448979592,0.2346938775510204]}},{\"hole\":0.35,\"labels\":[\"No\",\"Yes\"],\"marker\":{\"colors\":[\"#4285f4\",\"#ea4335\",\"#fbbc05\",\"#34a853\"]},\"rotation\":-45,\"textinfo\":\"label+percent\",\"values\":[289976,29819],\"type\":\"pie\",\"domain\":{\"x\":[0.0,0.45],\"y\":[0.0,0.08163265306122448]}}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"HeartDisease\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Smoking\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"AlcoholDrinking\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.8469387755102039,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Stroke\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.8469387755102039,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"DiffWalking\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6938775510204082,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Sex\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6938775510204082,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Race\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.5408163265306122,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Diabetic\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.5408163265306122,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"PhysicalActivity\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.3877551020408163,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"GenHealth\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.3877551020408163,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Asthma\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2346938775510204,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"KidneyDisease\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2346938775510204,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"SkinCancer\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.08163265306122448,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"font\":{\"size\":14},\"height\":3200,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c96b383e-81cf-4191-825c-5722f8d2b133');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = make_subplots(\n",
        "    rows=7, cols=2, subplot_titles=(\"HeartDisease\", \"Smoking\",\n",
        "                                    \"AlcoholDrinking\",\"Stroke\",\n",
        "                                    \"DiffWalking\", \"Sex\",\n",
        "                                    'Race', 'Diabetic',\n",
        "                                    'PhysicalActivity','GenHealth',\n",
        "                                    'Asthma', 'KidneyDisease',\n",
        "                                    'SkinCancer'),\n",
        "    specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}],\n",
        "           [{\"type\": \"domain\"}, {\"type\": \"domain\"}]],\n",
        ")\n",
        "\n",
        "colours = ['#4285f4', '#ea4335', '#fbbc05', '#34a853']\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['HeartDisease'].value_counts().index),\n",
        "                     values=[x for x in df['HeartDisease'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Smoking'].value_counts().index),\n",
        "                     values=[x for x in df['Smoking'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['AlcoholDrinking'].value_counts().index),\n",
        "                     values=[x for x in df['AlcoholDrinking'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=2, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Stroke'].value_counts().index),\n",
        "                     values=[x for x in df['Stroke'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=2, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['DiffWalking'].value_counts().index),\n",
        "                     values=[x for x in df['DiffWalking'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=3, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Sex'].value_counts().index),\n",
        "                     values=[x for x in df['Sex'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=3, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Race'].value_counts().index),\n",
        "                     values=[x for x in df['Race'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=4, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['PhysicalActivity'].value_counts().index),\n",
        "                     values=[x for x in df['PhysicalActivity'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=4, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Diabetic'].value_counts().index),\n",
        "                     values=[x for x in df['Diabetic'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=5, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['GenHealth'].value_counts().index),\n",
        "                     values=[x for x in df['GenHealth'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', marker_colors=colours),\n",
        "              row=5, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['Asthma'].value_counts().index),\n",
        "                     values=[x for x in df['Asthma'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=6, col=1)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['KidneyDisease'].value_counts().index),\n",
        "                     values=[x for x in df['KidneyDisease'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=6, col=2)\n",
        "\n",
        "fig.add_trace(go.Pie(labels=np.array(df['SkinCancer'].value_counts().index),\n",
        "                     values=[x for x in df['SkinCancer'].value_counts()], hole=.35,\n",
        "                     textinfo='label+percent', rotation=-45, marker_colors=colours),\n",
        "              row=7, col=1)\n",
        "\n",
        "\n",
        "fig.update_layout(height=3200, font=dict(size=14), showlegend=False)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN80WRX0B9oF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw2J_z97E2tT"
      },
      "source": [
        "## Preprocessing datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "jGLMUNYVE7EZ",
        "outputId": "dc7290a0-182a-4d6d-88b3-ee7a6f9da381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Continuous Columns\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fb84d56fb50>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_1136d_row0_col0, #T_1136d_row0_col1, #T_1136d_row0_col2 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_1136d_row1_col0, #T_1136d_row1_col1, #T_1136d_row2_col0, #T_1136d_row3_col2 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_1136d_row1_col2, #T_1136d_row2_col2, #T_1136d_row3_col0 {\n",
              "  background-color: #e7f0fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_1136d_row2_col1 {\n",
              "  background-color: #f3f8fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_1136d_row3_col1 {\n",
              "  background-color: #d9e8f5;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_1136d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_1136d_level0_col0\" class=\"col_heading level0 col0\" >min</th>\n",
              "      <th id=\"T_1136d_level0_col1\" class=\"col_heading level0 col1\" >mean</th>\n",
              "      <th id=\"T_1136d_level0_col2\" class=\"col_heading level0 col2\" >max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_1136d_level0_row0\" class=\"row_heading level0 row0\" >BMI</th>\n",
              "      <td id=\"T_1136d_row0_col0\" class=\"data row0 col0\" >12.020000</td>\n",
              "      <td id=\"T_1136d_row0_col1\" class=\"data row0 col1\" >28.325399</td>\n",
              "      <td id=\"T_1136d_row0_col2\" class=\"data row0 col2\" >94.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1136d_level0_row1\" class=\"row_heading level0 row1\" >PhysicalHealth</th>\n",
              "      <td id=\"T_1136d_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
              "      <td id=\"T_1136d_row1_col1\" class=\"data row1 col1\" >3.371710</td>\n",
              "      <td id=\"T_1136d_row1_col2\" class=\"data row1 col2\" >30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1136d_level0_row2\" class=\"row_heading level0 row2\" >MentalHealth</th>\n",
              "      <td id=\"T_1136d_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n",
              "      <td id=\"T_1136d_row2_col1\" class=\"data row2 col1\" >3.898366</td>\n",
              "      <td id=\"T_1136d_row2_col2\" class=\"data row2 col2\" >30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1136d_level0_row3\" class=\"row_heading level0 row3\" >SleepTime</th>\n",
              "      <td id=\"T_1136d_row3_col0\" class=\"data row3 col0\" >1.000000</td>\n",
              "      <td id=\"T_1136d_row3_col1\" class=\"data row3 col1\" >7.097075</td>\n",
              "      <td id=\"T_1136d_row3_col2\" class=\"data row3 col2\" >24.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "print('Continuous Columns')\n",
        "df.select_dtypes(include=['float']).describe().T[['min', 'mean', 'max']].style.background_gradient(cmap='Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw_1jCBkFCTN"
      },
      "outputs": [],
      "source": [
        "for col in ContinuesFeatues:\n",
        "    df[col] = df[col]/df[col].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV5KQTcYFKPm",
        "outputId": "be90bc55-99cf-471c-8327-4ab1598bcbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Columns\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeartDisease        2\n",
              "Smoking             2\n",
              "AlcoholDrinking     2\n",
              "Stroke              2\n",
              "DiffWalking         2\n",
              "Sex                 2\n",
              "Race                6\n",
              "Diabetic            4\n",
              "PhysicalActivity    2\n",
              "GenHealth           5\n",
              "Asthma              2\n",
              "KidneyDisease       2\n",
              "SkinCancer          2\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "print('Categorical Columns\\n')\n",
        "df.select_dtypes(include=['O']).nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLcWgN4gFXXP"
      },
      "source": [
        "## Correlation Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHat_bJ8GHYc"
      },
      "outputs": [],
      "source": [
        "# Integer encode columns with 2 unique values\n",
        "for col in ['HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex', 'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer']:\n",
        "    if df[col].dtype == 'O':\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "# One-hot encode columns with more than 2 unique values\n",
        "df = pd.get_dummies(df, columns=['Race', 'Diabetic', 'GenHealth', ], prefix = ['Race', 'Diabetic', 'GenHealth'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "vmTNiD-YFYJA",
        "outputId": "32431278-d8dd-4d96-d1d8-ac7be4a75318"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"efa921d1-a9e1-4283-90fe-7dee79c22d2b\" class=\"plotly-graph-div\" style=\"height:800px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"efa921d1-a9e1-4283-90fe-7dee79c22d2b\")) {                    Plotly.newPlot(                        \"efa921d1-a9e1-4283-90fe-7dee79c22d2b\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"HeartDisease\",\"Smoking\",\"AlcoholDrinking\",\"Stroke\",\"DiffWalking\",\"Sex\",\"PhysicalActivity\",\"Asthma\",\"KidneyDisease\",\"SkinCancer\",\"BMI\",\"PhysicalHealth\",\"MentalHealth\",\"AgeCategory\",\"SleepTime\"],\"y\":[\"HeartDisease\",\"Smoking\",\"AlcoholDrinking\",\"Stroke\",\"DiffWalking\",\"Sex\",\"PhysicalActivity\",\"Asthma\",\"KidneyDisease\",\"SkinCancer\",\"BMI\",\"PhysicalHealth\",\"MentalHealth\",\"AgeCategory\",\"SleepTime\"],\"z\":[[1.0,0.10776415602593577,-0.03207974262500417,0.19683529884287068,0.2012580485737773,0.07004047624891856,-0.10002993385404217,0.04144415110032234,0.14519709877256354,0.09331687769597886,0.05180319065574589,0.1707209719661753,0.028590714546905302,0.23158320851970648,0.008326646858286093],[0.10776415602593577,1.0,0.11176752021942088,0.06122603951647455,0.12007416384008394,0.08505248562950851,-0.0971737659539628,0.024148532018300563,0.03491968628492668,0.03397738727204867,0.02311811243270034,0.1153524139404747,0.08515728503775259,0.1306120821603677,-0.030335635088083682],[-0.03207974262500417,0.11176752021942088,1.0,-0.01985791423814558,-0.03532758282688487,0.004200142395935715,0.01748698277743856,-0.0022021002378207716,-0.028280091717843844,-0.005702370452955972,-0.03881622302230272,-0.01725428832282563,0.0512819728218972,-0.058587008626551716,-0.00506545083386641],[0.19683529884287068,0.06122603951647455,-0.01985791423814558,1.0,0.17414321368906785,-0.0030910549780765223,-0.0794551948440212,0.03886614032726421,0.09116684064825993,0.04811610449906195,0.019732982346395637,0.13701382711696813,0.046467061321632086,0.13673831258982225,0.01189998096058299],[0.2012580485737773,0.12007416384008394,-0.03532758282688487,0.17414321368906785,1.0,-0.06885955945150612,-0.2785239644744156,0.10322204883284222,0.15306375213121043,0.06484040178427677,0.18167826388865732,0.4283727985461525,0.15223466914599132,0.2413276153382443,-0.022216357334953533],[0.07004047624891856,0.08505248562950851,0.004200142395935715,-0.0030910549780765223,-0.06885955945150612,1.0,0.048246845809630415,-0.06919111638314385,-0.009083858389322482,0.01343379974373332,0.026939645259099483,-0.040903839313019365,-0.10005847280447244,-0.06700074478508969,-0.01570374828060869],[-0.10002993385404217,-0.0971737659539628,0.01748698277743856,-0.0794551948440212,-0.2785239644744156,0.048246845809630415,1.0,-0.0415258817742273,-0.0818273214414323,-0.0013278104683075525,-0.15061599390258318,-0.23228317707681606,-0.09580810490226797,-0.12036867337825854,0.003848841379442122],[0.04144415110032234,0.024148532018300563,-0.0022021002378207716,0.03886614032726421,0.10322204883284222,-0.06919111638314385,-0.0415258817742273,1.0,0.03970700033814702,-0.0003964769423400938,0.092345019006225,0.11790658022327909,0.1140081744247549,-0.05788658709501616,-0.04824528030804831],[0.14519709877256354,0.03491968628492668,-0.028280091717843844,0.09116684064825993,0.15306375213121043,-0.009083858389322482,-0.0818273214414323,0.03970700033814702,1.0,0.06181621651419233,0.050767532862528414,0.1421971848382409,0.03728112808809446,0.12249331295148701,0.006237934101449502],[0.09331687769597886,0.03397738727204867,-0.005702370452955972,0.04811610449906195,0.06484040178427677,0.01343379974373332,-0.0013278104683075525,-0.0003964769423400938,0.06181621651419233,1.0,-0.033643618515338786,0.0416996854817131,-0.033412190709220524,0.26139092150617554,0.041266167486529615],[0.05180319065574589,0.02311811243270034,-0.03881622302230272,0.019732982346395637,0.18167826388865732,0.026939645259099483,-0.15061599390258318,0.092345019006225,0.050767532862528414,-0.033643618515338786,1.0,0.1097875436096874,0.06413056947152597,-0.0004463909346209674,-0.05182225399892374],[0.1707209719661753,0.1153524139404747,-0.01725428832282563,0.13701382711696813,0.4283727985461525,-0.040903839313019365,-0.23228317707681606,0.11790658022327909,0.1421971848382409,0.0416996854817131,0.1097875436096874,1.0,0.2879866740873287,0.11100961741149926,-0.061386631929495],[0.028590714546905302,0.08515728503775259,0.0512819728218972,0.046467061321632086,0.15223466914599132,-0.10005847280447244,-0.09580810490226797,0.1140081744247549,0.03728112808809446,-0.033412190709220524,0.06413056947152597,0.2879866740873287,1.0,-0.1553270117678171,-0.11971678803142047],[0.23158320851970648,0.1306120821603677,-0.058587008626551716,0.13673831258982225,0.2413276153382443,-0.06700074478508969,-0.12036867337825854,-0.05788658709501616,0.12249331295148701,0.26139092150617554,-0.0004463909346209674,0.11100961741149926,-0.1553270117678171,1.0,0.1027001320622866],[0.008326646858286093,-0.030335635088083682,-0.00506545083386641,0.01189998096058299,-0.022216357334953533,-0.01570374828060869,0.003848841379442122,-0.04824528030804831,0.006237934101449502,0.041266167486529615,-0.05182225399892374,-0.061386631929495,-0.11971678803142047,0.1027001320622866,1.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"margin\":{\"t\":60},\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('efa921d1-a9e1-4283-90fe-7dee79c22d2b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = px.imshow(df[['HeartDisease', 'Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex',\n",
        "                    'PhysicalActivity', 'Asthma', 'KidneyDisease', 'SkinCancer','BMI', 'PhysicalHealth',\n",
        "                    'MentalHealth', 'AgeCategory', 'SleepTime']].corr(),\n",
        "                color_continuous_scale=\"Blues\")\n",
        "fig.update_layout(height=800)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayjGREK1GRIF",
        "outputId": "014fe28d-6e74-4232-e775-45282b4fd2b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    292422\n",
              "1     27373\n",
              "Name: HeartDisease, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df['HeartDisease'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiyNMxTwGgJM"
      },
      "source": [
        "# Проблема дисбалансировки\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPAWCNtEGfsk",
        "outputId": "217f8a7a-4c73-460d-a241-2c116707cfb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    292422\n",
            "1    292422\n",
            "Name: HeartDisease, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "class_0 = df[df['HeartDisease'] == 0]\n",
        "class_1 = df[df['HeartDisease'] == 1]\n",
        "\n",
        "class_1 = class_1.sample(len(class_0),replace=True)\n",
        "df = pd.concat([class_0, class_1], axis=0)\n",
        "print(df['HeartDisease'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BAUc285G-17"
      },
      "source": [
        "# Features & Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG0EK8-aHB6r",
        "outputId": "30e8c6c5-2f9e-47b4-be46-a9b99b74dd61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((584844, 29), (584844,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "features = np.array(df[['BMI', 'Smoking', 'AlcoholDrinking', 'Stroke',\n",
        "       'PhysicalHealth', 'MentalHealth', 'DiffWalking', 'Sex', 'AgeCategory',\n",
        "       'PhysicalActivity', 'SleepTime', 'Asthma', 'KidneyDisease',\n",
        "       'SkinCancer', 'Race_American Indian/Alaskan Native', 'Race_Asian',\n",
        "       'Race_Black', 'Race_Hispanic', 'Race_Other', 'Race_White',\n",
        "       'Diabetic_No', 'Diabetic_No, borderline diabetes', 'Diabetic_Yes',\n",
        "       'Diabetic_Yes (during pregnancy)', 'GenHealth_Excellent',\n",
        "       'GenHealth_Fair', 'GenHealth_Good', 'GenHealth_Poor',\n",
        "       'GenHealth_Very good']])\n",
        "\n",
        "labels = np.array(df['HeartDisease'])\n",
        "\n",
        "features.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDAk2jldA3UC",
        "outputId": "1225e347-74a6-4f75-fbbc-ce5904c33c5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.21444386, 0.        , 0.        , 1.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 1.        , 1.        ,\n",
              "       0.29166667, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
              "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "features[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlnF4H9tFLSD"
      },
      "source": [
        "# Model Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "firJhsq7i7bz",
        "outputId": "9d698b52-cc68-49a3-83a9-a9b2d039b5a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(467875, 116969)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "input_tensor = torch.from_numpy(features.astype(np.float32))\n",
        "label_tensor = torch.from_numpy(labels.astype(np.float32))\n",
        "\n",
        "dataset = TensorDataset(input_tensor, label_tensor)\n",
        "\n",
        "train_len = int(len(dataset) * 0.8)\n",
        "val_len = int(len(dataset) - train_len)\n",
        "\n",
        "(train_len, val_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKEhb-sTB5vD"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=29, num_workers=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=29, num_workers=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghyg_Zi6DRhx"
      },
      "source": [
        "### Test dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggcA9PSwDUET",
        "outputId": "6d8e4d65-4975-4571-f538-7cdc7f890479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element: 0\n",
            "X:\n",
            "tensor([[0.4129, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 1.0000, 0.2625,\n",
            "         1.0000, 0.3750, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.1815, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.4000,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4084, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.9625,\n",
            "         0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2966, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4669, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.6500,\n",
            "         0.0000, 0.2500, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3070, 1.0000, 0.0000, 0.0000, 0.1667, 0.9333, 1.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3407, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.5000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.4281, 0.0000, 0.0000, 0.0000, 0.0667, 0.6667, 0.0000, 1.0000, 0.5250,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2521, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 1.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2147, 1.0000, 0.0000, 0.0000, 0.3333, 0.6667, 1.0000, 0.0000, 0.7125,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3286, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.2500, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2394, 0.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2500, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2144, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2118, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2625,\n",
            "         1.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2144, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3468, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3032, 1.0000, 0.0000, 1.0000, 0.0000, 0.0667, 0.0000, 1.0000, 0.7750,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3118, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2801, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3253, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 1.0000, 0.5875,\n",
            "         1.0000, 0.2083, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3318, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2958, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2325, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.3375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3085, 0.0000, 0.0000, 0.0000, 0.1000, 0.3333, 1.0000, 0.0000, 0.7750,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2940, 1.0000, 0.0000, 1.0000, 0.0000, 0.9667, 1.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2805, 0.0000, 0.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 0.4625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2552, 1.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.4625,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2383, 0.0000, 0.0000, 0.0000, 0.0667, 0.3333, 0.0000, 0.0000, 0.8375,\n",
            "         0.0000, 0.3333, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 1\n",
            "X:\n",
            "tensor([[0.3511, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.9625,\n",
            "         0.0000, 0.4583, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2882, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2405, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3667, 0.0000, 0.0000, 0.0000, 0.1667, 0.0333, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.3750, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3061, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.3333, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3785, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.9625,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4433, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.7125,\n",
            "         0.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3063, 1.0000, 0.0000, 0.0000, 0.6667, 0.1667, 1.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2212, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.4625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2467, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3912, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.9000,\n",
            "         0.0000, 0.3333, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3113, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2895, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3044, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3550, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3316, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3403, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3258, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.5875,\n",
            "         1.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3101, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2707, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2895, 1.0000, 0.0000, 0.0000, 0.0000, 0.1333, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.2500, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3362, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2879, 0.0000, 0.0000, 0.0000, 0.0000, 0.1667, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2856, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3158, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2212, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4211, 1.0000, 0.0000, 0.0000, 0.0000, 0.5000, 1.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2647, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
            "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 2\n",
            "X:\n",
            "tensor([[0.3607, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2859, 0.0000, 0.0000, 0.0000, 0.0667, 0.1667, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2368, 1.0000, 0.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3801, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9625,\n",
            "         0.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3477, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2718, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4488, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.5450, 1.0000, 1.0000, 0.0000, 1.0000, 0.6667, 1.0000, 1.0000, 0.5250,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2859, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3268, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3735, 0.0000, 0.0000, 0.0000, 0.1000, 0.1667, 0.0000, 0.0000, 0.5875,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2312, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5875,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5665, 1.0000, 0.0000, 0.0000, 0.5000, 0.0000, 1.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4628, 0.0000, 0.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 0.5250,\n",
            "         1.0000, 0.2083, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2782, 1.0000, 1.0000, 0.0000, 0.0000, 0.0667, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2897, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2491, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2862, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1928, 1.0000, 0.0000, 0.0000, 0.0333, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2808, 0.0000, 0.0000, 0.0000, 0.0000, 0.1333, 1.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3425, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2162, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2280, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2116, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.2625,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3735, 0.0000, 0.0000, 0.0000, 0.9667, 0.5000, 1.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3308, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2702, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.5000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2571, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7125,\n",
            "         0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 3\n",
            "X:\n",
            "tensor([[0.3113, 0.0000, 0.0000, 0.0000, 0.1667, 0.0333, 0.0000, 1.0000, 0.2625,\n",
            "         1.0000, 0.4167, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3912, 0.0000, 0.0000, 0.0000, 0.3333, 0.6667, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1873, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 1.0000, 0.0000, 0.9625,\n",
            "         0.0000, 0.5000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8375,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.4162, 0.0000, 0.0000, 0.0000, 1.0000, 0.3333, 1.0000, 0.0000, 0.7750,\n",
            "         0.0000, 0.1667, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3347, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5250,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.1928, 0.0000, 0.0000, 0.0000, 1.0000, 0.3333, 1.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2700, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2334, 0.0000, 0.0000, 0.0000, 0.5000, 0.0667, 0.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2886, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4050, 1.0000, 0.0000, 1.0000, 0.0333, 0.0000, 0.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.1667, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2874, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2972, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2603, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.4625,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4424, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2950, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2624, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.5875,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3645, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.0000, 0.0000, 0.4000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2978, 1.0000, 0.0000, 0.0000, 0.2333, 1.0000, 0.0000, 0.0000, 0.5250,\n",
            "         0.0000, 0.2083, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3061, 1.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.5780, 1.0000, 0.0000, 0.0000, 0.0667, 1.0000, 1.0000, 0.0000, 0.4625,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3616, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4625,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4216, 1.0000, 0.0000, 0.0000, 0.5000, 0.1333, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3347, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8375,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2279, 1.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 1.0000, 0.4625,\n",
            "         1.0000, 0.2500, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2368, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.9625,\n",
            "         0.0000, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3425, 0.0000, 0.0000, 0.0000, 0.1000, 0.0333, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 4\n",
            "X:\n",
            "tensor([[0.2571, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2732, 0.0000, 0.0000, 0.0000, 0.0667, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3066, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.2083, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2263, 1.0000, 0.0000, 1.0000, 1.0000, 0.5000, 1.0000, 0.0000, 0.9000,\n",
            "         0.0000, 0.2500, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.2500, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2638, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3503, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3076, 0.0000, 0.0000, 0.0000, 0.2667, 0.2667, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.2625,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3735, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 1.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.4006, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3735, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.7750,\n",
            "         0.0000, 0.3750, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.3301, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2421, 1.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.0000, 0.0000, 0.7125,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4152, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3782, 1.0000, 0.0000, 0.0000, 0.2000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2927, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 1.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000],\n",
            "        [0.2693, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.5875,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3567, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2430, 0.0000, 0.0000, 0.0000, 0.7000, 0.1667, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3389, 1.0000, 0.0000, 0.0000, 0.0333, 0.0000, 1.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2596, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2662, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2461, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.2917, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2808, 1.0000, 0.0000, 0.0000, 0.3333, 0.1667, 1.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.3333, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2545, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3074, 0.0000, 0.0000, 0.0000, 0.0667, 1.0000, 0.0000, 1.0000, 0.5250,\n",
            "         0.0000, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "y.shape:\n",
            "torch.Size([29])\n",
            "Element: 5\n",
            "X:\n",
            "tensor([[0.3477, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2603, 1.0000, 0.0000, 0.0000, 0.0667, 1.0000, 0.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2426, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.4633, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.2500, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3619, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.9000,\n",
            "         1.0000, 0.3750, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4432, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.5875,\n",
            "         1.0000, 0.3333, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3031, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2406, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6500,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3518, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3764, 0.0000, 0.0000, 0.0000, 0.1333, 0.0000, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.3333, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2368, 1.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.2917, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3085, 1.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3295, 0.0000, 0.0000, 0.0000, 0.1667, 0.5000, 0.0000, 0.0000, 0.5250,\n",
            "         1.0000, 0.2083, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2360, 1.0000, 0.0000, 1.0000, 0.5000, 0.5000, 0.0000, 1.0000, 0.7750,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2468, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2265, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4625,\n",
            "         1.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2972, 0.0000, 0.0000, 0.0000, 0.5000, 0.5000, 1.0000, 1.0000, 0.8375,\n",
            "         0.0000, 0.2083, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2715, 0.0000, 0.0000, 0.0000, 0.1333, 0.2333, 0.0000, 0.0000, 0.4000,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2886, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2978, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6500,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2676, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.8375,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.3823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000,\n",
            "         0.0000, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3162, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 1.0000, 0.5875,\n",
            "         1.0000, 0.2917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2722, 1.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.0000, 1.0000, 1.0000,\n",
            "         1.0000, 0.3333, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2886, 1.0000, 0.0000, 0.0000, 0.0333, 0.0333, 0.0000, 1.0000, 0.9000,\n",
            "         1.0000, 0.2917, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.3026, 1.0000, 0.0000, 0.0000, 0.5000, 0.1667, 0.0000, 1.0000, 0.9625,\n",
            "         1.0000, 0.4583, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2759, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.8375,\n",
            "         1.0000, 0.3333, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000],\n",
            "        [0.2723, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.7125,\n",
            "         0.0000, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000]])\n",
            "X.shape:\n",
            "torch.Size([29, 29])\n",
            "y:\n",
            "tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
            "        0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
            "y.shape:\n",
            "torch.Size([29])\n"
          ]
        }
      ],
      "source": [
        "size = len(train_dataloader.dataset)\n",
        "for idx, (X, y) in enumerate(train_dataloader):\n",
        "    print(\"Element: \" + str(idx))\n",
        "    print(\"X:\")\n",
        "    print(X)\n",
        "    print(\"X.shape:\")\n",
        "    print(X.shape)\n",
        "    print(\"y:\")\n",
        "    print(y)\n",
        "    print(\"y.shape:\")\n",
        "    print(y.shape)\n",
        "    if idx == 5:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJRZj0TxCWxQ"
      },
      "source": [
        "## Torch model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTowd4AQHOiH"
      },
      "outputs": [],
      "source": [
        "class TropicalPlusTorch:\n",
        "\n",
        "\n",
        "    def max_plus_sum(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        # max-plus sum\n",
        "        return torch.maximum(A, B).to(device)\n",
        "\n",
        "    def min_plus_sum(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        # min-plus sum\n",
        "        return torch.minimum(A, B).to(device)\n",
        "\n",
        "    def max_plus_mul(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        # max-plus mul\n",
        "        res_m = A.size()[0]\n",
        "        res_n = B.size()[1]\n",
        "        new_B = torch.transpose(B, 0, 1).repeat(res_m, 1).to(device)\n",
        "        new_A = torch.repeat_interleave(A, torch.full((1, A.size()[0]), res_n)[0].to(device), dim=0).to(device)\n",
        "        return torch.reshape(torch.amax(new_A + new_B, 1), (res_m, res_n)).to(device)\n",
        "\n",
        "    def min_plus_mul(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        # min-plus mul\n",
        "        res_m = A.size()[0]\n",
        "        res_n = B.size()[1]\n",
        "        new_B = torch.transpose(B, 0, 1).repeat(res_m, 1).to(device)\n",
        "        new_A = torch.repeat_interleave(A, torch.full((1, A.size()[0]), res_n)[0].to(device), dim=0).to(device)\n",
        "        return torch.reshape(torch.amin(new_A + new_B, 1), (res_m, res_n)).to(device)\n",
        "\n",
        "\n",
        "tropic = TropicalPlusTorch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL910nRO2bp8"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class TropicalLinearLayer_max(nn.Module):\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "    def __init__(self, size_in, size_out):\n",
        "        super().__init__()\n",
        "        self.size_in, self.size_out = size_in, size_out\n",
        "        weights = torch.Tensor(size_out, size_in)\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "        bias = torch.Tensor(size_out)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        # initialize weights and biases\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape, self.weights.t().shape)\n",
        "        w_time_x = tropic.max_plus_mul(x, self.weights.t()) #tropical_max_plus(x, self.weights.t())\n",
        "        #print(w_time_x.shape, self.bias.shape)\n",
        "        return tropic.max_plus_sum(w_time_x, self.bias)\n",
        "\n",
        "class TropicalLinearLayer_min(nn.Module):\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "    def __init__(self, size_in, size_out):\n",
        "        super().__init__()\n",
        "        self.size_in, self.size_out = size_in, size_out\n",
        "        weights = torch.Tensor(size_out, size_in)\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "        bias = torch.Tensor(size_out)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        # initialize weights and biases\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape, self.weights.t().shape)\n",
        "        w_time_x = tropic.min_plus_mul(x, self.weights.t()) #tropical_max_plus(x, self.weights.t())\n",
        "        #print(w_time_x.shape, self.bias.shape)\n",
        "        return tropic.min_plus_sum(w_time_x, self.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F3hpk0LCWIq",
        "outputId": "f7f0665e-cd1d-40f1-d32a-609d4b9408b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (network): Sequential(\n",
              "    (0): Linear(in_features=29, out_features=58, bias=True)\n",
              "    (1): TropicalLinearLayer_max()\n",
              "    (2): Linear(in_features=29, out_features=58, bias=True)\n",
              "    (3): TropicalLinearLayer_max()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from torch import nn, Tensor\n",
        "import math \n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        linLayer = TropicalLinearLayer_max # nn.Linear\n",
        "        minLayer = TropicalLinearLayer_min\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(29, 58),\n",
        "            linLayer(58, 29),\n",
        "            nn.Linear(29, 58),\n",
        "            linLayer(58, 2),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.network(x)\n",
        "        #x = torch.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VmWb9vKCsUf"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEgNwTt2_ZXd"
      },
      "outputs": [],
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "train_acc = []\n",
        "val_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQhdsFszCtRx"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, epoch=0):\n",
        "    size = len(dataloader.dataset)\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.type(torch.LongTensor)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        epoch_loss += loss.item()\n",
        "        # Backpropagation\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    train_acc.append(correct / size)\n",
        "    train_loss.append(epoch_loss / size)\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, epoch):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.type(torch.LongTensor)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            i += 1\n",
        "    test_loss /= num_batches\n",
        "    val_loss.append(test_loss)\n",
        "    correct /= size\n",
        "    val_acc.append(correct)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Rye25al-6cjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du3NOCgxCzIa",
        "outputId": "b653299d-117d-4f59-d3d3-7f159bb24b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "loss: 0.509818  [466900/467875]\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.477136 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.401281  [    0/467875]\n",
            "loss: 0.387143  [ 2900/467875]\n",
            "loss: 0.524934  [ 5800/467875]\n",
            "loss: 0.384239  [ 8700/467875]\n",
            "loss: 0.396292  [11600/467875]\n",
            "loss: 0.721632  [14500/467875]\n",
            "loss: 0.503254  [17400/467875]\n",
            "loss: 0.489028  [20300/467875]\n",
            "loss: 0.369661  [23200/467875]\n",
            "loss: 0.368792  [26100/467875]\n",
            "loss: 0.351292  [29000/467875]\n",
            "loss: 0.518983  [31900/467875]\n",
            "loss: 0.382069  [34800/467875]\n",
            "loss: 0.445745  [37700/467875]\n",
            "loss: 0.759497  [40600/467875]\n",
            "loss: 0.410129  [43500/467875]\n",
            "loss: 0.460267  [46400/467875]\n",
            "loss: 0.516613  [49300/467875]\n",
            "loss: 0.517343  [52200/467875]\n",
            "loss: 0.511370  [55100/467875]\n",
            "loss: 0.405190  [58000/467875]\n",
            "loss: 0.691821  [60900/467875]\n",
            "loss: 0.370434  [63800/467875]\n",
            "loss: 0.427573  [66700/467875]\n",
            "loss: 0.462353  [69600/467875]\n",
            "loss: 0.592581  [72500/467875]\n",
            "loss: 0.478060  [75400/467875]\n",
            "loss: 0.535697  [78300/467875]\n",
            "loss: 0.557476  [81200/467875]\n",
            "loss: 0.453868  [84100/467875]\n",
            "loss: 0.364212  [87000/467875]\n",
            "loss: 0.537322  [89900/467875]\n",
            "loss: 0.641391  [92800/467875]\n",
            "loss: 0.433092  [95700/467875]\n",
            "loss: 0.638705  [98600/467875]\n",
            "loss: 0.469165  [101500/467875]\n",
            "loss: 0.404608  [104400/467875]\n",
            "loss: 0.261247  [107300/467875]\n",
            "loss: 0.366426  [110200/467875]\n",
            "loss: 0.491080  [113100/467875]\n",
            "loss: 0.511590  [116000/467875]\n",
            "loss: 0.604264  [118900/467875]\n",
            "loss: 0.508367  [121800/467875]\n",
            "loss: 0.667116  [124700/467875]\n",
            "loss: 0.408684  [127600/467875]\n",
            "loss: 0.575408  [130500/467875]\n",
            "loss: 0.315569  [133400/467875]\n",
            "loss: 0.386098  [136300/467875]\n",
            "loss: 0.601773  [139200/467875]\n",
            "loss: 0.505133  [142100/467875]\n",
            "loss: 0.433587  [145000/467875]\n",
            "loss: 0.338151  [147900/467875]\n",
            "loss: 0.589879  [150800/467875]\n",
            "loss: 0.446921  [153700/467875]\n",
            "loss: 0.516800  [156600/467875]\n",
            "loss: 0.670301  [159500/467875]\n",
            "loss: 0.439949  [162400/467875]\n",
            "loss: 0.408737  [165300/467875]\n",
            "loss: 0.459569  [168200/467875]\n",
            "loss: 0.372325  [171100/467875]\n",
            "loss: 0.295385  [174000/467875]\n",
            "loss: 0.478847  [176900/467875]\n",
            "loss: 0.472516  [179800/467875]\n",
            "loss: 0.520018  [182700/467875]\n",
            "loss: 0.458803  [185600/467875]\n",
            "loss: 0.395808  [188500/467875]\n",
            "loss: 0.477866  [191400/467875]\n",
            "loss: 0.400608  [194300/467875]\n",
            "loss: 0.454595  [197200/467875]\n",
            "loss: 0.331956  [200100/467875]\n",
            "loss: 0.383196  [203000/467875]\n",
            "loss: 0.311580  [205900/467875]\n",
            "loss: 0.395836  [208800/467875]\n",
            "loss: 0.602866  [211700/467875]\n",
            "loss: 0.449863  [214600/467875]\n",
            "loss: 0.345169  [217500/467875]\n",
            "loss: 0.691787  [220400/467875]\n",
            "loss: 0.532341  [223300/467875]\n",
            "loss: 0.500682  [226200/467875]\n",
            "loss: 0.564410  [229100/467875]\n",
            "loss: 0.266578  [232000/467875]\n",
            "loss: 0.439484  [234900/467875]\n",
            "loss: 0.487085  [237800/467875]\n",
            "loss: 0.636096  [240700/467875]\n",
            "loss: 0.476683  [243600/467875]\n",
            "loss: 0.443297  [246500/467875]\n",
            "loss: 0.314883  [249400/467875]\n",
            "loss: 0.282971  [252300/467875]\n",
            "loss: 0.575240  [255200/467875]\n",
            "loss: 0.415158  [258100/467875]\n",
            "loss: 0.486221  [261000/467875]\n",
            "loss: 0.568907  [263900/467875]\n",
            "loss: 0.385380  [266800/467875]\n",
            "loss: 0.419364  [269700/467875]\n",
            "loss: 0.510326  [272600/467875]\n",
            "loss: 0.598587  [275500/467875]\n",
            "loss: 0.609586  [278400/467875]\n",
            "loss: 0.567442  [281300/467875]\n",
            "loss: 0.534517  [284200/467875]\n",
            "loss: 0.508130  [287100/467875]\n",
            "loss: 0.406022  [290000/467875]\n",
            "loss: 0.458968  [292900/467875]\n",
            "loss: 0.543944  [295800/467875]\n",
            "loss: 0.474620  [298700/467875]\n",
            "loss: 0.574224  [301600/467875]\n",
            "loss: 0.556997  [304500/467875]\n",
            "loss: 0.372970  [307400/467875]\n",
            "loss: 0.540855  [310300/467875]\n",
            "loss: 0.387059  [313200/467875]\n",
            "loss: 0.612788  [316100/467875]\n",
            "loss: 0.485001  [319000/467875]\n",
            "loss: 0.403045  [321900/467875]\n",
            "loss: 0.472776  [324800/467875]\n",
            "loss: 0.486376  [327700/467875]\n",
            "loss: 0.464511  [330600/467875]\n",
            "loss: 0.332474  [333500/467875]\n",
            "loss: 0.383833  [336400/467875]\n",
            "loss: 0.396846  [339300/467875]\n",
            "loss: 0.356310  [342200/467875]\n",
            "loss: 0.362063  [345100/467875]\n",
            "loss: 0.373761  [348000/467875]\n",
            "loss: 0.426161  [350900/467875]\n",
            "loss: 0.658492  [353800/467875]\n",
            "loss: 0.499229  [356700/467875]\n",
            "loss: 0.644177  [359600/467875]\n",
            "loss: 0.446862  [362500/467875]\n",
            "loss: 0.440899  [365400/467875]\n",
            "loss: 0.504626  [368300/467875]\n",
            "loss: 0.259413  [371200/467875]\n",
            "loss: 0.612056  [374100/467875]\n",
            "loss: 0.652741  [377000/467875]\n",
            "loss: 0.353101  [379900/467875]\n",
            "loss: 0.410986  [382800/467875]\n",
            "loss: 0.441006  [385700/467875]\n",
            "loss: 0.406549  [388600/467875]\n",
            "loss: 0.460802  [391500/467875]\n",
            "loss: 0.419821  [394400/467875]\n",
            "loss: 0.520245  [397300/467875]\n",
            "loss: 0.628447  [400200/467875]\n",
            "loss: 0.604493  [403100/467875]\n",
            "loss: 0.408354  [406000/467875]\n",
            "loss: 0.558385  [408900/467875]\n",
            "loss: 0.459372  [411800/467875]\n",
            "loss: 0.406354  [414700/467875]\n",
            "loss: 0.399940  [417600/467875]\n",
            "loss: 0.472729  [420500/467875]\n",
            "loss: 0.455079  [423400/467875]\n",
            "loss: 0.453849  [426300/467875]\n",
            "loss: 0.491063  [429200/467875]\n",
            "loss: 0.810517  [432100/467875]\n",
            "loss: 0.423445  [435000/467875]\n",
            "loss: 0.375097  [437900/467875]\n",
            "loss: 0.566856  [440800/467875]\n",
            "loss: 0.452551  [443700/467875]\n",
            "loss: 0.504811  [446600/467875]\n",
            "loss: 0.529566  [449500/467875]\n",
            "loss: 0.460768  [452400/467875]\n",
            "loss: 0.566502  [455300/467875]\n",
            "loss: 0.439219  [458200/467875]\n",
            "loss: 0.542577  [461100/467875]\n",
            "loss: 0.634585  [464000/467875]\n",
            "loss: 0.420225  [466900/467875]\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476460 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 24 s\n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.636109  [    0/467875]\n",
            "loss: 0.457361  [ 2900/467875]\n",
            "loss: 0.455575  [ 5800/467875]\n",
            "loss: 0.611680  [ 8700/467875]\n",
            "loss: 0.670794  [11600/467875]\n",
            "loss: 0.618513  [14500/467875]\n",
            "loss: 0.506886  [17400/467875]\n",
            "loss: 0.357776  [20300/467875]\n",
            "loss: 0.480088  [23200/467875]\n",
            "loss: 0.420975  [26100/467875]\n",
            "loss: 0.480568  [29000/467875]\n",
            "loss: 0.501486  [31900/467875]\n",
            "loss: 0.346712  [34800/467875]\n",
            "loss: 0.449346  [37700/467875]\n",
            "loss: 0.523599  [40600/467875]\n",
            "loss: 0.457556  [43500/467875]\n",
            "loss: 0.363210  [46400/467875]\n",
            "loss: 0.456218  [49300/467875]\n",
            "loss: 0.422793  [52200/467875]\n",
            "loss: 0.536399  [55100/467875]\n",
            "loss: 0.562091  [58000/467875]\n",
            "loss: 0.477022  [60900/467875]\n",
            "loss: 0.562225  [63800/467875]\n",
            "loss: 0.449099  [66700/467875]\n",
            "loss: 0.484729  [69600/467875]\n",
            "loss: 0.512249  [72500/467875]\n",
            "loss: 0.474992  [75400/467875]\n",
            "loss: 0.626043  [78300/467875]\n",
            "loss: 0.648323  [81200/467875]\n",
            "loss: 0.467146  [84100/467875]\n",
            "loss: 0.367969  [87000/467875]\n",
            "loss: 0.695553  [89900/467875]\n",
            "loss: 0.455884  [92800/467875]\n",
            "loss: 0.390090  [95700/467875]\n",
            "loss: 0.522033  [98600/467875]\n",
            "loss: 0.433979  [101500/467875]\n",
            "loss: 0.515400  [104400/467875]\n",
            "loss: 0.504790  [107300/467875]\n",
            "loss: 0.515909  [110200/467875]\n",
            "loss: 0.278856  [113100/467875]\n",
            "loss: 0.346653  [116000/467875]\n",
            "loss: 0.450613  [118900/467875]\n",
            "loss: 0.426333  [121800/467875]\n",
            "loss: 0.372876  [124700/467875]\n",
            "loss: 0.498257  [127600/467875]\n",
            "loss: 0.545985  [130500/467875]\n",
            "loss: 0.409653  [133400/467875]\n",
            "loss: 0.373060  [136300/467875]\n",
            "loss: 0.317952  [139200/467875]\n",
            "loss: 0.329160  [142100/467875]\n",
            "loss: 0.459800  [145000/467875]\n",
            "loss: 0.509492  [147900/467875]\n",
            "loss: 0.523812  [150800/467875]\n",
            "loss: 0.537762  [153700/467875]\n",
            "loss: 0.387813  [156600/467875]\n",
            "loss: 0.373787  [159500/467875]\n",
            "loss: 0.696513  [162400/467875]\n",
            "loss: 0.459855  [165300/467875]\n",
            "loss: 0.532775  [168200/467875]\n",
            "loss: 0.732975  [171100/467875]\n",
            "loss: 0.404918  [174000/467875]\n",
            "loss: 0.505567  [176900/467875]\n",
            "loss: 0.463285  [179800/467875]\n",
            "loss: 0.449691  [182700/467875]\n",
            "loss: 0.413040  [185600/467875]\n",
            "loss: 0.526695  [188500/467875]\n",
            "loss: 0.494236  [191400/467875]\n",
            "loss: 0.527731  [194300/467875]\n",
            "loss: 0.546041  [197200/467875]\n",
            "loss: 0.568400  [200100/467875]\n",
            "loss: 0.413532  [203000/467875]\n",
            "loss: 0.454662  [205900/467875]\n",
            "loss: 0.558216  [208800/467875]\n",
            "loss: 0.323448  [211700/467875]\n",
            "loss: 0.380023  [214600/467875]\n",
            "loss: 0.428454  [217500/467875]\n",
            "loss: 0.331744  [220400/467875]\n",
            "loss: 0.422638  [223300/467875]\n",
            "loss: 0.501359  [226200/467875]\n",
            "loss: 0.347879  [229100/467875]\n",
            "loss: 0.390251  [232000/467875]\n",
            "loss: 0.401732  [234900/467875]\n",
            "loss: 0.692278  [237800/467875]\n",
            "loss: 0.565753  [240700/467875]\n",
            "loss: 0.476014  [243600/467875]\n",
            "loss: 0.378802  [246500/467875]\n",
            "loss: 0.546166  [249400/467875]\n",
            "loss: 0.508232  [252300/467875]\n",
            "loss: 0.522831  [255200/467875]\n",
            "loss: 0.454832  [258100/467875]\n",
            "loss: 0.502652  [261000/467875]\n",
            "loss: 0.320593  [263900/467875]\n",
            "loss: 0.533484  [266800/467875]\n",
            "loss: 0.572754  [269700/467875]\n",
            "loss: 0.593926  [272600/467875]\n",
            "loss: 0.604362  [275500/467875]\n",
            "loss: 0.488879  [278400/467875]\n",
            "loss: 0.437190  [281300/467875]\n",
            "loss: 0.622052  [284200/467875]\n",
            "loss: 0.278955  [287100/467875]\n",
            "loss: 0.537659  [290000/467875]\n",
            "loss: 0.556310  [292900/467875]\n",
            "loss: 0.529167  [295800/467875]\n",
            "loss: 0.383300  [298700/467875]\n",
            "loss: 0.512740  [301600/467875]\n",
            "loss: 0.452535  [304500/467875]\n",
            "loss: 0.333813  [307400/467875]\n",
            "loss: 0.474304  [310300/467875]\n",
            "loss: 0.461089  [313200/467875]\n",
            "loss: 0.646321  [316100/467875]\n",
            "loss: 0.469518  [319000/467875]\n",
            "loss: 0.668592  [321900/467875]\n",
            "loss: 0.469459  [324800/467875]\n",
            "loss: 0.567999  [327700/467875]\n",
            "loss: 0.285266  [330600/467875]\n",
            "loss: 0.500431  [333500/467875]\n",
            "loss: 0.384128  [336400/467875]\n",
            "loss: 0.401219  [339300/467875]\n",
            "loss: 0.501610  [342200/467875]\n",
            "loss: 0.330731  [345100/467875]\n",
            "loss: 0.380182  [348000/467875]\n",
            "loss: 0.569189  [350900/467875]\n",
            "loss: 0.413399  [353800/467875]\n",
            "loss: 0.416842  [356700/467875]\n",
            "loss: 0.617588  [359600/467875]\n",
            "loss: 0.317610  [362500/467875]\n",
            "loss: 0.553123  [365400/467875]\n",
            "loss: 0.447496  [368300/467875]\n",
            "loss: 0.618445  [371200/467875]\n",
            "loss: 0.464650  [374100/467875]\n",
            "loss: 0.346556  [377000/467875]\n",
            "loss: 0.467306  [379900/467875]\n",
            "loss: 0.437271  [382800/467875]\n",
            "loss: 0.391351  [385700/467875]\n",
            "loss: 0.370194  [388600/467875]\n",
            "loss: 0.402065  [391500/467875]\n",
            "loss: 0.479298  [394400/467875]\n",
            "loss: 0.423803  [397300/467875]\n",
            "loss: 0.442824  [400200/467875]\n",
            "loss: 0.416687  [403100/467875]\n",
            "loss: 0.512451  [406000/467875]\n",
            "loss: 0.358069  [408900/467875]\n",
            "loss: 0.456717  [411800/467875]\n",
            "loss: 0.575333  [414700/467875]\n",
            "loss: 0.497748  [417600/467875]\n",
            "loss: 0.435104  [420500/467875]\n",
            "loss: 0.507971  [423400/467875]\n",
            "loss: 0.338057  [426300/467875]\n",
            "loss: 0.333699  [429200/467875]\n",
            "loss: 0.548397  [432100/467875]\n",
            "loss: 0.535204  [435000/467875]\n",
            "loss: 0.666195  [437900/467875]\n",
            "loss: 0.498073  [440800/467875]\n",
            "loss: 0.526191  [443700/467875]\n",
            "loss: 0.380661  [446600/467875]\n",
            "loss: 0.528281  [449500/467875]\n",
            "loss: 0.471771  [452400/467875]\n",
            "loss: 0.530357  [455300/467875]\n",
            "loss: 0.571812  [458200/467875]\n",
            "loss: 0.383905  [461100/467875]\n",
            "loss: 0.420316  [464000/467875]\n",
            "loss: 0.323094  [466900/467875]\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.477300 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.539559  [    0/467875]\n",
            "loss: 0.501124  [ 2900/467875]\n",
            "loss: 0.504258  [ 5800/467875]\n",
            "loss: 0.437137  [ 8700/467875]\n",
            "loss: 0.400859  [11600/467875]\n",
            "loss: 0.356884  [14500/467875]\n",
            "loss: 0.358116  [17400/467875]\n",
            "loss: 0.529162  [20300/467875]\n",
            "loss: 0.497683  [23200/467875]\n",
            "loss: 0.432151  [26100/467875]\n",
            "loss: 0.533771  [29000/467875]\n",
            "loss: 0.608431  [31900/467875]\n",
            "loss: 0.500154  [34800/467875]\n",
            "loss: 0.488419  [37700/467875]\n",
            "loss: 0.227251  [40600/467875]\n",
            "loss: 0.426230  [43500/467875]\n",
            "loss: 0.905079  [46400/467875]\n",
            "loss: 0.404118  [49300/467875]\n",
            "loss: 0.485263  [52200/467875]\n",
            "loss: 0.553632  [55100/467875]\n",
            "loss: 0.364047  [58000/467875]\n",
            "loss: 0.576131  [60900/467875]\n",
            "loss: 0.402937  [63800/467875]\n",
            "loss: 0.532154  [66700/467875]\n",
            "loss: 0.466597  [69600/467875]\n",
            "loss: 0.411554  [72500/467875]\n",
            "loss: 0.376043  [75400/467875]\n",
            "loss: 0.377506  [78300/467875]\n",
            "loss: 0.481894  [81200/467875]\n",
            "loss: 0.487907  [84100/467875]\n",
            "loss: 0.408750  [87000/467875]\n",
            "loss: 0.534617  [89900/467875]\n",
            "loss: 0.364980  [92800/467875]\n",
            "loss: 0.329572  [95700/467875]\n",
            "loss: 0.380844  [98600/467875]\n",
            "loss: 0.551259  [101500/467875]\n",
            "loss: 0.487162  [104400/467875]\n",
            "loss: 0.389017  [107300/467875]\n",
            "loss: 0.538363  [110200/467875]\n",
            "loss: 0.415542  [113100/467875]\n",
            "loss: 0.623537  [116000/467875]\n",
            "loss: 0.298674  [118900/467875]\n",
            "loss: 0.484911  [121800/467875]\n",
            "loss: 0.393788  [124700/467875]\n",
            "loss: 0.525761  [127600/467875]\n",
            "loss: 0.375835  [130500/467875]\n",
            "loss: 0.667845  [133400/467875]\n",
            "loss: 0.578278  [136300/467875]\n",
            "loss: 0.556140  [139200/467875]\n",
            "loss: 0.498254  [142100/467875]\n",
            "loss: 0.491767  [145000/467875]\n",
            "loss: 0.501026  [147900/467875]\n",
            "loss: 0.582412  [150800/467875]\n",
            "loss: 0.490017  [153700/467875]\n",
            "loss: 0.346670  [156600/467875]\n",
            "loss: 0.403803  [159500/467875]\n",
            "loss: 0.401145  [162400/467875]\n",
            "loss: 0.524767  [165300/467875]\n",
            "loss: 0.491754  [168200/467875]\n",
            "loss: 0.593647  [171100/467875]\n",
            "loss: 0.599390  [174000/467875]\n",
            "loss: 0.385483  [176900/467875]\n",
            "loss: 0.336802  [179800/467875]\n",
            "loss: 0.672595  [182700/467875]\n",
            "loss: 0.492118  [185600/467875]\n",
            "loss: 0.455551  [188500/467875]\n",
            "loss: 0.591213  [191400/467875]\n",
            "loss: 0.527013  [194300/467875]\n",
            "loss: 0.394837  [197200/467875]\n",
            "loss: 0.553827  [200100/467875]\n",
            "loss: 0.449817  [203000/467875]\n",
            "loss: 0.372625  [205900/467875]\n",
            "loss: 0.559650  [208800/467875]\n",
            "loss: 0.777433  [211700/467875]\n",
            "loss: 0.275184  [214600/467875]\n",
            "loss: 0.426431  [217500/467875]\n",
            "loss: 0.417397  [220400/467875]\n",
            "loss: 0.455859  [223300/467875]\n",
            "loss: 0.620786  [226200/467875]\n",
            "loss: 0.422575  [229100/467875]\n",
            "loss: 0.405802  [232000/467875]\n",
            "loss: 0.419514  [234900/467875]\n",
            "loss: 0.629780  [237800/467875]\n",
            "loss: 0.450943  [240700/467875]\n",
            "loss: 0.615912  [243600/467875]\n",
            "loss: 0.321240  [246500/467875]\n",
            "loss: 0.424558  [249400/467875]\n",
            "loss: 0.395898  [252300/467875]\n",
            "loss: 0.526120  [255200/467875]\n",
            "loss: 0.442432  [258100/467875]\n",
            "loss: 0.481541  [261000/467875]\n",
            "loss: 0.429806  [263900/467875]\n",
            "loss: 0.454585  [266800/467875]\n",
            "loss: 0.465354  [269700/467875]\n",
            "loss: 0.482406  [272600/467875]\n",
            "loss: 0.425190  [275500/467875]\n",
            "loss: 0.339716  [278400/467875]\n",
            "loss: 0.573721  [281300/467875]\n",
            "loss: 0.402386  [284200/467875]\n",
            "loss: 0.349479  [287100/467875]\n",
            "loss: 0.594474  [290000/467875]\n",
            "loss: 0.418018  [292900/467875]\n",
            "loss: 0.434993  [295800/467875]\n",
            "loss: 0.483117  [298700/467875]\n",
            "loss: 0.406524  [301600/467875]\n",
            "loss: 0.482525  [304500/467875]\n",
            "loss: 0.542833  [307400/467875]\n",
            "loss: 0.458407  [310300/467875]\n",
            "loss: 0.392670  [313200/467875]\n",
            "loss: 0.383172  [316100/467875]\n",
            "loss: 0.228880  [319000/467875]\n",
            "loss: 0.459387  [321900/467875]\n",
            "loss: 0.484291  [324800/467875]\n",
            "loss: 0.434964  [327700/467875]\n",
            "loss: 0.496326  [330600/467875]\n",
            "loss: 0.599483  [333500/467875]\n",
            "loss: 0.472259  [336400/467875]\n",
            "loss: 0.462420  [339300/467875]\n",
            "loss: 0.562807  [342200/467875]\n",
            "loss: 0.485029  [345100/467875]\n",
            "loss: 0.367096  [348000/467875]\n",
            "loss: 0.369970  [350900/467875]\n",
            "loss: 0.428090  [353800/467875]\n",
            "loss: 0.457306  [356700/467875]\n",
            "loss: 0.300375  [359600/467875]\n",
            "loss: 0.458796  [362500/467875]\n",
            "loss: 0.499065  [365400/467875]\n",
            "loss: 0.474327  [368300/467875]\n",
            "loss: 0.532326  [371200/467875]\n",
            "loss: 0.499967  [374100/467875]\n",
            "loss: 0.412741  [377000/467875]\n",
            "loss: 0.425185  [379900/467875]\n",
            "loss: 0.527669  [382800/467875]\n",
            "loss: 0.405331  [385700/467875]\n",
            "loss: 0.586461  [388600/467875]\n",
            "loss: 0.449300  [391500/467875]\n",
            "loss: 0.334879  [394400/467875]\n",
            "loss: 0.482896  [397300/467875]\n",
            "loss: 0.395776  [400200/467875]\n",
            "loss: 0.525172  [403100/467875]\n",
            "loss: 0.431740  [406000/467875]\n",
            "loss: 0.427100  [408900/467875]\n",
            "loss: 0.386927  [411800/467875]\n",
            "loss: 0.569875  [414700/467875]\n",
            "loss: 0.506033  [417600/467875]\n",
            "loss: 0.488193  [420500/467875]\n",
            "loss: 0.476499  [423400/467875]\n",
            "loss: 0.519913  [426300/467875]\n",
            "loss: 0.449102  [429200/467875]\n",
            "loss: 0.351981  [432100/467875]\n",
            "loss: 0.479925  [435000/467875]\n",
            "loss: 0.434565  [437900/467875]\n",
            "loss: 0.400432  [440800/467875]\n",
            "loss: 0.381832  [443700/467875]\n",
            "loss: 0.498240  [446600/467875]\n",
            "loss: 0.377522  [449500/467875]\n",
            "loss: 0.403607  [452400/467875]\n",
            "loss: 0.434424  [455300/467875]\n",
            "loss: 0.563834  [458200/467875]\n",
            "loss: 0.525451  [461100/467875]\n",
            "loss: 0.455380  [464000/467875]\n",
            "loss: 0.317190  [466900/467875]\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476706 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 24 s\n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.532377  [    0/467875]\n",
            "loss: 0.472541  [ 2900/467875]\n",
            "loss: 0.637268  [ 5800/467875]\n",
            "loss: 0.422219  [ 8700/467875]\n",
            "loss: 0.439078  [11600/467875]\n",
            "loss: 0.331079  [14500/467875]\n",
            "loss: 0.624834  [17400/467875]\n",
            "loss: 0.548609  [20300/467875]\n",
            "loss: 0.589115  [23200/467875]\n",
            "loss: 0.570783  [26100/467875]\n",
            "loss: 0.439088  [29000/467875]\n",
            "loss: 0.341561  [31900/467875]\n",
            "loss: 0.258231  [34800/467875]\n",
            "loss: 0.295991  [37700/467875]\n",
            "loss: 0.554540  [40600/467875]\n",
            "loss: 0.422733  [43500/467875]\n",
            "loss: 0.421300  [46400/467875]\n",
            "loss: 0.647511  [49300/467875]\n",
            "loss: 0.423217  [52200/467875]\n",
            "loss: 0.521154  [55100/467875]\n",
            "loss: 0.486474  [58000/467875]\n",
            "loss: 0.377283  [60900/467875]\n",
            "loss: 0.670256  [63800/467875]\n",
            "loss: 0.599433  [66700/467875]\n",
            "loss: 0.247190  [69600/467875]\n",
            "loss: 0.509324  [72500/467875]\n",
            "loss: 0.529818  [75400/467875]\n",
            "loss: 0.474708  [78300/467875]\n",
            "loss: 0.421838  [81200/467875]\n",
            "loss: 0.370922  [84100/467875]\n",
            "loss: 0.344572  [87000/467875]\n",
            "loss: 0.441024  [89900/467875]\n",
            "loss: 0.493493  [92800/467875]\n",
            "loss: 0.355336  [95700/467875]\n",
            "loss: 0.450159  [98600/467875]\n",
            "loss: 0.588029  [101500/467875]\n",
            "loss: 0.368382  [104400/467875]\n",
            "loss: 0.535152  [107300/467875]\n",
            "loss: 0.477642  [110200/467875]\n",
            "loss: 0.480795  [113100/467875]\n",
            "loss: 0.480904  [116000/467875]\n",
            "loss: 0.560580  [118900/467875]\n",
            "loss: 0.600204  [121800/467875]\n",
            "loss: 0.394949  [124700/467875]\n",
            "loss: 0.566096  [127600/467875]\n",
            "loss: 0.541813  [130500/467875]\n",
            "loss: 0.336690  [133400/467875]\n",
            "loss: 0.424835  [136300/467875]\n",
            "loss: 0.423396  [139200/467875]\n",
            "loss: 0.338896  [142100/467875]\n",
            "loss: 0.470537  [145000/467875]\n",
            "loss: 0.513329  [147900/467875]\n",
            "loss: 0.564043  [150800/467875]\n",
            "loss: 0.582245  [153700/467875]\n",
            "loss: 0.412999  [156600/467875]\n",
            "loss: 0.452818  [159500/467875]\n",
            "loss: 0.777472  [162400/467875]\n",
            "loss: 0.507240  [165300/467875]\n",
            "loss: 0.378395  [168200/467875]\n",
            "loss: 0.589965  [171100/467875]\n",
            "loss: 0.446755  [174000/467875]\n",
            "loss: 0.317179  [176900/467875]\n",
            "loss: 0.444869  [179800/467875]\n",
            "loss: 0.398240  [182700/467875]\n",
            "loss: 0.577214  [185600/467875]\n",
            "loss: 0.515341  [188500/467875]\n",
            "loss: 0.547858  [191400/467875]\n",
            "loss: 0.498347  [194300/467875]\n",
            "loss: 0.349319  [197200/467875]\n",
            "loss: 0.571545  [200100/467875]\n",
            "loss: 0.349477  [203000/467875]\n",
            "loss: 0.378011  [205900/467875]\n",
            "loss: 0.410482  [208800/467875]\n",
            "loss: 0.466238  [211700/467875]\n",
            "loss: 0.456583  [214600/467875]\n",
            "loss: 0.580093  [217500/467875]\n",
            "loss: 0.376908  [220400/467875]\n",
            "loss: 0.461260  [223300/467875]\n",
            "loss: 0.362994  [226200/467875]\n",
            "loss: 0.469653  [229100/467875]\n",
            "loss: 0.384832  [232000/467875]\n",
            "loss: 0.592518  [234900/467875]\n",
            "loss: 0.462793  [237800/467875]\n",
            "loss: 0.541520  [240700/467875]\n",
            "loss: 0.330505  [243600/467875]\n",
            "loss: 0.389358  [246500/467875]\n",
            "loss: 0.400486  [249400/467875]\n",
            "loss: 0.506378  [252300/467875]\n",
            "loss: 0.616645  [255200/467875]\n",
            "loss: 0.535465  [258100/467875]\n",
            "loss: 0.356267  [261000/467875]\n",
            "loss: 0.516169  [263900/467875]\n",
            "loss: 0.464781  [266800/467875]\n",
            "loss: 0.462307  [269700/467875]\n",
            "loss: 0.387730  [272600/467875]\n",
            "loss: 0.340735  [275500/467875]\n",
            "loss: 0.465480  [278400/467875]\n",
            "loss: 0.406474  [281300/467875]\n",
            "loss: 0.424811  [284200/467875]\n",
            "loss: 0.455415  [287100/467875]\n",
            "loss: 0.566938  [290000/467875]\n",
            "loss: 0.397453  [292900/467875]\n",
            "loss: 0.704037  [295800/467875]\n",
            "loss: 0.421260  [298700/467875]\n",
            "loss: 0.318766  [301600/467875]\n",
            "loss: 0.310286  [304500/467875]\n",
            "loss: 0.612868  [307400/467875]\n",
            "loss: 0.467555  [310300/467875]\n",
            "loss: 0.619148  [313200/467875]\n",
            "loss: 0.367783  [316100/467875]\n",
            "loss: 0.304251  [319000/467875]\n",
            "loss: 0.337864  [321900/467875]\n",
            "loss: 0.378902  [324800/467875]\n",
            "loss: 0.349426  [327700/467875]\n",
            "loss: 0.341896  [330600/467875]\n",
            "loss: 0.417546  [333500/467875]\n",
            "loss: 0.407952  [336400/467875]\n",
            "loss: 0.422006  [339300/467875]\n",
            "loss: 0.453217  [342200/467875]\n",
            "loss: 0.314814  [345100/467875]\n",
            "loss: 0.594607  [348000/467875]\n",
            "loss: 0.465964  [350900/467875]\n",
            "loss: 0.407030  [353800/467875]\n",
            "loss: 0.393778  [356700/467875]\n",
            "loss: 0.521386  [359600/467875]\n",
            "loss: 0.399995  [362500/467875]\n",
            "loss: 0.463724  [365400/467875]\n",
            "loss: 0.253466  [368300/467875]\n",
            "loss: 0.539525  [371200/467875]\n",
            "loss: 0.422119  [374100/467875]\n",
            "loss: 0.280737  [377000/467875]\n",
            "loss: 0.384411  [379900/467875]\n",
            "loss: 0.436870  [382800/467875]\n",
            "loss: 0.401311  [385700/467875]\n",
            "loss: 0.587702  [388600/467875]\n",
            "loss: 0.473924  [391500/467875]\n",
            "loss: 0.541551  [394400/467875]\n",
            "loss: 0.462482  [397300/467875]\n",
            "loss: 0.442095  [400200/467875]\n",
            "loss: 0.528676  [403100/467875]\n",
            "loss: 0.391013  [406000/467875]\n",
            "loss: 0.310122  [408900/467875]\n",
            "loss: 0.253476  [411800/467875]\n",
            "loss: 0.287306  [414700/467875]\n",
            "loss: 0.348904  [417600/467875]\n",
            "loss: 0.446485  [420500/467875]\n",
            "loss: 0.539766  [423400/467875]\n",
            "loss: 0.360222  [426300/467875]\n",
            "loss: 0.572617  [429200/467875]\n",
            "loss: 0.358681  [432100/467875]\n",
            "loss: 0.637086  [435000/467875]\n",
            "loss: 0.310217  [437900/467875]\n",
            "loss: 0.756541  [440800/467875]\n",
            "loss: 0.316480  [443700/467875]\n",
            "loss: 0.568667  [446600/467875]\n",
            "loss: 0.445509  [449500/467875]\n",
            "loss: 0.610355  [452400/467875]\n",
            "loss: 0.552515  [455300/467875]\n",
            "loss: 0.362553  [458200/467875]\n",
            "loss: 0.371381  [461100/467875]\n",
            "loss: 0.483514  [464000/467875]\n",
            "loss: 0.338655  [466900/467875]\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.0%, Avg loss: 0.481252 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 24 s\n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.445240  [    0/467875]\n",
            "loss: 0.426511  [ 2900/467875]\n",
            "loss: 0.360131  [ 5800/467875]\n",
            "loss: 0.429956  [ 8700/467875]\n",
            "loss: 0.468528  [11600/467875]\n",
            "loss: 0.372543  [14500/467875]\n",
            "loss: 0.636572  [17400/467875]\n",
            "loss: 0.518609  [20300/467875]\n",
            "loss: 0.583669  [23200/467875]\n",
            "loss: 0.392768  [26100/467875]\n",
            "loss: 0.479856  [29000/467875]\n",
            "loss: 0.398168  [31900/467875]\n",
            "loss: 0.425977  [34800/467875]\n",
            "loss: 0.606278  [37700/467875]\n",
            "loss: 0.479466  [40600/467875]\n",
            "loss: 0.327140  [43500/467875]\n",
            "loss: 0.361485  [46400/467875]\n",
            "loss: 0.425629  [49300/467875]\n",
            "loss: 0.442737  [52200/467875]\n",
            "loss: 0.564294  [55100/467875]\n",
            "loss: 0.389910  [58000/467875]\n",
            "loss: 0.527960  [60900/467875]\n",
            "loss: 0.262422  [63800/467875]\n",
            "loss: 0.323514  [66700/467875]\n",
            "loss: 0.505073  [69600/467875]\n",
            "loss: 0.380420  [72500/467875]\n",
            "loss: 0.381816  [75400/467875]\n",
            "loss: 0.431859  [78300/467875]\n",
            "loss: 0.433133  [81200/467875]\n",
            "loss: 0.412708  [84100/467875]\n",
            "loss: 0.296845  [87000/467875]\n",
            "loss: 0.439123  [89900/467875]\n",
            "loss: 0.365935  [92800/467875]\n",
            "loss: 0.293574  [95700/467875]\n",
            "loss: 0.534741  [98600/467875]\n",
            "loss: 0.328132  [101500/467875]\n",
            "loss: 0.401136  [104400/467875]\n",
            "loss: 0.466913  [107300/467875]\n",
            "loss: 0.331867  [110200/467875]\n",
            "loss: 0.533931  [113100/467875]\n",
            "loss: 0.598968  [116000/467875]\n",
            "loss: 0.465852  [118900/467875]\n",
            "loss: 0.335016  [121800/467875]\n",
            "loss: 0.398112  [124700/467875]\n",
            "loss: 0.425822  [127600/467875]\n",
            "loss: 0.493305  [130500/467875]\n",
            "loss: 0.491885  [133400/467875]\n",
            "loss: 0.444107  [136300/467875]\n",
            "loss: 0.627088  [139200/467875]\n",
            "loss: 0.555157  [142100/467875]\n",
            "loss: 0.350018  [145000/467875]\n",
            "loss: 0.434326  [147900/467875]\n",
            "loss: 0.352159  [150800/467875]\n",
            "loss: 0.632477  [153700/467875]\n",
            "loss: 0.529019  [156600/467875]\n",
            "loss: 0.453512  [159500/467875]\n",
            "loss: 0.316017  [162400/467875]\n",
            "loss: 0.347701  [165300/467875]\n",
            "loss: 0.391316  [168200/467875]\n",
            "loss: 0.620533  [171100/467875]\n",
            "loss: 0.508482  [174000/467875]\n",
            "loss: 0.466302  [176900/467875]\n",
            "loss: 0.501903  [179800/467875]\n",
            "loss: 0.368152  [182700/467875]\n",
            "loss: 0.364314  [185600/467875]\n",
            "loss: 0.639998  [188500/467875]\n",
            "loss: 0.346755  [191400/467875]\n",
            "loss: 0.436781  [194300/467875]\n",
            "loss: 0.411348  [197200/467875]\n",
            "loss: 0.618137  [200100/467875]\n",
            "loss: 0.407631  [203000/467875]\n",
            "loss: 0.362870  [205900/467875]\n",
            "loss: 0.503533  [208800/467875]\n",
            "loss: 0.355245  [211700/467875]\n",
            "loss: 0.309231  [214600/467875]\n",
            "loss: 0.447362  [217500/467875]\n",
            "loss: 0.560562  [220400/467875]\n",
            "loss: 0.374042  [223300/467875]\n",
            "loss: 0.447256  [226200/467875]\n",
            "loss: 0.290374  [229100/467875]\n",
            "loss: 0.437770  [232000/467875]\n",
            "loss: 0.353515  [234900/467875]\n",
            "loss: 0.542745  [237800/467875]\n",
            "loss: 0.359855  [240700/467875]\n",
            "loss: 0.496790  [243600/467875]\n",
            "loss: 0.394762  [246500/467875]\n",
            "loss: 0.487366  [249400/467875]\n",
            "loss: 0.580245  [252300/467875]\n",
            "loss: 0.394522  [255200/467875]\n",
            "loss: 0.532768  [258100/467875]\n",
            "loss: 0.510246  [261000/467875]\n",
            "loss: 0.503802  [263900/467875]\n",
            "loss: 0.533400  [266800/467875]\n",
            "loss: 0.548439  [269700/467875]\n",
            "loss: 0.331154  [272600/467875]\n",
            "loss: 0.360219  [275500/467875]\n",
            "loss: 0.457217  [278400/467875]\n",
            "loss: 0.492641  [281300/467875]\n",
            "loss: 0.293381  [284200/467875]\n",
            "loss: 0.542161  [287100/467875]\n",
            "loss: 0.353102  [290000/467875]\n",
            "loss: 0.462402  [292900/467875]\n",
            "loss: 0.339766  [295800/467875]\n",
            "loss: 0.626157  [298700/467875]\n",
            "loss: 0.717928  [301600/467875]\n",
            "loss: 0.458628  [304500/467875]\n",
            "loss: 0.424333  [307400/467875]\n",
            "loss: 0.287338  [310300/467875]\n",
            "loss: 0.335137  [313200/467875]\n",
            "loss: 0.461404  [316100/467875]\n",
            "loss: 0.532021  [319000/467875]\n",
            "loss: 0.379085  [321900/467875]\n",
            "loss: 0.297022  [324800/467875]\n",
            "loss: 0.454128  [327700/467875]\n",
            "loss: 0.265699  [330600/467875]\n",
            "loss: 0.377300  [333500/467875]\n",
            "loss: 0.652381  [336400/467875]\n",
            "loss: 0.386667  [339300/467875]\n",
            "loss: 0.415208  [342200/467875]\n",
            "loss: 0.456999  [345100/467875]\n",
            "loss: 0.539730  [348000/467875]\n",
            "loss: 0.405343  [350900/467875]\n",
            "loss: 0.668856  [353800/467875]\n",
            "loss: 0.577152  [356700/467875]\n",
            "loss: 0.393620  [359600/467875]\n",
            "loss: 0.363233  [362500/467875]\n",
            "loss: 0.485006  [365400/467875]\n",
            "loss: 0.474774  [368300/467875]\n",
            "loss: 0.456415  [371200/467875]\n",
            "loss: 0.488924  [374100/467875]\n",
            "loss: 0.693605  [377000/467875]\n",
            "loss: 0.471343  [379900/467875]\n",
            "loss: 0.372612  [382800/467875]\n",
            "loss: 0.411098  [385700/467875]\n",
            "loss: 0.357782  [388600/467875]\n",
            "loss: 0.513230  [391500/467875]\n",
            "loss: 0.463954  [394400/467875]\n",
            "loss: 0.457375  [397300/467875]\n",
            "loss: 0.527449  [400200/467875]\n",
            "loss: 0.535593  [403100/467875]\n",
            "loss: 0.452344  [406000/467875]\n",
            "loss: 0.508763  [408900/467875]\n",
            "loss: 0.551662  [411800/467875]\n",
            "loss: 0.584894  [414700/467875]\n",
            "loss: 0.492516  [417600/467875]\n",
            "loss: 0.425396  [420500/467875]\n",
            "loss: 0.603679  [423400/467875]\n",
            "loss: 0.359424  [426300/467875]\n",
            "loss: 0.618359  [429200/467875]\n",
            "loss: 0.396297  [432100/467875]\n",
            "loss: 0.595434  [435000/467875]\n",
            "loss: 0.384209  [437900/467875]\n",
            "loss: 0.469752  [440800/467875]\n",
            "loss: 0.470048  [443700/467875]\n",
            "loss: 0.318169  [446600/467875]\n",
            "loss: 0.451565  [449500/467875]\n",
            "loss: 0.404917  [452400/467875]\n",
            "loss: 0.453470  [455300/467875]\n",
            "loss: 0.489723  [458200/467875]\n",
            "loss: 0.530787  [461100/467875]\n",
            "loss: 0.600110  [464000/467875]\n",
            "loss: 0.363892  [466900/467875]\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.478755 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 24 s\n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.609246  [    0/467875]\n",
            "loss: 0.565394  [ 2900/467875]\n",
            "loss: 0.556397  [ 5800/467875]\n",
            "loss: 0.276389  [ 8700/467875]\n",
            "loss: 0.324320  [11600/467875]\n",
            "loss: 0.575922  [14500/467875]\n",
            "loss: 0.659646  [17400/467875]\n",
            "loss: 0.531492  [20300/467875]\n",
            "loss: 0.497705  [23200/467875]\n",
            "loss: 0.639535  [26100/467875]\n",
            "loss: 0.376901  [29000/467875]\n",
            "loss: 0.497457  [31900/467875]\n",
            "loss: 0.360862  [34800/467875]\n",
            "loss: 0.523797  [37700/467875]\n",
            "loss: 0.537237  [40600/467875]\n",
            "loss: 0.407271  [43500/467875]\n",
            "loss: 0.596060  [46400/467875]\n",
            "loss: 0.425404  [49300/467875]\n",
            "loss: 0.393422  [52200/467875]\n",
            "loss: 0.465385  [55100/467875]\n",
            "loss: 0.378274  [58000/467875]\n",
            "loss: 0.675221  [60900/467875]\n",
            "loss: 0.384879  [63800/467875]\n",
            "loss: 0.617103  [66700/467875]\n",
            "loss: 0.421632  [69600/467875]\n",
            "loss: 0.461968  [72500/467875]\n",
            "loss: 0.630091  [75400/467875]\n",
            "loss: 0.549496  [78300/467875]\n",
            "loss: 0.312329  [81200/467875]\n",
            "loss: 0.382936  [84100/467875]\n",
            "loss: 0.480092  [87000/467875]\n",
            "loss: 0.495899  [89900/467875]\n",
            "loss: 0.550895  [92800/467875]\n",
            "loss: 0.662318  [95700/467875]\n",
            "loss: 0.522537  [98600/467875]\n",
            "loss: 0.422869  [101500/467875]\n",
            "loss: 0.368921  [104400/467875]\n",
            "loss: 0.456876  [107300/467875]\n",
            "loss: 0.357042  [110200/467875]\n",
            "loss: 0.669870  [113100/467875]\n",
            "loss: 0.512662  [116000/467875]\n",
            "loss: 0.460035  [118900/467875]\n",
            "loss: 0.343189  [121800/467875]\n",
            "loss: 0.405938  [124700/467875]\n",
            "loss: 0.482683  [127600/467875]\n",
            "loss: 0.599207  [130500/467875]\n",
            "loss: 0.584360  [133400/467875]\n",
            "loss: 0.442899  [136300/467875]\n",
            "loss: 0.425492  [139200/467875]\n",
            "loss: 0.513191  [142100/467875]\n",
            "loss: 0.554516  [145000/467875]\n",
            "loss: 0.515629  [147900/467875]\n",
            "loss: 0.462852  [150800/467875]\n",
            "loss: 0.311084  [153700/467875]\n",
            "loss: 0.441069  [156600/467875]\n",
            "loss: 0.559658  [159500/467875]\n",
            "loss: 0.525137  [162400/467875]\n",
            "loss: 0.563987  [165300/467875]\n",
            "loss: 0.337915  [168200/467875]\n",
            "loss: 0.511442  [171100/467875]\n",
            "loss: 0.456781  [174000/467875]\n",
            "loss: 0.311816  [176900/467875]\n",
            "loss: 0.376828  [179800/467875]\n",
            "loss: 0.584544  [182700/467875]\n",
            "loss: 0.499884  [185600/467875]\n",
            "loss: 0.429788  [188500/467875]\n",
            "loss: 0.470150  [191400/467875]\n",
            "loss: 0.555797  [194300/467875]\n",
            "loss: 0.430420  [197200/467875]\n",
            "loss: 0.551930  [200100/467875]\n",
            "loss: 0.521321  [203000/467875]\n",
            "loss: 0.504122  [205900/467875]\n",
            "loss: 0.466313  [208800/467875]\n",
            "loss: 0.616968  [211700/467875]\n",
            "loss: 0.358653  [214600/467875]\n",
            "loss: 0.511564  [217500/467875]\n",
            "loss: 0.611580  [220400/467875]\n",
            "loss: 0.772114  [223300/467875]\n",
            "loss: 0.619127  [226200/467875]\n",
            "loss: 0.286746  [229100/467875]\n",
            "loss: 0.272012  [232000/467875]\n",
            "loss: 0.488887  [234900/467875]\n",
            "loss: 0.523253  [237800/467875]\n",
            "loss: 0.446771  [240700/467875]\n",
            "loss: 0.594885  [243600/467875]\n",
            "loss: 0.400368  [246500/467875]\n",
            "loss: 0.469094  [249400/467875]\n",
            "loss: 0.485140  [252300/467875]\n",
            "loss: 0.402440  [255200/467875]\n",
            "loss: 0.462752  [258100/467875]\n",
            "loss: 0.464852  [261000/467875]\n",
            "loss: 0.326494  [263900/467875]\n",
            "loss: 0.423392  [266800/467875]\n",
            "loss: 0.365699  [269700/467875]\n",
            "loss: 0.316262  [272600/467875]\n",
            "loss: 0.597145  [275500/467875]\n",
            "loss: 0.505665  [278400/467875]\n",
            "loss: 0.333488  [281300/467875]\n",
            "loss: 0.584147  [284200/467875]\n",
            "loss: 0.346866  [287100/467875]\n",
            "loss: 0.482157  [290000/467875]\n",
            "loss: 0.381106  [292900/467875]\n",
            "loss: 0.257658  [295800/467875]\n",
            "loss: 0.627903  [298700/467875]\n",
            "loss: 0.333720  [301600/467875]\n",
            "loss: 0.355328  [304500/467875]\n",
            "loss: 0.483855  [307400/467875]\n",
            "loss: 0.556944  [310300/467875]\n",
            "loss: 0.668912  [313200/467875]\n",
            "loss: 0.457336  [316100/467875]\n",
            "loss: 0.421045  [319000/467875]\n",
            "loss: 0.326541  [321900/467875]\n",
            "loss: 0.367010  [324800/467875]\n",
            "loss: 0.469350  [327700/467875]\n",
            "loss: 0.477362  [330600/467875]\n",
            "loss: 0.495455  [333500/467875]\n",
            "loss: 0.369322  [336400/467875]\n",
            "loss: 0.500084  [339300/467875]\n",
            "loss: 0.623735  [342200/467875]\n",
            "loss: 0.564207  [345100/467875]\n",
            "loss: 0.419262  [348000/467875]\n",
            "loss: 0.361307  [350900/467875]\n",
            "loss: 0.431237  [353800/467875]\n",
            "loss: 0.542822  [356700/467875]\n",
            "loss: 0.504645  [359600/467875]\n",
            "loss: 0.862975  [362500/467875]\n",
            "loss: 0.365516  [365400/467875]\n",
            "loss: 0.498228  [368300/467875]\n",
            "loss: 0.442518  [371200/467875]\n",
            "loss: 0.529685  [374100/467875]\n",
            "loss: 0.553865  [377000/467875]\n",
            "loss: 0.493921  [379900/467875]\n",
            "loss: 0.515277  [382800/467875]\n",
            "loss: 0.331877  [385700/467875]\n",
            "loss: 0.472942  [388600/467875]\n",
            "loss: 0.684093  [391500/467875]\n",
            "loss: 0.556482  [394400/467875]\n",
            "loss: 0.466791  [397300/467875]\n",
            "loss: 0.446644  [400200/467875]\n",
            "loss: 0.394945  [403100/467875]\n",
            "loss: 0.466774  [406000/467875]\n",
            "loss: 0.422332  [408900/467875]\n",
            "loss: 0.508809  [411800/467875]\n",
            "loss: 0.474327  [414700/467875]\n",
            "loss: 0.533799  [417600/467875]\n",
            "loss: 0.395381  [420500/467875]\n",
            "loss: 0.335750  [423400/467875]\n",
            "loss: 0.453676  [426300/467875]\n",
            "loss: 0.379349  [429200/467875]\n",
            "loss: 0.648565  [432100/467875]\n",
            "loss: 0.362423  [435000/467875]\n",
            "loss: 0.534082  [437900/467875]\n",
            "loss: 0.616085  [440800/467875]\n",
            "loss: 0.513848  [443700/467875]\n",
            "loss: 0.439196  [446600/467875]\n",
            "loss: 0.374766  [449500/467875]\n",
            "loss: 0.523856  [452400/467875]\n",
            "loss: 0.377730  [455300/467875]\n",
            "loss: 0.471185  [458200/467875]\n",
            "loss: 0.546347  [461100/467875]\n",
            "loss: 0.503562  [464000/467875]\n",
            "loss: 0.555837  [466900/467875]\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.476743 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 24 s\n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.729185  [    0/467875]\n",
            "loss: 0.555966  [ 2900/467875]\n",
            "loss: 0.466541  [ 5800/467875]\n",
            "loss: 0.505764  [ 8700/467875]\n",
            "loss: 0.465743  [11600/467875]\n",
            "loss: 0.412203  [14500/467875]\n",
            "loss: 0.384785  [17400/467875]\n",
            "loss: 0.459804  [20300/467875]\n",
            "loss: 0.472028  [23200/467875]\n",
            "loss: 0.517047  [26100/467875]\n",
            "loss: 0.666668  [29000/467875]\n",
            "loss: 0.639705  [31900/467875]\n",
            "loss: 0.641029  [34800/467875]\n",
            "loss: 0.636358  [37700/467875]\n",
            "loss: 0.377443  [40600/467875]\n",
            "loss: 0.446201  [43500/467875]\n",
            "loss: 0.313901  [46400/467875]\n",
            "loss: 0.464953  [49300/467875]\n",
            "loss: 0.277564  [52200/467875]\n",
            "loss: 0.523430  [55100/467875]\n",
            "loss: 0.374014  [58000/467875]\n",
            "loss: 0.465763  [60900/467875]\n",
            "loss: 0.513510  [63800/467875]\n",
            "loss: 0.484042  [66700/467875]\n",
            "loss: 0.641053  [69600/467875]\n",
            "loss: 0.491391  [72500/467875]\n",
            "loss: 0.478472  [75400/467875]\n",
            "loss: 0.378826  [78300/467875]\n",
            "loss: 0.499944  [81200/467875]\n",
            "loss: 0.458316  [84100/467875]\n",
            "loss: 0.344717  [87000/467875]\n",
            "loss: 0.516760  [89900/467875]\n",
            "loss: 0.262254  [92800/467875]\n",
            "loss: 0.411526  [95700/467875]\n",
            "loss: 0.437301  [98600/467875]\n",
            "loss: 0.345207  [101500/467875]\n",
            "loss: 0.274813  [104400/467875]\n",
            "loss: 0.402678  [107300/467875]\n",
            "loss: 0.450298  [110200/467875]\n",
            "loss: 0.588706  [113100/467875]\n",
            "loss: 0.461250  [116000/467875]\n",
            "loss: 0.498417  [118900/467875]\n",
            "loss: 0.351109  [121800/467875]\n",
            "loss: 0.463902  [124700/467875]\n",
            "loss: 0.471146  [127600/467875]\n",
            "loss: 0.446044  [130500/467875]\n",
            "loss: 0.620488  [133400/467875]\n",
            "loss: 0.559410  [136300/467875]\n",
            "loss: 0.344026  [139200/467875]\n",
            "loss: 0.378255  [142100/467875]\n",
            "loss: 0.549019  [145000/467875]\n",
            "loss: 0.408252  [147900/467875]\n",
            "loss: 0.536904  [150800/467875]\n",
            "loss: 0.571233  [153700/467875]\n",
            "loss: 0.299853  [156600/467875]\n",
            "loss: 0.444998  [159500/467875]\n",
            "loss: 0.313739  [162400/467875]\n",
            "loss: 0.558772  [165300/467875]\n",
            "loss: 0.516991  [168200/467875]\n",
            "loss: 0.474904  [171100/467875]\n",
            "loss: 0.388508  [174000/467875]\n",
            "loss: 0.501989  [176900/467875]\n",
            "loss: 0.675868  [179800/467875]\n",
            "loss: 0.412971  [182700/467875]\n",
            "loss: 0.467366  [185600/467875]\n",
            "loss: 0.469767  [188500/467875]\n",
            "loss: 0.521697  [191400/467875]\n",
            "loss: 0.635885  [194300/467875]\n",
            "loss: 0.412228  [197200/467875]\n",
            "loss: 0.521945  [200100/467875]\n",
            "loss: 0.432003  [203000/467875]\n",
            "loss: 0.661428  [205900/467875]\n",
            "loss: 0.337296  [208800/467875]\n",
            "loss: 0.265890  [211700/467875]\n",
            "loss: 0.374540  [214600/467875]\n",
            "loss: 0.374789  [217500/467875]\n",
            "loss: 0.399896  [220400/467875]\n",
            "loss: 0.388900  [223300/467875]\n",
            "loss: 0.449552  [226200/467875]\n",
            "loss: 0.448828  [229100/467875]\n",
            "loss: 0.573510  [232000/467875]\n",
            "loss: 0.429738  [234900/467875]\n",
            "loss: 0.517095  [237800/467875]\n",
            "loss: 0.343902  [240700/467875]\n",
            "loss: 0.397185  [243600/467875]\n",
            "loss: 0.446658  [246500/467875]\n",
            "loss: 0.323511  [249400/467875]\n",
            "loss: 0.328132  [252300/467875]\n",
            "loss: 0.395393  [255200/467875]\n",
            "loss: 0.452465  [258100/467875]\n",
            "loss: 0.293697  [261000/467875]\n",
            "loss: 0.652242  [263900/467875]\n",
            "loss: 0.477901  [266800/467875]\n",
            "loss: 0.501363  [269700/467875]\n",
            "loss: 0.600226  [272600/467875]\n",
            "loss: 0.470651  [275500/467875]\n",
            "loss: 0.608037  [278400/467875]\n",
            "loss: 0.396843  [281300/467875]\n",
            "loss: 0.382806  [284200/467875]\n",
            "loss: 0.508449  [287100/467875]\n",
            "loss: 0.525035  [290000/467875]\n",
            "loss: 0.508288  [292900/467875]\n",
            "loss: 0.284672  [295800/467875]\n",
            "loss: 0.622137  [298700/467875]\n",
            "loss: 0.448567  [301600/467875]\n",
            "loss: 0.458351  [304500/467875]\n",
            "loss: 0.362148  [307400/467875]\n",
            "loss: 0.387411  [310300/467875]\n",
            "loss: 0.367658  [313200/467875]\n",
            "loss: 0.517951  [316100/467875]\n",
            "loss: 0.439866  [319000/467875]\n",
            "loss: 0.563065  [321900/467875]\n",
            "loss: 0.571468  [324800/467875]\n",
            "loss: 0.494324  [327700/467875]\n",
            "loss: 0.371403  [330600/467875]\n",
            "loss: 0.452226  [333500/467875]\n",
            "loss: 0.400861  [336400/467875]\n",
            "loss: 0.402186  [339300/467875]\n",
            "loss: 0.512755  [342200/467875]\n",
            "loss: 0.433153  [345100/467875]\n",
            "loss: 0.521352  [348000/467875]\n",
            "loss: 0.407043  [350900/467875]\n",
            "loss: 0.550144  [353800/467875]\n",
            "loss: 0.444274  [356700/467875]\n",
            "loss: 0.447628  [359600/467875]\n",
            "loss: 0.491580  [362500/467875]\n",
            "loss: 0.367061  [365400/467875]\n",
            "loss: 0.441353  [368300/467875]\n",
            "loss: 0.394929  [371200/467875]\n",
            "loss: 0.507475  [374100/467875]\n",
            "loss: 0.298236  [377000/467875]\n",
            "loss: 0.470748  [379900/467875]\n",
            "loss: 0.544538  [382800/467875]\n",
            "loss: 0.623932  [385700/467875]\n",
            "loss: 0.579207  [388600/467875]\n",
            "loss: 0.416367  [391500/467875]\n",
            "loss: 0.334263  [394400/467875]\n",
            "loss: 0.470998  [397300/467875]\n",
            "loss: 0.336994  [400200/467875]\n",
            "loss: 0.594144  [403100/467875]\n",
            "loss: 0.456013  [406000/467875]\n",
            "loss: 0.561420  [408900/467875]\n",
            "loss: 0.460671  [411800/467875]\n",
            "loss: 0.533767  [414700/467875]\n",
            "loss: 0.427680  [417600/467875]\n",
            "loss: 0.404407  [420500/467875]\n",
            "loss: 0.513376  [423400/467875]\n",
            "loss: 0.452238  [426300/467875]\n",
            "loss: 0.468708  [429200/467875]\n",
            "loss: 0.383041  [432100/467875]\n",
            "loss: 0.473460  [435000/467875]\n",
            "loss: 0.563241  [437900/467875]\n",
            "loss: 0.466095  [440800/467875]\n",
            "loss: 0.480458  [443700/467875]\n",
            "loss: 0.728392  [446600/467875]\n",
            "loss: 0.511018  [449500/467875]\n",
            "loss: 0.635355  [452400/467875]\n",
            "loss: 0.542683  [455300/467875]\n",
            "loss: 0.368864  [458200/467875]\n",
            "loss: 0.388292  [461100/467875]\n",
            "loss: 0.443958  [464000/467875]\n",
            "loss: 0.479008  [466900/467875]\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.477596 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.365122  [    0/467875]\n",
            "loss: 0.474592  [ 2900/467875]\n",
            "loss: 0.369490  [ 5800/467875]\n",
            "loss: 0.514393  [ 8700/467875]\n",
            "loss: 0.557623  [11600/467875]\n",
            "loss: 0.558611  [14500/467875]\n",
            "loss: 0.447144  [17400/467875]\n",
            "loss: 0.602583  [20300/467875]\n",
            "loss: 0.432287  [23200/467875]\n",
            "loss: 0.545924  [26100/467875]\n",
            "loss: 0.355427  [29000/467875]\n",
            "loss: 0.468639  [31900/467875]\n",
            "loss: 0.498838  [34800/467875]\n",
            "loss: 0.454921  [37700/467875]\n",
            "loss: 0.446413  [40600/467875]\n",
            "loss: 0.331444  [43500/467875]\n",
            "loss: 0.551429  [46400/467875]\n",
            "loss: 0.561370  [49300/467875]\n",
            "loss: 0.431836  [52200/467875]\n",
            "loss: 0.377754  [55100/467875]\n",
            "loss: 0.728926  [58000/467875]\n",
            "loss: 0.484696  [60900/467875]\n",
            "loss: 0.299175  [63800/467875]\n",
            "loss: 0.551325  [66700/467875]\n",
            "loss: 0.464229  [69600/467875]\n",
            "loss: 0.693548  [72500/467875]\n",
            "loss: 0.610116  [75400/467875]\n",
            "loss: 0.569601  [78300/467875]\n",
            "loss: 0.340573  [81200/467875]\n",
            "loss: 0.401120  [84100/467875]\n",
            "loss: 0.352588  [87000/467875]\n",
            "loss: 0.596730  [89900/467875]\n",
            "loss: 0.693680  [92800/467875]\n",
            "loss: 0.535243  [95700/467875]\n",
            "loss: 0.529266  [98600/467875]\n",
            "loss: 0.376611  [101500/467875]\n",
            "loss: 0.587037  [104400/467875]\n",
            "loss: 0.470932  [107300/467875]\n",
            "loss: 0.407083  [110200/467875]\n",
            "loss: 0.468125  [113100/467875]\n",
            "loss: 0.301694  [116000/467875]\n",
            "loss: 0.430292  [118900/467875]\n",
            "loss: 0.418019  [121800/467875]\n",
            "loss: 0.503829  [124700/467875]\n",
            "loss: 0.461759  [127600/467875]\n",
            "loss: 0.575931  [130500/467875]\n",
            "loss: 0.492106  [133400/467875]\n",
            "loss: 0.393443  [136300/467875]\n",
            "loss: 0.590299  [139200/467875]\n",
            "loss: 0.546413  [142100/467875]\n",
            "loss: 0.515533  [145000/467875]\n",
            "loss: 0.671685  [147900/467875]\n",
            "loss: 0.569681  [150800/467875]\n",
            "loss: 0.494884  [153700/467875]\n",
            "loss: 0.376635  [156600/467875]\n",
            "loss: 0.400249  [159500/467875]\n",
            "loss: 0.406294  [162400/467875]\n",
            "loss: 0.476700  [165300/467875]\n",
            "loss: 0.358086  [168200/467875]\n",
            "loss: 0.565757  [171100/467875]\n",
            "loss: 0.487725  [174000/467875]\n",
            "loss: 0.469034  [176900/467875]\n",
            "loss: 0.569938  [179800/467875]\n",
            "loss: 0.660207  [182700/467875]\n",
            "loss: 0.347902  [185600/467875]\n",
            "loss: 0.360507  [188500/467875]\n",
            "loss: 0.562612  [191400/467875]\n",
            "loss: 0.468971  [194300/467875]\n",
            "loss: 0.468430  [197200/467875]\n",
            "loss: 0.452311  [200100/467875]\n",
            "loss: 0.367458  [203000/467875]\n",
            "loss: 0.415242  [205900/467875]\n",
            "loss: 0.467037  [208800/467875]\n",
            "loss: 0.368880  [211700/467875]\n",
            "loss: 0.459527  [214600/467875]\n",
            "loss: 0.488876  [217500/467875]\n",
            "loss: 0.522303  [220400/467875]\n",
            "loss: 0.309256  [223300/467875]\n",
            "loss: 0.396030  [226200/467875]\n",
            "loss: 0.480390  [229100/467875]\n",
            "loss: 0.685571  [232000/467875]\n",
            "loss: 0.404792  [234900/467875]\n",
            "loss: 0.473091  [237800/467875]\n",
            "loss: 0.527323  [240700/467875]\n",
            "loss: 0.527307  [243600/467875]\n",
            "loss: 0.360261  [246500/467875]\n",
            "loss: 0.525755  [249400/467875]\n",
            "loss: 0.390742  [252300/467875]\n",
            "loss: 0.611626  [255200/467875]\n",
            "loss: 0.436150  [258100/467875]\n",
            "loss: 0.447719  [261000/467875]\n",
            "loss: 0.731409  [263900/467875]\n",
            "loss: 0.564285  [266800/467875]\n",
            "loss: 0.387730  [269700/467875]\n",
            "loss: 0.370712  [272600/467875]\n",
            "loss: 0.484366  [275500/467875]\n",
            "loss: 0.372968  [278400/467875]\n",
            "loss: 0.373335  [281300/467875]\n",
            "loss: 0.378551  [284200/467875]\n",
            "loss: 0.510163  [287100/467875]\n",
            "loss: 0.297046  [290000/467875]\n",
            "loss: 0.446697  [292900/467875]\n",
            "loss: 0.414188  [295800/467875]\n",
            "loss: 0.494229  [298700/467875]\n",
            "loss: 0.397712  [301600/467875]\n",
            "loss: 0.349849  [304500/467875]\n",
            "loss: 0.417762  [307400/467875]\n",
            "loss: 0.348606  [310300/467875]\n",
            "loss: 0.417589  [313200/467875]\n",
            "loss: 0.406254  [316100/467875]\n",
            "loss: 0.462947  [319000/467875]\n",
            "loss: 0.489582  [321900/467875]\n",
            "loss: 0.540963  [324800/467875]\n",
            "loss: 0.522804  [327700/467875]\n",
            "loss: 0.444641  [330600/467875]\n",
            "loss: 0.320854  [333500/467875]\n",
            "loss: 0.380291  [336400/467875]\n",
            "loss: 0.426509  [339300/467875]\n",
            "loss: 0.429188  [342200/467875]\n",
            "loss: 0.436491  [345100/467875]\n",
            "loss: 0.539727  [348000/467875]\n",
            "loss: 0.330966  [350900/467875]\n",
            "loss: 0.371619  [353800/467875]\n",
            "loss: 0.659295  [356700/467875]\n",
            "loss: 0.398435  [359600/467875]\n",
            "loss: 0.405167  [362500/467875]\n",
            "loss: 0.627157  [365400/467875]\n",
            "loss: 0.512435  [368300/467875]\n",
            "loss: 0.528036  [371200/467875]\n",
            "loss: 0.488385  [374100/467875]\n",
            "loss: 0.601409  [377000/467875]\n",
            "loss: 0.479440  [379900/467875]\n",
            "loss: 0.373802  [382800/467875]\n",
            "loss: 0.482404  [385700/467875]\n",
            "loss: 0.555681  [388600/467875]\n",
            "loss: 0.519907  [391500/467875]\n",
            "loss: 0.360257  [394400/467875]\n",
            "loss: 0.541382  [397300/467875]\n",
            "loss: 0.357137  [400200/467875]\n",
            "loss: 0.519015  [403100/467875]\n",
            "loss: 0.393368  [406000/467875]\n",
            "loss: 0.330067  [408900/467875]\n",
            "loss: 0.496328  [411800/467875]\n",
            "loss: 0.477376  [414700/467875]\n",
            "loss: 0.386368  [417600/467875]\n",
            "loss: 0.578578  [420500/467875]\n",
            "loss: 0.589572  [423400/467875]\n",
            "loss: 0.541541  [426300/467875]\n",
            "loss: 0.501939  [429200/467875]\n",
            "loss: 0.631407  [432100/467875]\n",
            "loss: 0.458193  [435000/467875]\n",
            "loss: 0.453366  [437900/467875]\n",
            "loss: 0.385290  [440800/467875]\n",
            "loss: 0.420417  [443700/467875]\n",
            "loss: 0.465245  [446600/467875]\n",
            "loss: 0.496390  [449500/467875]\n",
            "loss: 0.345374  [452400/467875]\n",
            "loss: 0.589054  [455300/467875]\n",
            "loss: 0.546361  [458200/467875]\n",
            "loss: 0.504870  [461100/467875]\n",
            "loss: 0.373005  [464000/467875]\n",
            "loss: 0.482543  [466900/467875]\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.478524 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.594280  [    0/467875]\n",
            "loss: 0.399566  [ 2900/467875]\n",
            "loss: 0.663120  [ 5800/467875]\n",
            "loss: 0.490177  [ 8700/467875]\n",
            "loss: 0.491541  [11600/467875]\n",
            "loss: 0.461874  [14500/467875]\n",
            "loss: 0.387757  [17400/467875]\n",
            "loss: 0.499453  [20300/467875]\n",
            "loss: 0.374401  [23200/467875]\n",
            "loss: 0.589142  [26100/467875]\n",
            "loss: 0.694890  [29000/467875]\n",
            "loss: 0.394466  [31900/467875]\n",
            "loss: 0.510026  [34800/467875]\n",
            "loss: 0.421946  [37700/467875]\n",
            "loss: 0.569059  [40600/467875]\n",
            "loss: 0.635568  [43500/467875]\n",
            "loss: 0.441121  [46400/467875]\n",
            "loss: 0.443061  [49300/467875]\n",
            "loss: 0.458733  [52200/467875]\n",
            "loss: 0.610252  [55100/467875]\n",
            "loss: 0.459502  [58000/467875]\n",
            "loss: 0.360517  [60900/467875]\n",
            "loss: 0.583967  [63800/467875]\n",
            "loss: 0.564456  [66700/467875]\n",
            "loss: 0.576303  [69600/467875]\n",
            "loss: 0.534278  [72500/467875]\n",
            "loss: 0.319602  [75400/467875]\n",
            "loss: 0.699967  [78300/467875]\n",
            "loss: 0.550480  [81200/467875]\n",
            "loss: 0.447913  [84100/467875]\n",
            "loss: 0.445510  [87000/467875]\n",
            "loss: 0.391907  [89900/467875]\n",
            "loss: 0.420539  [92800/467875]\n",
            "loss: 0.374829  [95700/467875]\n",
            "loss: 0.509898  [98600/467875]\n",
            "loss: 0.605241  [101500/467875]\n",
            "loss: 0.462802  [104400/467875]\n",
            "loss: 0.394770  [107300/467875]\n",
            "loss: 0.476695  [110200/467875]\n",
            "loss: 0.445760  [113100/467875]\n",
            "loss: 0.526495  [116000/467875]\n",
            "loss: 0.600573  [118900/467875]\n",
            "loss: 0.499710  [121800/467875]\n",
            "loss: 0.508399  [124700/467875]\n",
            "loss: 0.453908  [127600/467875]\n",
            "loss: 0.400676  [130500/467875]\n",
            "loss: 0.453278  [133400/467875]\n",
            "loss: 0.301882  [136300/467875]\n",
            "loss: 0.586543  [139200/467875]\n",
            "loss: 0.562218  [142100/467875]\n",
            "loss: 0.370380  [145000/467875]\n",
            "loss: 0.363970  [147900/467875]\n",
            "loss: 0.460605  [150800/467875]\n",
            "loss: 0.509694  [153700/467875]\n",
            "loss: 0.614965  [156600/467875]\n",
            "loss: 0.297842  [159500/467875]\n",
            "loss: 0.615786  [162400/467875]\n",
            "loss: 0.625540  [165300/467875]\n",
            "loss: 0.505643  [168200/467875]\n",
            "loss: 0.534139  [171100/467875]\n",
            "loss: 0.612763  [174000/467875]\n",
            "loss: 0.281920  [176900/467875]\n",
            "loss: 0.449649  [179800/467875]\n",
            "loss: 0.472578  [182700/467875]\n",
            "loss: 0.661507  [185600/467875]\n",
            "loss: 0.440026  [188500/467875]\n",
            "loss: 0.288439  [191400/467875]\n",
            "loss: 0.511278  [194300/467875]\n",
            "loss: 0.398805  [197200/467875]\n",
            "loss: 0.509580  [200100/467875]\n",
            "loss: 0.367231  [203000/467875]\n",
            "loss: 0.570920  [205900/467875]\n",
            "loss: 0.323437  [208800/467875]\n",
            "loss: 0.565934  [211700/467875]\n",
            "loss: 0.364167  [214600/467875]\n",
            "loss: 0.441440  [217500/467875]\n",
            "loss: 0.493409  [220400/467875]\n",
            "loss: 0.522257  [223300/467875]\n",
            "loss: 0.642746  [226200/467875]\n",
            "loss: 0.356509  [229100/467875]\n",
            "loss: 0.640704  [232000/467875]\n",
            "loss: 0.520751  [234900/467875]\n",
            "loss: 0.431776  [237800/467875]\n",
            "loss: 0.521296  [240700/467875]\n",
            "loss: 0.322428  [243600/467875]\n",
            "loss: 0.600021  [246500/467875]\n",
            "loss: 0.543200  [249400/467875]\n",
            "loss: 0.522512  [252300/467875]\n",
            "loss: 0.467669  [255200/467875]\n",
            "loss: 0.676489  [258100/467875]\n",
            "loss: 0.321978  [261000/467875]\n",
            "loss: 0.500745  [263900/467875]\n",
            "loss: 0.684614  [266800/467875]\n",
            "loss: 0.273039  [269700/467875]\n",
            "loss: 0.724433  [272600/467875]\n",
            "loss: 0.486773  [275500/467875]\n",
            "loss: 0.508428  [278400/467875]\n",
            "loss: 0.500261  [281300/467875]\n",
            "loss: 0.406995  [284200/467875]\n",
            "loss: 0.445950  [287100/467875]\n",
            "loss: 0.627274  [290000/467875]\n",
            "loss: 0.633858  [292900/467875]\n",
            "loss: 0.363724  [295800/467875]\n",
            "loss: 0.541827  [298700/467875]\n",
            "loss: 0.486013  [301600/467875]\n",
            "loss: 0.576092  [304500/467875]\n",
            "loss: 0.428280  [307400/467875]\n",
            "loss: 0.604861  [310300/467875]\n",
            "loss: 0.558667  [313200/467875]\n",
            "loss: 0.378227  [316100/467875]\n",
            "loss: 0.373407  [319000/467875]\n",
            "loss: 0.521458  [321900/467875]\n",
            "loss: 0.435514  [324800/467875]\n",
            "loss: 0.395026  [327700/467875]\n",
            "loss: 0.512510  [330600/467875]\n",
            "loss: 0.436981  [333500/467875]\n",
            "loss: 0.589875  [336400/467875]\n",
            "loss: 0.486277  [339300/467875]\n",
            "loss: 0.429769  [342200/467875]\n",
            "loss: 0.453837  [345100/467875]\n",
            "loss: 0.355858  [348000/467875]\n",
            "loss: 0.384300  [350900/467875]\n",
            "loss: 0.326152  [353800/467875]\n",
            "loss: 0.509178  [356700/467875]\n",
            "loss: 0.447655  [359600/467875]\n",
            "loss: 0.618749  [362500/467875]\n",
            "loss: 0.327508  [365400/467875]\n",
            "loss: 0.528181  [368300/467875]\n",
            "loss: 0.575518  [371200/467875]\n",
            "loss: 0.529715  [374100/467875]\n",
            "loss: 0.349201  [377000/467875]\n",
            "loss: 0.439591  [379900/467875]\n",
            "loss: 0.341892  [382800/467875]\n",
            "loss: 0.536177  [385700/467875]\n",
            "loss: 0.621393  [388600/467875]\n",
            "loss: 0.546652  [391500/467875]\n",
            "loss: 0.388242  [394400/467875]\n",
            "loss: 0.358509  [397300/467875]\n",
            "loss: 0.472602  [400200/467875]\n",
            "loss: 0.510655  [403100/467875]\n",
            "loss: 0.366461  [406000/467875]\n",
            "loss: 0.508582  [408900/467875]\n",
            "loss: 0.467227  [411800/467875]\n",
            "loss: 0.576312  [414700/467875]\n",
            "loss: 0.382033  [417600/467875]\n",
            "loss: 0.410417  [420500/467875]\n",
            "loss: 0.525896  [423400/467875]\n",
            "loss: 0.335478  [426300/467875]\n",
            "loss: 0.406065  [429200/467875]\n",
            "loss: 0.550027  [432100/467875]\n",
            "loss: 0.324470  [435000/467875]\n",
            "loss: 0.492132  [437900/467875]\n",
            "loss: 0.446277  [440800/467875]\n",
            "loss: 0.385869  [443700/467875]\n",
            "loss: 0.323147  [446600/467875]\n",
            "loss: 0.587168  [449500/467875]\n",
            "loss: 0.468758  [452400/467875]\n",
            "loss: 0.297704  [455300/467875]\n",
            "loss: 0.449753  [458200/467875]\n",
            "loss: 0.531622  [461100/467875]\n",
            "loss: 0.372743  [464000/467875]\n",
            "loss: 0.461245  [466900/467875]\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.475737 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 24 s\n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.552954  [    0/467875]\n",
            "loss: 0.451478  [ 2900/467875]\n",
            "loss: 0.489393  [ 5800/467875]\n",
            "loss: 0.409360  [ 8700/467875]\n",
            "loss: 0.489629  [11600/467875]\n",
            "loss: 0.497332  [14500/467875]\n",
            "loss: 0.443190  [17400/467875]\n",
            "loss: 0.349431  [20300/467875]\n",
            "loss: 0.395351  [23200/467875]\n",
            "loss: 0.558006  [26100/467875]\n",
            "loss: 0.532808  [29000/467875]\n",
            "loss: 0.478247  [31900/467875]\n",
            "loss: 0.441490  [34800/467875]\n",
            "loss: 0.397414  [37700/467875]\n",
            "loss: 0.573588  [40600/467875]\n",
            "loss: 0.300987  [43500/467875]\n",
            "loss: 0.464059  [46400/467875]\n",
            "loss: 0.533498  [49300/467875]\n",
            "loss: 0.459541  [52200/467875]\n",
            "loss: 0.542362  [55100/467875]\n",
            "loss: 0.431694  [58000/467875]\n",
            "loss: 0.641083  [60900/467875]\n",
            "loss: 0.668231  [63800/467875]\n",
            "loss: 0.310411  [66700/467875]\n",
            "loss: 0.365927  [69600/467875]\n",
            "loss: 0.455004  [72500/467875]\n",
            "loss: 0.569360  [75400/467875]\n",
            "loss: 0.519306  [78300/467875]\n",
            "loss: 0.537538  [81200/467875]\n",
            "loss: 0.548081  [84100/467875]\n",
            "loss: 0.406427  [87000/467875]\n",
            "loss: 0.500220  [89900/467875]\n",
            "loss: 0.321963  [92800/467875]\n",
            "loss: 0.527342  [95700/467875]\n",
            "loss: 0.516465  [98600/467875]\n",
            "loss: 0.321938  [101500/467875]\n",
            "loss: 0.436631  [104400/467875]\n",
            "loss: 0.368175  [107300/467875]\n",
            "loss: 0.411369  [110200/467875]\n",
            "loss: 0.384998  [113100/467875]\n",
            "loss: 0.821583  [116000/467875]\n",
            "loss: 0.372032  [118900/467875]\n",
            "loss: 0.506458  [121800/467875]\n",
            "loss: 0.399118  [124700/467875]\n",
            "loss: 0.590500  [127600/467875]\n",
            "loss: 0.328699  [130500/467875]\n",
            "loss: 0.471806  [133400/467875]\n",
            "loss: 0.474369  [136300/467875]\n",
            "loss: 0.534963  [139200/467875]\n",
            "loss: 0.535692  [142100/467875]\n",
            "loss: 0.510531  [145000/467875]\n",
            "loss: 0.471121  [147900/467875]\n",
            "loss: 0.555532  [150800/467875]\n",
            "loss: 0.398616  [153700/467875]\n",
            "loss: 0.362471  [156600/467875]\n",
            "loss: 0.381662  [159500/467875]\n",
            "loss: 0.371313  [162400/467875]\n",
            "loss: 0.545137  [165300/467875]\n",
            "loss: 0.506987  [168200/467875]\n",
            "loss: 0.530276  [171100/467875]\n",
            "loss: 0.559291  [174000/467875]\n",
            "loss: 0.359318  [176900/467875]\n",
            "loss: 0.466619  [179800/467875]\n",
            "loss: 0.477081  [182700/467875]\n",
            "loss: 0.406910  [185600/467875]\n",
            "loss: 0.649165  [188500/467875]\n",
            "loss: 0.523408  [191400/467875]\n",
            "loss: 0.652142  [194300/467875]\n",
            "loss: 0.377512  [197200/467875]\n",
            "loss: 0.474386  [200100/467875]\n",
            "loss: 0.369354  [203000/467875]\n",
            "loss: 0.338189  [205900/467875]\n",
            "loss: 0.400992  [208800/467875]\n",
            "loss: 0.541868  [211700/467875]\n",
            "loss: 0.419034  [214600/467875]\n",
            "loss: 0.520194  [217500/467875]\n",
            "loss: 0.448905  [220400/467875]\n",
            "loss: 0.449372  [223300/467875]\n",
            "loss: 0.557211  [226200/467875]\n",
            "loss: 0.355849  [229100/467875]\n",
            "loss: 0.431499  [232000/467875]\n",
            "loss: 0.481828  [234900/467875]\n",
            "loss: 0.459064  [237800/467875]\n",
            "loss: 0.460087  [240700/467875]\n",
            "loss: 0.540986  [243600/467875]\n",
            "loss: 0.475766  [246500/467875]\n",
            "loss: 0.474751  [249400/467875]\n",
            "loss: 0.671914  [252300/467875]\n",
            "loss: 0.485296  [255200/467875]\n",
            "loss: 0.278583  [258100/467875]\n",
            "loss: 0.392301  [261000/467875]\n",
            "loss: 0.399054  [263900/467875]\n",
            "loss: 0.494709  [266800/467875]\n",
            "loss: 0.553205  [269700/467875]\n",
            "loss: 0.440037  [272600/467875]\n",
            "loss: 0.495452  [275500/467875]\n",
            "loss: 0.580073  [278400/467875]\n",
            "loss: 0.577102  [281300/467875]\n",
            "loss: 0.383356  [284200/467875]\n",
            "loss: 0.432999  [287100/467875]\n",
            "loss: 0.588602  [290000/467875]\n",
            "loss: 0.716596  [292900/467875]\n",
            "loss: 0.572855  [295800/467875]\n",
            "loss: 0.567063  [298700/467875]\n",
            "loss: 0.443711  [301600/467875]\n",
            "loss: 0.351707  [304500/467875]\n",
            "loss: 0.643569  [307400/467875]\n",
            "loss: 0.394841  [310300/467875]\n",
            "loss: 0.523320  [313200/467875]\n",
            "loss: 0.493241  [316100/467875]\n",
            "loss: 0.542861  [319000/467875]\n",
            "loss: 0.603956  [321900/467875]\n",
            "loss: 0.539215  [324800/467875]\n",
            "loss: 0.446402  [327700/467875]\n",
            "loss: 0.406075  [330600/467875]\n",
            "loss: 0.491073  [333500/467875]\n",
            "loss: 0.721634  [336400/467875]\n",
            "loss: 0.524893  [339300/467875]\n",
            "loss: 0.425431  [342200/467875]\n",
            "loss: 0.488287  [345100/467875]\n",
            "loss: 0.646356  [348000/467875]\n",
            "loss: 0.734734  [350900/467875]\n",
            "loss: 0.654503  [353800/467875]\n",
            "loss: 0.464028  [356700/467875]\n",
            "loss: 0.433891  [359600/467875]\n",
            "loss: 0.368979  [362500/467875]\n",
            "loss: 0.371047  [365400/467875]\n",
            "loss: 0.510178  [368300/467875]\n",
            "loss: 0.517334  [371200/467875]\n",
            "loss: 0.439040  [374100/467875]\n",
            "loss: 0.500606  [377000/467875]\n",
            "loss: 0.462800  [379900/467875]\n",
            "loss: 0.429000  [382800/467875]\n",
            "loss: 0.319438  [385700/467875]\n",
            "loss: 0.407253  [388600/467875]\n",
            "loss: 0.662026  [391500/467875]\n",
            "loss: 0.483250  [394400/467875]\n",
            "loss: 0.555127  [397300/467875]\n",
            "loss: 0.569218  [400200/467875]\n",
            "loss: 0.423541  [403100/467875]\n",
            "loss: 0.505814  [406000/467875]\n",
            "loss: 0.623614  [408900/467875]\n",
            "loss: 0.498774  [411800/467875]\n",
            "loss: 0.458929  [414700/467875]\n",
            "loss: 0.341021  [417600/467875]\n",
            "loss: 0.247871  [420500/467875]\n",
            "loss: 0.759241  [423400/467875]\n",
            "loss: 0.447586  [426300/467875]\n",
            "loss: 0.462905  [429200/467875]\n",
            "loss: 0.497447  [432100/467875]\n",
            "loss: 0.391454  [435000/467875]\n",
            "loss: 0.444935  [437900/467875]\n",
            "loss: 0.379671  [440800/467875]\n",
            "loss: 0.423164  [443700/467875]\n",
            "loss: 0.463168  [446600/467875]\n",
            "loss: 0.324762  [449500/467875]\n",
            "loss: 0.420029  [452400/467875]\n",
            "loss: 0.417600  [455300/467875]\n",
            "loss: 0.404336  [458200/467875]\n",
            "loss: 0.556837  [461100/467875]\n",
            "loss: 0.332373  [464000/467875]\n",
            "loss: 0.731756  [466900/467875]\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.477182 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.513822  [    0/467875]\n",
            "loss: 0.375425  [ 2900/467875]\n",
            "loss: 0.548149  [ 5800/467875]\n",
            "loss: 0.537647  [ 8700/467875]\n",
            "loss: 0.418108  [11600/467875]\n",
            "loss: 0.406849  [14500/467875]\n",
            "loss: 0.550530  [17400/467875]\n",
            "loss: 0.404741  [20300/467875]\n",
            "loss: 0.561584  [23200/467875]\n",
            "loss: 0.501348  [26100/467875]\n",
            "loss: 0.326038  [29000/467875]\n",
            "loss: 0.443883  [31900/467875]\n",
            "loss: 0.533447  [34800/467875]\n",
            "loss: 0.390934  [37700/467875]\n",
            "loss: 0.446928  [40600/467875]\n",
            "loss: 0.621978  [43500/467875]\n",
            "loss: 0.551392  [46400/467875]\n",
            "loss: 0.574069  [49300/467875]\n",
            "loss: 0.417504  [52200/467875]\n",
            "loss: 0.531637  [55100/467875]\n",
            "loss: 0.473266  [58000/467875]\n",
            "loss: 0.628171  [60900/467875]\n",
            "loss: 0.577071  [63800/467875]\n",
            "loss: 0.571602  [66700/467875]\n",
            "loss: 0.600413  [69600/467875]\n",
            "loss: 0.703774  [72500/467875]\n",
            "loss: 0.456277  [75400/467875]\n",
            "loss: 0.514052  [78300/467875]\n",
            "loss: 0.539001  [81200/467875]\n",
            "loss: 0.296591  [84100/467875]\n",
            "loss: 0.492523  [87000/467875]\n",
            "loss: 0.634813  [89900/467875]\n",
            "loss: 0.442110  [92800/467875]\n",
            "loss: 0.475943  [95700/467875]\n",
            "loss: 0.305127  [98600/467875]\n",
            "loss: 0.428978  [101500/467875]\n",
            "loss: 0.669442  [104400/467875]\n",
            "loss: 0.579442  [107300/467875]\n",
            "loss: 0.560130  [110200/467875]\n",
            "loss: 0.541474  [113100/467875]\n",
            "loss: 0.516548  [116000/467875]\n",
            "loss: 0.811754  [118900/467875]\n",
            "loss: 0.491248  [121800/467875]\n",
            "loss: 0.521821  [124700/467875]\n",
            "loss: 0.631804  [127600/467875]\n",
            "loss: 0.324866  [130500/467875]\n",
            "loss: 0.454727  [133400/467875]\n",
            "loss: 0.587320  [136300/467875]\n",
            "loss: 0.391023  [139200/467875]\n",
            "loss: 0.534343  [142100/467875]\n",
            "loss: 0.503798  [145000/467875]\n",
            "loss: 0.428176  [147900/467875]\n",
            "loss: 0.338716  [150800/467875]\n",
            "loss: 0.420588  [153700/467875]\n",
            "loss: 0.567774  [156600/467875]\n",
            "loss: 0.390839  [159500/467875]\n",
            "loss: 0.525318  [162400/467875]\n",
            "loss: 0.456414  [165300/467875]\n",
            "loss: 0.542826  [168200/467875]\n",
            "loss: 0.352975  [171100/467875]\n",
            "loss: 0.493632  [174000/467875]\n",
            "loss: 0.380431  [176900/467875]\n",
            "loss: 0.376156  [179800/467875]\n",
            "loss: 0.498843  [182700/467875]\n",
            "loss: 0.586234  [185600/467875]\n",
            "loss: 0.389741  [188500/467875]\n",
            "loss: 0.454779  [191400/467875]\n",
            "loss: 0.622924  [194300/467875]\n",
            "loss: 0.466243  [197200/467875]\n",
            "loss: 0.355474  [200100/467875]\n",
            "loss: 0.511508  [203000/467875]\n",
            "loss: 0.421791  [205900/467875]\n",
            "loss: 0.644651  [208800/467875]\n",
            "loss: 0.559565  [211700/467875]\n",
            "loss: 0.453990  [214600/467875]\n",
            "loss: 0.565420  [217500/467875]\n",
            "loss: 0.484350  [220400/467875]\n",
            "loss: 0.369262  [223300/467875]\n",
            "loss: 0.316725  [226200/467875]\n",
            "loss: 0.556552  [229100/467875]\n",
            "loss: 0.607397  [232000/467875]\n",
            "loss: 0.562003  [234900/467875]\n",
            "loss: 0.467737  [237800/467875]\n",
            "loss: 0.539605  [240700/467875]\n",
            "loss: 0.559046  [243600/467875]\n",
            "loss: 0.424048  [246500/467875]\n",
            "loss: 0.489333  [249400/467875]\n",
            "loss: 0.585877  [252300/467875]\n",
            "loss: 0.623400  [255200/467875]\n",
            "loss: 0.302352  [258100/467875]\n",
            "loss: 0.333331  [261000/467875]\n",
            "loss: 0.468207  [263900/467875]\n",
            "loss: 0.390710  [266800/467875]\n",
            "loss: 0.551487  [269700/467875]\n",
            "loss: 0.414192  [272600/467875]\n",
            "loss: 0.368941  [275500/467875]\n",
            "loss: 0.384927  [278400/467875]\n",
            "loss: 0.398571  [281300/467875]\n",
            "loss: 0.459585  [284200/467875]\n",
            "loss: 0.314227  [287100/467875]\n",
            "loss: 0.305132  [290000/467875]\n",
            "loss: 0.476427  [292900/467875]\n",
            "loss: 0.488169  [295800/467875]\n",
            "loss: 0.391035  [298700/467875]\n",
            "loss: 0.606896  [301600/467875]\n",
            "loss: 0.537709  [304500/467875]\n",
            "loss: 0.877583  [307400/467875]\n",
            "loss: 0.349570  [310300/467875]\n",
            "loss: 0.591066  [313200/467875]\n",
            "loss: 0.409037  [316100/467875]\n",
            "loss: 0.580673  [319000/467875]\n",
            "loss: 0.314320  [321900/467875]\n",
            "loss: 0.424194  [324800/467875]\n",
            "loss: 0.322961  [327700/467875]\n",
            "loss: 0.426044  [330600/467875]\n",
            "loss: 0.424606  [333500/467875]\n",
            "loss: 0.298534  [336400/467875]\n",
            "loss: 0.417673  [339300/467875]\n",
            "loss: 0.372073  [342200/467875]\n",
            "loss: 0.400694  [345100/467875]\n",
            "loss: 0.538287  [348000/467875]\n",
            "loss: 0.413022  [350900/467875]\n",
            "loss: 0.560797  [353800/467875]\n",
            "loss: 0.441768  [356700/467875]\n",
            "loss: 0.424736  [359600/467875]\n",
            "loss: 0.484433  [362500/467875]\n",
            "loss: 0.512570  [365400/467875]\n",
            "loss: 0.385159  [368300/467875]\n",
            "loss: 0.568054  [371200/467875]\n",
            "loss: 0.299470  [374100/467875]\n",
            "loss: 0.426362  [377000/467875]\n",
            "loss: 0.345291  [379900/467875]\n",
            "loss: 0.441658  [382800/467875]\n",
            "loss: 0.540778  [385700/467875]\n",
            "loss: 0.573998  [388600/467875]\n",
            "loss: 0.533907  [391500/467875]\n",
            "loss: 0.423829  [394400/467875]\n",
            "loss: 0.313264  [397300/467875]\n",
            "loss: 0.402406  [400200/467875]\n",
            "loss: 0.523340  [403100/467875]\n",
            "loss: 0.455218  [406000/467875]\n",
            "loss: 0.487020  [408900/467875]\n",
            "loss: 0.467498  [411800/467875]\n",
            "loss: 0.407479  [414700/467875]\n",
            "loss: 0.308829  [417600/467875]\n",
            "loss: 0.405462  [420500/467875]\n",
            "loss: 0.516285  [423400/467875]\n",
            "loss: 0.568845  [426300/467875]\n",
            "loss: 0.600906  [429200/467875]\n",
            "loss: 0.367766  [432100/467875]\n",
            "loss: 0.580760  [435000/467875]\n",
            "loss: 0.420004  [437900/467875]\n",
            "loss: 0.575864  [440800/467875]\n",
            "loss: 0.327774  [443700/467875]\n",
            "loss: 0.358208  [446600/467875]\n",
            "loss: 0.466739  [449500/467875]\n",
            "loss: 0.387725  [452400/467875]\n",
            "loss: 0.461171  [455300/467875]\n",
            "loss: 0.327281  [458200/467875]\n",
            "loss: 0.562155  [461100/467875]\n",
            "loss: 0.462025  [464000/467875]\n",
            "loss: 0.339344  [466900/467875]\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.477438 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.649042  [    0/467875]\n",
            "loss: 0.390237  [ 2900/467875]\n",
            "loss: 0.341871  [ 5800/467875]\n",
            "loss: 0.576940  [ 8700/467875]\n",
            "loss: 0.573653  [11600/467875]\n",
            "loss: 0.447974  [14500/467875]\n",
            "loss: 0.427000  [17400/467875]\n",
            "loss: 0.466956  [20300/467875]\n",
            "loss: 0.383293  [23200/467875]\n",
            "loss: 0.555447  [26100/467875]\n",
            "loss: 0.510457  [29000/467875]\n",
            "loss: 0.442627  [31900/467875]\n",
            "loss: 0.506637  [34800/467875]\n",
            "loss: 0.538442  [37700/467875]\n",
            "loss: 0.512395  [40600/467875]\n",
            "loss: 0.641972  [43500/467875]\n",
            "loss: 0.426417  [46400/467875]\n",
            "loss: 0.547026  [49300/467875]\n",
            "loss: 0.432750  [52200/467875]\n",
            "loss: 0.427072  [55100/467875]\n",
            "loss: 0.530001  [58000/467875]\n",
            "loss: 0.545732  [60900/467875]\n",
            "loss: 0.438758  [63800/467875]\n",
            "loss: 0.401877  [66700/467875]\n",
            "loss: 0.399388  [69600/467875]\n",
            "loss: 0.497162  [72500/467875]\n",
            "loss: 0.286347  [75400/467875]\n",
            "loss: 0.501387  [78300/467875]\n",
            "loss: 0.388443  [81200/467875]\n",
            "loss: 0.461459  [84100/467875]\n",
            "loss: 0.496655  [87000/467875]\n",
            "loss: 0.531154  [89900/467875]\n",
            "loss: 0.479857  [92800/467875]\n",
            "loss: 0.425753  [95700/467875]\n",
            "loss: 0.465241  [98600/467875]\n",
            "loss: 0.391728  [101500/467875]\n",
            "loss: 0.546629  [104400/467875]\n",
            "loss: 0.446167  [107300/467875]\n",
            "loss: 0.564738  [110200/467875]\n",
            "loss: 0.290443  [113100/467875]\n",
            "loss: 0.428155  [116000/467875]\n",
            "loss: 0.539360  [118900/467875]\n",
            "loss: 0.489909  [121800/467875]\n",
            "loss: 0.453717  [124700/467875]\n",
            "loss: 0.531680  [127600/467875]\n",
            "loss: 0.470858  [130500/467875]\n",
            "loss: 0.628736  [133400/467875]\n",
            "loss: 0.393683  [136300/467875]\n",
            "loss: 0.643383  [139200/467875]\n",
            "loss: 0.572764  [142100/467875]\n",
            "loss: 0.366850  [145000/467875]\n",
            "loss: 0.521458  [147900/467875]\n",
            "loss: 0.648887  [150800/467875]\n",
            "loss: 0.693187  [153700/467875]\n",
            "loss: 0.397562  [156600/467875]\n",
            "loss: 0.449424  [159500/467875]\n",
            "loss: 0.572383  [162400/467875]\n",
            "loss: 0.486226  [165300/467875]\n",
            "loss: 0.618670  [168200/467875]\n",
            "loss: 0.463287  [171100/467875]\n",
            "loss: 0.393927  [174000/467875]\n",
            "loss: 0.582934  [176900/467875]\n",
            "loss: 0.507442  [179800/467875]\n",
            "loss: 0.436835  [182700/467875]\n",
            "loss: 0.495940  [185600/467875]\n",
            "loss: 0.595100  [188500/467875]\n",
            "loss: 0.466820  [191400/467875]\n",
            "loss: 0.451681  [194300/467875]\n",
            "loss: 0.421411  [197200/467875]\n",
            "loss: 0.464355  [200100/467875]\n",
            "loss: 0.364385  [203000/467875]\n",
            "loss: 0.436933  [205900/467875]\n",
            "loss: 0.553935  [208800/467875]\n",
            "loss: 0.517033  [211700/467875]\n",
            "loss: 0.423734  [214600/467875]\n",
            "loss: 0.460426  [217500/467875]\n",
            "loss: 0.565077  [220400/467875]\n",
            "loss: 0.476571  [223300/467875]\n",
            "loss: 0.510618  [226200/467875]\n",
            "loss: 0.529231  [229100/467875]\n",
            "loss: 0.566206  [232000/467875]\n",
            "loss: 0.295512  [234900/467875]\n",
            "loss: 0.433985  [237800/467875]\n",
            "loss: 0.481647  [240700/467875]\n",
            "loss: 0.487816  [243600/467875]\n",
            "loss: 0.316385  [246500/467875]\n",
            "loss: 0.532018  [249400/467875]\n",
            "loss: 0.385172  [252300/467875]\n",
            "loss: 0.462917  [255200/467875]\n",
            "loss: 0.510386  [258100/467875]\n",
            "loss: 0.648604  [261000/467875]\n",
            "loss: 0.436844  [263900/467875]\n",
            "loss: 0.462598  [266800/467875]\n",
            "loss: 0.337161  [269700/467875]\n",
            "loss: 0.492832  [272600/467875]\n",
            "loss: 0.315286  [275500/467875]\n",
            "loss: 0.353247  [278400/467875]\n",
            "loss: 0.497118  [281300/467875]\n",
            "loss: 0.364998  [284200/467875]\n",
            "loss: 0.451413  [287100/467875]\n",
            "loss: 0.477416  [290000/467875]\n",
            "loss: 0.494796  [292900/467875]\n",
            "loss: 0.368514  [295800/467875]\n",
            "loss: 0.405000  [298700/467875]\n",
            "loss: 0.508718  [301600/467875]\n",
            "loss: 0.447774  [304500/467875]\n",
            "loss: 0.812190  [307400/467875]\n",
            "loss: 0.442455  [310300/467875]\n",
            "loss: 0.380082  [313200/467875]\n",
            "loss: 0.484671  [316100/467875]\n",
            "loss: 0.430889  [319000/467875]\n",
            "loss: 0.588289  [321900/467875]\n",
            "loss: 0.541520  [324800/467875]\n",
            "loss: 0.492639  [327700/467875]\n",
            "loss: 0.287287  [330600/467875]\n",
            "loss: 0.557509  [333500/467875]\n",
            "loss: 0.450277  [336400/467875]\n",
            "loss: 0.577001  [339300/467875]\n",
            "loss: 0.499909  [342200/467875]\n",
            "loss: 0.556614  [345100/467875]\n",
            "loss: 0.485763  [348000/467875]\n",
            "loss: 0.293133  [350900/467875]\n",
            "loss: 0.580789  [353800/467875]\n",
            "loss: 0.502390  [356700/467875]\n",
            "loss: 0.535755  [359600/467875]\n",
            "loss: 0.397141  [362500/467875]\n",
            "loss: 0.478747  [365400/467875]\n",
            "loss: 0.586694  [368300/467875]\n",
            "loss: 0.352926  [371200/467875]\n",
            "loss: 0.403753  [374100/467875]\n",
            "loss: 0.460968  [377000/467875]\n",
            "loss: 0.385461  [379900/467875]\n",
            "loss: 0.495415  [382800/467875]\n",
            "loss: 0.468952  [385700/467875]\n",
            "loss: 0.439151  [388600/467875]\n",
            "loss: 0.415924  [391500/467875]\n",
            "loss: 0.446908  [394400/467875]\n",
            "loss: 0.617181  [397300/467875]\n",
            "loss: 0.357096  [400200/467875]\n",
            "loss: 0.440398  [403100/467875]\n",
            "loss: 0.310886  [406000/467875]\n",
            "loss: 0.565818  [408900/467875]\n",
            "loss: 0.662470  [411800/467875]\n",
            "loss: 0.492856  [414700/467875]\n",
            "loss: 0.313685  [417600/467875]\n",
            "loss: 0.279268  [420500/467875]\n",
            "loss: 0.392268  [423400/467875]\n",
            "loss: 0.506580  [426300/467875]\n",
            "loss: 0.502191  [429200/467875]\n",
            "loss: 0.545079  [432100/467875]\n",
            "loss: 0.408152  [435000/467875]\n",
            "loss: 0.568976  [437900/467875]\n",
            "loss: 0.692528  [440800/467875]\n",
            "loss: 0.594325  [443700/467875]\n",
            "loss: 0.379141  [446600/467875]\n",
            "loss: 0.391290  [449500/467875]\n",
            "loss: 0.505564  [452400/467875]\n",
            "loss: 0.406378  [455300/467875]\n",
            "loss: 0.534843  [458200/467875]\n",
            "loss: 0.376676  [461100/467875]\n",
            "loss: 0.389961  [464000/467875]\n",
            "loss: 0.524562  [466900/467875]\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.475879 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.429703  [    0/467875]\n",
            "loss: 0.505015  [ 2900/467875]\n",
            "loss: 0.461197  [ 5800/467875]\n",
            "loss: 0.582465  [ 8700/467875]\n",
            "loss: 0.512149  [11600/467875]\n",
            "loss: 0.531290  [14500/467875]\n",
            "loss: 0.333207  [17400/467875]\n",
            "loss: 0.396187  [20300/467875]\n",
            "loss: 0.346385  [23200/467875]\n",
            "loss: 0.645267  [26100/467875]\n",
            "loss: 0.589647  [29000/467875]\n",
            "loss: 0.568385  [31900/467875]\n",
            "loss: 0.507416  [34800/467875]\n",
            "loss: 0.429347  [37700/467875]\n",
            "loss: 0.799276  [40600/467875]\n",
            "loss: 0.373125  [43500/467875]\n",
            "loss: 0.546914  [46400/467875]\n",
            "loss: 0.511152  [49300/467875]\n",
            "loss: 0.492642  [52200/467875]\n",
            "loss: 0.483574  [55100/467875]\n",
            "loss: 0.442010  [58000/467875]\n",
            "loss: 0.334281  [60900/467875]\n",
            "loss: 0.492519  [63800/467875]\n",
            "loss: 0.452344  [66700/467875]\n",
            "loss: 0.392577  [69600/467875]\n",
            "loss: 0.437326  [72500/467875]\n",
            "loss: 0.459994  [75400/467875]\n",
            "loss: 0.317745  [78300/467875]\n",
            "loss: 0.428148  [81200/467875]\n",
            "loss: 0.414782  [84100/467875]\n",
            "loss: 0.434893  [87000/467875]\n",
            "loss: 0.428132  [89900/467875]\n",
            "loss: 0.310980  [92800/467875]\n",
            "loss: 0.469131  [95700/467875]\n",
            "loss: 0.474359  [98600/467875]\n",
            "loss: 0.473520  [101500/467875]\n",
            "loss: 0.454098  [104400/467875]\n",
            "loss: 0.428324  [107300/467875]\n",
            "loss: 0.598836  [110200/467875]\n",
            "loss: 0.468216  [113100/467875]\n",
            "loss: 0.608368  [116000/467875]\n",
            "loss: 0.569802  [118900/467875]\n",
            "loss: 0.489885  [121800/467875]\n",
            "loss: 0.620849  [124700/467875]\n",
            "loss: 0.414206  [127600/467875]\n",
            "loss: 0.487789  [130500/467875]\n",
            "loss: 0.456098  [133400/467875]\n",
            "loss: 0.588921  [136300/467875]\n",
            "loss: 0.523797  [139200/467875]\n",
            "loss: 0.452526  [142100/467875]\n",
            "loss: 0.412130  [145000/467875]\n",
            "loss: 0.379409  [147900/467875]\n",
            "loss: 0.510824  [150800/467875]\n",
            "loss: 0.351965  [153700/467875]\n",
            "loss: 0.419206  [156600/467875]\n",
            "loss: 0.398215  [159500/467875]\n",
            "loss: 0.514642  [162400/467875]\n",
            "loss: 0.466459  [165300/467875]\n",
            "loss: 0.586825  [168200/467875]\n",
            "loss: 0.414838  [171100/467875]\n",
            "loss: 0.331155  [174000/467875]\n",
            "loss: 0.339277  [176900/467875]\n",
            "loss: 0.464641  [179800/467875]\n",
            "loss: 0.346248  [182700/467875]\n",
            "loss: 0.499884  [185600/467875]\n",
            "loss: 0.322364  [188500/467875]\n",
            "loss: 0.431568  [191400/467875]\n",
            "loss: 0.466238  [194300/467875]\n",
            "loss: 0.560599  [197200/467875]\n",
            "loss: 0.455982  [200100/467875]\n",
            "loss: 0.657183  [203000/467875]\n",
            "loss: 0.373824  [205900/467875]\n",
            "loss: 0.597375  [208800/467875]\n",
            "loss: 0.308856  [211700/467875]\n",
            "loss: 0.550555  [214600/467875]\n",
            "loss: 0.464665  [217500/467875]\n",
            "loss: 0.669615  [220400/467875]\n",
            "loss: 0.402236  [223300/467875]\n",
            "loss: 0.327398  [226200/467875]\n",
            "loss: 0.445625  [229100/467875]\n",
            "loss: 0.414391  [232000/467875]\n",
            "loss: 0.570973  [234900/467875]\n",
            "loss: 0.418311  [237800/467875]\n",
            "loss: 0.395540  [240700/467875]\n",
            "loss: 0.447068  [243600/467875]\n",
            "loss: 0.279663  [246500/467875]\n",
            "loss: 0.305892  [249400/467875]\n",
            "loss: 0.380571  [252300/467875]\n",
            "loss: 0.568829  [255200/467875]\n",
            "loss: 0.468252  [258100/467875]\n",
            "loss: 0.415611  [261000/467875]\n",
            "loss: 0.435986  [263900/467875]\n",
            "loss: 0.544058  [266800/467875]\n",
            "loss: 0.562597  [269700/467875]\n",
            "loss: 0.448086  [272600/467875]\n",
            "loss: 0.541388  [275500/467875]\n",
            "loss: 0.407113  [278400/467875]\n",
            "loss: 0.338895  [281300/467875]\n",
            "loss: 0.314742  [284200/467875]\n",
            "loss: 0.315236  [287100/467875]\n",
            "loss: 0.474045  [290000/467875]\n",
            "loss: 0.551149  [292900/467875]\n",
            "loss: 0.397676  [295800/467875]\n",
            "loss: 0.581285  [298700/467875]\n",
            "loss: 0.483296  [301600/467875]\n",
            "loss: 0.552189  [304500/467875]\n",
            "loss: 0.389012  [307400/467875]\n",
            "loss: 0.679493  [310300/467875]\n",
            "loss: 0.254327  [313200/467875]\n",
            "loss: 0.574039  [316100/467875]\n",
            "loss: 0.502393  [319000/467875]\n",
            "loss: 0.497241  [321900/467875]\n",
            "loss: 0.484090  [324800/467875]\n",
            "loss: 0.414167  [327700/467875]\n",
            "loss: 0.434078  [330600/467875]\n",
            "loss: 0.450419  [333500/467875]\n",
            "loss: 0.462705  [336400/467875]\n",
            "loss: 0.591809  [339300/467875]\n",
            "loss: 0.431731  [342200/467875]\n",
            "loss: 0.318155  [345100/467875]\n",
            "loss: 0.466888  [348000/467875]\n",
            "loss: 0.506711  [350900/467875]\n",
            "loss: 0.435718  [353800/467875]\n",
            "loss: 0.313403  [356700/467875]\n",
            "loss: 0.557639  [359600/467875]\n",
            "loss: 0.365731  [362500/467875]\n",
            "loss: 0.372944  [365400/467875]\n",
            "loss: 0.504249  [368300/467875]\n",
            "loss: 0.499042  [371200/467875]\n",
            "loss: 0.426324  [374100/467875]\n",
            "loss: 0.518491  [377000/467875]\n",
            "loss: 0.427185  [379900/467875]\n",
            "loss: 0.510419  [382800/467875]\n",
            "loss: 0.460616  [385700/467875]\n",
            "loss: 0.498141  [388600/467875]\n",
            "loss: 0.579894  [391500/467875]\n",
            "loss: 0.448327  [394400/467875]\n",
            "loss: 0.359052  [397300/467875]\n",
            "loss: 0.588092  [400200/467875]\n",
            "loss: 0.524538  [403100/467875]\n",
            "loss: 0.590712  [406000/467875]\n",
            "loss: 0.353318  [408900/467875]\n",
            "loss: 0.448567  [411800/467875]\n",
            "loss: 0.630911  [414700/467875]\n",
            "loss: 0.391461  [417600/467875]\n",
            "loss: 0.467026  [420500/467875]\n",
            "loss: 0.398421  [423400/467875]\n",
            "loss: 0.401899  [426300/467875]\n",
            "loss: 0.497044  [429200/467875]\n",
            "loss: 0.518220  [432100/467875]\n",
            "loss: 0.449285  [435000/467875]\n",
            "loss: 0.559632  [437900/467875]\n",
            "loss: 0.561417  [440800/467875]\n",
            "loss: 0.349965  [443700/467875]\n",
            "loss: 0.363166  [446600/467875]\n",
            "loss: 0.418998  [449500/467875]\n",
            "loss: 0.414203  [452400/467875]\n",
            "loss: 0.551118  [455300/467875]\n",
            "loss: 0.499231  [458200/467875]\n",
            "loss: 0.532397  [461100/467875]\n",
            "loss: 0.550648  [464000/467875]\n",
            "loss: 0.487464  [466900/467875]\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476186 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.385212  [    0/467875]\n",
            "loss: 0.616474  [ 2900/467875]\n",
            "loss: 0.514504  [ 5800/467875]\n",
            "loss: 0.448526  [ 8700/467875]\n",
            "loss: 0.351853  [11600/467875]\n",
            "loss: 0.473394  [14500/467875]\n",
            "loss: 0.364851  [17400/467875]\n",
            "loss: 0.427910  [20300/467875]\n",
            "loss: 0.502024  [23200/467875]\n",
            "loss: 0.610317  [26100/467875]\n",
            "loss: 0.474495  [29000/467875]\n",
            "loss: 0.691849  [31900/467875]\n",
            "loss: 0.561234  [34800/467875]\n",
            "loss: 0.452411  [37700/467875]\n",
            "loss: 0.597987  [40600/467875]\n",
            "loss: 0.328661  [43500/467875]\n",
            "loss: 0.491483  [46400/467875]\n",
            "loss: 0.489467  [49300/467875]\n",
            "loss: 0.533471  [52200/467875]\n",
            "loss: 0.271847  [55100/467875]\n",
            "loss: 0.435546  [58000/467875]\n",
            "loss: 0.436011  [60900/467875]\n",
            "loss: 0.520769  [63800/467875]\n",
            "loss: 0.562711  [66700/467875]\n",
            "loss: 0.508710  [69600/467875]\n",
            "loss: 0.510772  [72500/467875]\n",
            "loss: 0.476379  [75400/467875]\n",
            "loss: 0.440274  [78300/467875]\n",
            "loss: 0.398677  [81200/467875]\n",
            "loss: 0.393965  [84100/467875]\n",
            "loss: 0.474300  [87000/467875]\n",
            "loss: 0.491437  [89900/467875]\n",
            "loss: 0.479442  [92800/467875]\n",
            "loss: 0.423123  [95700/467875]\n",
            "loss: 0.561844  [98600/467875]\n",
            "loss: 0.393386  [101500/467875]\n",
            "loss: 0.557872  [104400/467875]\n",
            "loss: 0.426473  [107300/467875]\n",
            "loss: 0.463010  [110200/467875]\n",
            "loss: 0.388033  [113100/467875]\n",
            "loss: 0.533840  [116000/467875]\n",
            "loss: 0.372346  [118900/467875]\n",
            "loss: 0.545680  [121800/467875]\n",
            "loss: 0.520778  [124700/467875]\n",
            "loss: 0.416532  [127600/467875]\n",
            "loss: 0.513975  [130500/467875]\n",
            "loss: 0.456654  [133400/467875]\n",
            "loss: 0.605006  [136300/467875]\n",
            "loss: 0.362017  [139200/467875]\n",
            "loss: 0.416117  [142100/467875]\n",
            "loss: 0.433377  [145000/467875]\n",
            "loss: 0.388012  [147900/467875]\n",
            "loss: 0.506954  [150800/467875]\n",
            "loss: 0.593285  [153700/467875]\n",
            "loss: 0.382882  [156600/467875]\n",
            "loss: 0.520638  [159500/467875]\n",
            "loss: 0.324251  [162400/467875]\n",
            "loss: 0.414017  [165300/467875]\n",
            "loss: 0.526740  [168200/467875]\n",
            "loss: 0.723271  [171100/467875]\n",
            "loss: 0.413022  [174000/467875]\n",
            "loss: 0.421069  [176900/467875]\n",
            "loss: 0.427719  [179800/467875]\n",
            "loss: 0.417369  [182700/467875]\n",
            "loss: 0.537213  [185600/467875]\n",
            "loss: 0.465679  [188500/467875]\n",
            "loss: 0.459026  [191400/467875]\n",
            "loss: 0.422578  [194300/467875]\n",
            "loss: 0.412855  [197200/467875]\n",
            "loss: 0.302538  [200100/467875]\n",
            "loss: 0.500182  [203000/467875]\n",
            "loss: 0.450642  [205900/467875]\n",
            "loss: 0.458961  [208800/467875]\n",
            "loss: 0.497829  [211700/467875]\n",
            "loss: 0.554836  [214600/467875]\n",
            "loss: 0.375310  [217500/467875]\n",
            "loss: 0.451605  [220400/467875]\n",
            "loss: 0.485304  [223300/467875]\n",
            "loss: 0.527833  [226200/467875]\n",
            "loss: 0.466082  [229100/467875]\n",
            "loss: 0.447913  [232000/467875]\n",
            "loss: 0.482489  [234900/467875]\n",
            "loss: 0.499966  [237800/467875]\n",
            "loss: 0.344227  [240700/467875]\n",
            "loss: 0.557825  [243600/467875]\n",
            "loss: 0.359653  [246500/467875]\n",
            "loss: 0.578440  [249400/467875]\n",
            "loss: 0.452071  [252300/467875]\n",
            "loss: 0.552057  [255200/467875]\n",
            "loss: 0.396431  [258100/467875]\n",
            "loss: 0.303112  [261000/467875]\n",
            "loss: 0.328553  [263900/467875]\n",
            "loss: 0.639906  [266800/467875]\n",
            "loss: 0.697083  [269700/467875]\n",
            "loss: 0.632097  [272600/467875]\n",
            "loss: 0.315037  [275500/467875]\n",
            "loss: 0.504248  [278400/467875]\n",
            "loss: 0.458359  [281300/467875]\n",
            "loss: 0.663473  [284200/467875]\n",
            "loss: 0.690172  [287100/467875]\n",
            "loss: 0.498361  [290000/467875]\n",
            "loss: 0.404761  [292900/467875]\n",
            "loss: 0.471606  [295800/467875]\n",
            "loss: 0.426659  [298700/467875]\n",
            "loss: 0.335880  [301600/467875]\n",
            "loss: 0.595115  [304500/467875]\n",
            "loss: 0.433129  [307400/467875]\n",
            "loss: 0.460767  [310300/467875]\n",
            "loss: 0.551734  [313200/467875]\n",
            "loss: 0.529281  [316100/467875]\n",
            "loss: 0.497215  [319000/467875]\n",
            "loss: 0.392348  [321900/467875]\n",
            "loss: 0.531818  [324800/467875]\n",
            "loss: 0.382394  [327700/467875]\n",
            "loss: 0.550693  [330600/467875]\n",
            "loss: 0.429364  [333500/467875]\n",
            "loss: 0.434810  [336400/467875]\n",
            "loss: 0.596199  [339300/467875]\n",
            "loss: 0.457203  [342200/467875]\n",
            "loss: 0.536806  [345100/467875]\n",
            "loss: 0.549613  [348000/467875]\n",
            "loss: 0.417670  [350900/467875]\n",
            "loss: 0.418402  [353800/467875]\n",
            "loss: 0.264932  [356700/467875]\n",
            "loss: 0.360060  [359600/467875]\n",
            "loss: 0.695054  [362500/467875]\n",
            "loss: 0.440324  [365400/467875]\n",
            "loss: 0.331822  [368300/467875]\n",
            "loss: 0.601774  [371200/467875]\n",
            "loss: 0.352085  [374100/467875]\n",
            "loss: 0.508398  [377000/467875]\n",
            "loss: 0.452367  [379900/467875]\n",
            "loss: 0.440071  [382800/467875]\n",
            "loss: 0.468410  [385700/467875]\n",
            "loss: 0.350773  [388600/467875]\n",
            "loss: 0.458395  [391500/467875]\n",
            "loss: 0.524099  [394400/467875]\n",
            "loss: 0.288876  [397300/467875]\n",
            "loss: 0.397483  [400200/467875]\n",
            "loss: 0.444798  [403100/467875]\n",
            "loss: 0.425173  [406000/467875]\n",
            "loss: 0.490370  [408900/467875]\n",
            "loss: 0.510976  [411800/467875]\n",
            "loss: 0.600122  [414700/467875]\n",
            "loss: 0.436008  [417600/467875]\n",
            "loss: 0.473256  [420500/467875]\n",
            "loss: 0.461875  [423400/467875]\n",
            "loss: 0.486379  [426300/467875]\n",
            "loss: 0.374510  [429200/467875]\n",
            "loss: 0.524421  [432100/467875]\n",
            "loss: 0.384970  [435000/467875]\n",
            "loss: 0.572728  [437900/467875]\n",
            "loss: 0.523424  [440800/467875]\n",
            "loss: 0.472629  [443700/467875]\n",
            "loss: 0.455949  [446600/467875]\n",
            "loss: 0.232845  [449500/467875]\n",
            "loss: 0.393003  [452400/467875]\n",
            "loss: 0.446645  [455300/467875]\n",
            "loss: 0.461801  [458200/467875]\n",
            "loss: 0.496130  [461100/467875]\n",
            "loss: 0.297122  [464000/467875]\n",
            "loss: 0.484236  [466900/467875]\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476234 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.368927  [    0/467875]\n",
            "loss: 0.531566  [ 2900/467875]\n",
            "loss: 0.371516  [ 5800/467875]\n",
            "loss: 0.464583  [ 8700/467875]\n",
            "loss: 0.469273  [11600/467875]\n",
            "loss: 0.496583  [14500/467875]\n",
            "loss: 0.439758  [17400/467875]\n",
            "loss: 0.510180  [20300/467875]\n",
            "loss: 0.381804  [23200/467875]\n",
            "loss: 0.532305  [26100/467875]\n",
            "loss: 0.407578  [29000/467875]\n",
            "loss: 0.548896  [31900/467875]\n",
            "loss: 0.316401  [34800/467875]\n",
            "loss: 0.506578  [37700/467875]\n",
            "loss: 0.535250  [40600/467875]\n",
            "loss: 0.529942  [43500/467875]\n",
            "loss: 0.463066  [46400/467875]\n",
            "loss: 0.552498  [49300/467875]\n",
            "loss: 0.489244  [52200/467875]\n",
            "loss: 0.478437  [55100/467875]\n",
            "loss: 0.534998  [58000/467875]\n",
            "loss: 0.393028  [60900/467875]\n",
            "loss: 0.579194  [63800/467875]\n",
            "loss: 0.564153  [66700/467875]\n",
            "loss: 0.367095  [69600/467875]\n",
            "loss: 0.524474  [72500/467875]\n",
            "loss: 0.494526  [75400/467875]\n",
            "loss: 0.502209  [78300/467875]\n",
            "loss: 0.531235  [81200/467875]\n",
            "loss: 0.502644  [84100/467875]\n",
            "loss: 0.533490  [87000/467875]\n",
            "loss: 0.358416  [89900/467875]\n",
            "loss: 0.470617  [92800/467875]\n",
            "loss: 0.456435  [95700/467875]\n",
            "loss: 0.437652  [98600/467875]\n",
            "loss: 0.540074  [101500/467875]\n",
            "loss: 0.651206  [104400/467875]\n",
            "loss: 0.421758  [107300/467875]\n",
            "loss: 0.432624  [110200/467875]\n",
            "loss: 0.491018  [113100/467875]\n",
            "loss: 0.275806  [116000/467875]\n",
            "loss: 0.493398  [118900/467875]\n",
            "loss: 0.391901  [121800/467875]\n",
            "loss: 0.622091  [124700/467875]\n",
            "loss: 0.418817  [127600/467875]\n",
            "loss: 0.477386  [130500/467875]\n",
            "loss: 0.468257  [133400/467875]\n",
            "loss: 0.549066  [136300/467875]\n",
            "loss: 0.364774  [139200/467875]\n",
            "loss: 0.575775  [142100/467875]\n",
            "loss: 0.480947  [145000/467875]\n",
            "loss: 0.403679  [147900/467875]\n",
            "loss: 0.658277  [150800/467875]\n",
            "loss: 0.317953  [153700/467875]\n",
            "loss: 0.464114  [156600/467875]\n",
            "loss: 0.577535  [159500/467875]\n",
            "loss: 0.555295  [162400/467875]\n",
            "loss: 0.405288  [165300/467875]\n",
            "loss: 0.389225  [168200/467875]\n",
            "loss: 0.443002  [171100/467875]\n",
            "loss: 0.440379  [174000/467875]\n",
            "loss: 0.452902  [176900/467875]\n",
            "loss: 0.455801  [179800/467875]\n",
            "loss: 0.291048  [182700/467875]\n",
            "loss: 0.520406  [185600/467875]\n",
            "loss: 0.544922  [188500/467875]\n",
            "loss: 0.417156  [191400/467875]\n",
            "loss: 0.431786  [194300/467875]\n",
            "loss: 0.567214  [197200/467875]\n",
            "loss: 0.512663  [200100/467875]\n",
            "loss: 0.310206  [203000/467875]\n",
            "loss: 0.602126  [205900/467875]\n",
            "loss: 0.414048  [208800/467875]\n",
            "loss: 0.419172  [211700/467875]\n",
            "loss: 0.600997  [214600/467875]\n",
            "loss: 0.343514  [217500/467875]\n",
            "loss: 0.692411  [220400/467875]\n",
            "loss: 0.539181  [223300/467875]\n",
            "loss: 0.513342  [226200/467875]\n",
            "loss: 0.499654  [229100/467875]\n",
            "loss: 0.550448  [232000/467875]\n",
            "loss: 0.496922  [234900/467875]\n",
            "loss: 0.465703  [237800/467875]\n",
            "loss: 0.611464  [240700/467875]\n",
            "loss: 0.481072  [243600/467875]\n",
            "loss: 0.612795  [246500/467875]\n",
            "loss: 0.535755  [249400/467875]\n",
            "loss: 0.416359  [252300/467875]\n",
            "loss: 0.418793  [255200/467875]\n",
            "loss: 0.628665  [258100/467875]\n",
            "loss: 0.441374  [261000/467875]\n",
            "loss: 0.505749  [263900/467875]\n",
            "loss: 0.343818  [266800/467875]\n",
            "loss: 0.388801  [269700/467875]\n",
            "loss: 0.354856  [272600/467875]\n",
            "loss: 0.468627  [275500/467875]\n",
            "loss: 0.366640  [278400/467875]\n",
            "loss: 0.530212  [281300/467875]\n",
            "loss: 0.469077  [284200/467875]\n",
            "loss: 0.473682  [287100/467875]\n",
            "loss: 0.466385  [290000/467875]\n",
            "loss: 0.564139  [292900/467875]\n",
            "loss: 0.569047  [295800/467875]\n",
            "loss: 0.429420  [298700/467875]\n",
            "loss: 0.377853  [301600/467875]\n",
            "loss: 0.472499  [304500/467875]\n",
            "loss: 0.339638  [307400/467875]\n",
            "loss: 0.442840  [310300/467875]\n",
            "loss: 0.563755  [313200/467875]\n",
            "loss: 0.316277  [316100/467875]\n",
            "loss: 0.534236  [319000/467875]\n",
            "loss: 0.563302  [321900/467875]\n",
            "loss: 0.446990  [324800/467875]\n",
            "loss: 0.524318  [327700/467875]\n",
            "loss: 0.561926  [330600/467875]\n",
            "loss: 0.563376  [333500/467875]\n",
            "loss: 0.446275  [336400/467875]\n",
            "loss: 0.428731  [339300/467875]\n",
            "loss: 0.534044  [342200/467875]\n",
            "loss: 0.512675  [345100/467875]\n",
            "loss: 0.426420  [348000/467875]\n",
            "loss: 0.334900  [350900/467875]\n",
            "loss: 0.434461  [353800/467875]\n",
            "loss: 0.632979  [356700/467875]\n",
            "loss: 0.686304  [359600/467875]\n",
            "loss: 0.567741  [362500/467875]\n",
            "loss: 0.369854  [365400/467875]\n",
            "loss: 0.517976  [368300/467875]\n",
            "loss: 0.599261  [371200/467875]\n",
            "loss: 0.528014  [374100/467875]\n",
            "loss: 0.475815  [377000/467875]\n",
            "loss: 0.493276  [379900/467875]\n",
            "loss: 0.439421  [382800/467875]\n",
            "loss: 0.571688  [385700/467875]\n",
            "loss: 0.470847  [388600/467875]\n",
            "loss: 0.499363  [391500/467875]\n",
            "loss: 0.775261  [394400/467875]\n",
            "loss: 0.613971  [397300/467875]\n",
            "loss: 0.482429  [400200/467875]\n",
            "loss: 0.498358  [403100/467875]\n",
            "loss: 0.477482  [406000/467875]\n",
            "loss: 0.597536  [408900/467875]\n",
            "loss: 0.471008  [411800/467875]\n",
            "loss: 0.475617  [414700/467875]\n",
            "loss: 0.357646  [417600/467875]\n",
            "loss: 0.457810  [420500/467875]\n",
            "loss: 0.587306  [423400/467875]\n",
            "loss: 0.570396  [426300/467875]\n",
            "loss: 0.389785  [429200/467875]\n",
            "loss: 0.419954  [432100/467875]\n",
            "loss: 0.542023  [435000/467875]\n",
            "loss: 0.384489  [437900/467875]\n",
            "loss: 0.410161  [440800/467875]\n",
            "loss: 0.560528  [443700/467875]\n",
            "loss: 0.589887  [446600/467875]\n",
            "loss: 0.549868  [449500/467875]\n",
            "loss: 0.446535  [452400/467875]\n",
            "loss: 0.571993  [455300/467875]\n",
            "loss: 0.489234  [458200/467875]\n",
            "loss: 0.413526  [461100/467875]\n",
            "loss: 0.501193  [464000/467875]\n",
            "loss: 0.589449  [466900/467875]\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.477373 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.451163  [    0/467875]\n",
            "loss: 0.578380  [ 2900/467875]\n",
            "loss: 0.425714  [ 5800/467875]\n",
            "loss: 0.328126  [ 8700/467875]\n",
            "loss: 0.303024  [11600/467875]\n",
            "loss: 0.513968  [14500/467875]\n",
            "loss: 0.380780  [17400/467875]\n",
            "loss: 0.482154  [20300/467875]\n",
            "loss: 0.444401  [23200/467875]\n",
            "loss: 0.534240  [26100/467875]\n",
            "loss: 0.678744  [29000/467875]\n",
            "loss: 0.563973  [31900/467875]\n",
            "loss: 0.366552  [34800/467875]\n",
            "loss: 0.346672  [37700/467875]\n",
            "loss: 0.444205  [40600/467875]\n",
            "loss: 0.444604  [43500/467875]\n",
            "loss: 0.345554  [46400/467875]\n",
            "loss: 0.567137  [49300/467875]\n",
            "loss: 0.372836  [52200/467875]\n",
            "loss: 0.482134  [55100/467875]\n",
            "loss: 0.507877  [58000/467875]\n",
            "loss: 0.424182  [60900/467875]\n",
            "loss: 0.519378  [63800/467875]\n",
            "loss: 0.403175  [66700/467875]\n",
            "loss: 0.498484  [69600/467875]\n",
            "loss: 0.491281  [72500/467875]\n",
            "loss: 0.478910  [75400/467875]\n",
            "loss: 0.394096  [78300/467875]\n",
            "loss: 0.404791  [81200/467875]\n",
            "loss: 0.458341  [84100/467875]\n",
            "loss: 0.437060  [87000/467875]\n",
            "loss: 0.551170  [89900/467875]\n",
            "loss: 0.473556  [92800/467875]\n",
            "loss: 0.609702  [95700/467875]\n",
            "loss: 0.375389  [98600/467875]\n",
            "loss: 0.552899  [101500/467875]\n",
            "loss: 0.458600  [104400/467875]\n",
            "loss: 0.491632  [107300/467875]\n",
            "loss: 0.590460  [110200/467875]\n",
            "loss: 0.515144  [113100/467875]\n",
            "loss: 0.391358  [116000/467875]\n",
            "loss: 0.403356  [118900/467875]\n",
            "loss: 0.615227  [121800/467875]\n",
            "loss: 0.335397  [124700/467875]\n",
            "loss: 0.532229  [127600/467875]\n",
            "loss: 0.396003  [130500/467875]\n",
            "loss: 0.504539  [133400/467875]\n",
            "loss: 0.472519  [136300/467875]\n",
            "loss: 0.459199  [139200/467875]\n",
            "loss: 0.456799  [142100/467875]\n",
            "loss: 0.464106  [145000/467875]\n",
            "loss: 0.421004  [147900/467875]\n",
            "loss: 0.594775  [150800/467875]\n",
            "loss: 0.586236  [153700/467875]\n",
            "loss: 0.402252  [156600/467875]\n",
            "loss: 0.509165  [159500/467875]\n",
            "loss: 0.488114  [162400/467875]\n",
            "loss: 0.370240  [165300/467875]\n",
            "loss: 0.541875  [168200/467875]\n",
            "loss: 0.514911  [171100/467875]\n",
            "loss: 0.741411  [174000/467875]\n",
            "loss: 0.516338  [176900/467875]\n",
            "loss: 0.667242  [179800/467875]\n",
            "loss: 0.343340  [182700/467875]\n",
            "loss: 0.544787  [185600/467875]\n",
            "loss: 0.618889  [188500/467875]\n",
            "loss: 0.466728  [191400/467875]\n",
            "loss: 0.506389  [194300/467875]\n",
            "loss: 0.284483  [197200/467875]\n",
            "loss: 0.446594  [200100/467875]\n",
            "loss: 0.523939  [203000/467875]\n",
            "loss: 0.409455  [205900/467875]\n",
            "loss: 0.449515  [208800/467875]\n",
            "loss: 0.458803  [211700/467875]\n",
            "loss: 0.417994  [214600/467875]\n",
            "loss: 0.452675  [217500/467875]\n",
            "loss: 0.437381  [220400/467875]\n",
            "loss: 0.490171  [223300/467875]\n",
            "loss: 0.454500  [226200/467875]\n",
            "loss: 0.385078  [229100/467875]\n",
            "loss: 0.439528  [232000/467875]\n",
            "loss: 0.377924  [234900/467875]\n",
            "loss: 0.605355  [237800/467875]\n",
            "loss: 0.393433  [240700/467875]\n",
            "loss: 0.368226  [243600/467875]\n",
            "loss: 0.448022  [246500/467875]\n",
            "loss: 0.328824  [249400/467875]\n",
            "loss: 0.479683  [252300/467875]\n",
            "loss: 0.378450  [255200/467875]\n",
            "loss: 0.515904  [258100/467875]\n",
            "loss: 0.627209  [261000/467875]\n",
            "loss: 0.412059  [263900/467875]\n",
            "loss: 0.502789  [266800/467875]\n",
            "loss: 0.374629  [269700/467875]\n",
            "loss: 0.620266  [272600/467875]\n",
            "loss: 0.407445  [275500/467875]\n",
            "loss: 0.496491  [278400/467875]\n",
            "loss: 0.419613  [281300/467875]\n",
            "loss: 0.655647  [284200/467875]\n",
            "loss: 0.410069  [287100/467875]\n",
            "loss: 0.425527  [290000/467875]\n",
            "loss: 0.565371  [292900/467875]\n",
            "loss: 0.593915  [295800/467875]\n",
            "loss: 0.509401  [298700/467875]\n",
            "loss: 0.472595  [301600/467875]\n",
            "loss: 0.465365  [304500/467875]\n",
            "loss: 0.463085  [307400/467875]\n",
            "loss: 0.579752  [310300/467875]\n",
            "loss: 0.469690  [313200/467875]\n",
            "loss: 0.390756  [316100/467875]\n",
            "loss: 0.632741  [319000/467875]\n",
            "loss: 0.410634  [321900/467875]\n",
            "loss: 0.402541  [324800/467875]\n",
            "loss: 0.443437  [327700/467875]\n",
            "loss: 0.534591  [330600/467875]\n",
            "loss: 0.486414  [333500/467875]\n",
            "loss: 0.416760  [336400/467875]\n",
            "loss: 0.404320  [339300/467875]\n",
            "loss: 0.369910  [342200/467875]\n",
            "loss: 0.482030  [345100/467875]\n",
            "loss: 0.498923  [348000/467875]\n",
            "loss: 0.494057  [350900/467875]\n",
            "loss: 0.387824  [353800/467875]\n",
            "loss: 0.514809  [356700/467875]\n",
            "loss: 0.344098  [359600/467875]\n",
            "loss: 0.548306  [362500/467875]\n",
            "loss: 0.603014  [365400/467875]\n",
            "loss: 0.395931  [368300/467875]\n",
            "loss: 0.389763  [371200/467875]\n",
            "loss: 0.435923  [374100/467875]\n",
            "loss: 0.358272  [377000/467875]\n",
            "loss: 0.538025  [379900/467875]\n",
            "loss: 0.325890  [382800/467875]\n",
            "loss: 0.638361  [385700/467875]\n",
            "loss: 0.401972  [388600/467875]\n",
            "loss: 0.447002  [391500/467875]\n",
            "loss: 0.502355  [394400/467875]\n",
            "loss: 0.502199  [397300/467875]\n",
            "loss: 0.486154  [400200/467875]\n",
            "loss: 0.455464  [403100/467875]\n",
            "loss: 0.463825  [406000/467875]\n",
            "loss: 0.611442  [408900/467875]\n",
            "loss: 0.522983  [411800/467875]\n",
            "loss: 0.328948  [414700/467875]\n",
            "loss: 0.435561  [417600/467875]\n",
            "loss: 0.491756  [420500/467875]\n",
            "loss: 0.439272  [423400/467875]\n",
            "loss: 0.541162  [426300/467875]\n",
            "loss: 0.504950  [429200/467875]\n",
            "loss: 0.469282  [432100/467875]\n",
            "loss: 0.441889  [435000/467875]\n",
            "loss: 0.394669  [437900/467875]\n",
            "loss: 0.443811  [440800/467875]\n",
            "loss: 0.375282  [443700/467875]\n",
            "loss: 0.411980  [446600/467875]\n",
            "loss: 0.352950  [449500/467875]\n",
            "loss: 0.386305  [452400/467875]\n",
            "loss: 0.619847  [455300/467875]\n",
            "loss: 0.544476  [458200/467875]\n",
            "loss: 0.549837  [461100/467875]\n",
            "loss: 0.529106  [464000/467875]\n",
            "loss: 0.500694  [466900/467875]\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476934 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 26 s\n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.446960  [    0/467875]\n",
            "loss: 0.333937  [ 2900/467875]\n",
            "loss: 0.593315  [ 5800/467875]\n",
            "loss: 0.628281  [ 8700/467875]\n",
            "loss: 0.389018  [11600/467875]\n",
            "loss: 0.592899  [14500/467875]\n",
            "loss: 0.493281  [17400/467875]\n",
            "loss: 0.476529  [20300/467875]\n",
            "loss: 0.414289  [23200/467875]\n",
            "loss: 0.587128  [26100/467875]\n",
            "loss: 0.356229  [29000/467875]\n",
            "loss: 0.493605  [31900/467875]\n",
            "loss: 0.589219  [34800/467875]\n",
            "loss: 0.498592  [37700/467875]\n",
            "loss: 0.400721  [40600/467875]\n",
            "loss: 0.327753  [43500/467875]\n",
            "loss: 0.354048  [46400/467875]\n",
            "loss: 0.380607  [49300/467875]\n",
            "loss: 0.431342  [52200/467875]\n",
            "loss: 0.327566  [55100/467875]\n",
            "loss: 0.487859  [58000/467875]\n",
            "loss: 0.512077  [60900/467875]\n",
            "loss: 0.367532  [63800/467875]\n",
            "loss: 0.437015  [66700/467875]\n",
            "loss: 0.458272  [69600/467875]\n",
            "loss: 0.478022  [72500/467875]\n",
            "loss: 0.664026  [75400/467875]\n",
            "loss: 0.519856  [78300/467875]\n",
            "loss: 0.653275  [81200/467875]\n",
            "loss: 0.421764  [84100/467875]\n",
            "loss: 0.519242  [87000/467875]\n",
            "loss: 0.384459  [89900/467875]\n",
            "loss: 0.384168  [92800/467875]\n",
            "loss: 0.405122  [95700/467875]\n",
            "loss: 0.412373  [98600/467875]\n",
            "loss: 0.373988  [101500/467875]\n",
            "loss: 0.433240  [104400/467875]\n",
            "loss: 0.475887  [107300/467875]\n",
            "loss: 0.446283  [110200/467875]\n",
            "loss: 0.370837  [113100/467875]\n",
            "loss: 0.551847  [116000/467875]\n",
            "loss: 0.665250  [118900/467875]\n",
            "loss: 0.413311  [121800/467875]\n",
            "loss: 0.478366  [124700/467875]\n",
            "loss: 0.486091  [127600/467875]\n",
            "loss: 0.511794  [130500/467875]\n",
            "loss: 0.354783  [133400/467875]\n",
            "loss: 0.591933  [136300/467875]\n",
            "loss: 0.456997  [139200/467875]\n",
            "loss: 0.534101  [142100/467875]\n",
            "loss: 0.471257  [145000/467875]\n",
            "loss: 0.454007  [147900/467875]\n",
            "loss: 0.625375  [150800/467875]\n",
            "loss: 0.495290  [153700/467875]\n",
            "loss: 0.399959  [156600/467875]\n",
            "loss: 0.553152  [159500/467875]\n",
            "loss: 0.473370  [162400/467875]\n",
            "loss: 0.439934  [165300/467875]\n",
            "loss: 0.362239  [168200/467875]\n",
            "loss: 0.491530  [171100/467875]\n",
            "loss: 0.449355  [174000/467875]\n",
            "loss: 0.370450  [176900/467875]\n",
            "loss: 0.406045  [179800/467875]\n",
            "loss: 0.547724  [182700/467875]\n",
            "loss: 0.416015  [185600/467875]\n",
            "loss: 0.287329  [188500/467875]\n",
            "loss: 0.344274  [191400/467875]\n",
            "loss: 0.565225  [194300/467875]\n",
            "loss: 0.539081  [197200/467875]\n",
            "loss: 0.495078  [200100/467875]\n",
            "loss: 0.459981  [203000/467875]\n",
            "loss: 0.327616  [205900/467875]\n",
            "loss: 0.353327  [208800/467875]\n",
            "loss: 0.586344  [211700/467875]\n",
            "loss: 0.358512  [214600/467875]\n",
            "loss: 0.505094  [217500/467875]\n",
            "loss: 0.454998  [220400/467875]\n",
            "loss: 0.472556  [223300/467875]\n",
            "loss: 0.532216  [226200/467875]\n",
            "loss: 0.474574  [229100/467875]\n",
            "loss: 0.251337  [232000/467875]\n",
            "loss: 0.487310  [234900/467875]\n",
            "loss: 0.455487  [237800/467875]\n",
            "loss: 0.411064  [240700/467875]\n",
            "loss: 0.383940  [243600/467875]\n",
            "loss: 0.359469  [246500/467875]\n",
            "loss: 0.414357  [249400/467875]\n",
            "loss: 0.539632  [252300/467875]\n",
            "loss: 0.350083  [255200/467875]\n",
            "loss: 0.472526  [258100/467875]\n",
            "loss: 0.408292  [261000/467875]\n",
            "loss: 0.473623  [263900/467875]\n",
            "loss: 0.323729  [266800/467875]\n",
            "loss: 0.540184  [269700/467875]\n",
            "loss: 0.464063  [272600/467875]\n",
            "loss: 0.500198  [275500/467875]\n",
            "loss: 0.551401  [278400/467875]\n",
            "loss: 0.444275  [281300/467875]\n",
            "loss: 0.457743  [284200/467875]\n",
            "loss: 0.371550  [287100/467875]\n",
            "loss: 0.352358  [290000/467875]\n",
            "loss: 0.436802  [292900/467875]\n",
            "loss: 0.534587  [295800/467875]\n",
            "loss: 0.380272  [298700/467875]\n",
            "loss: 0.446591  [301600/467875]\n",
            "loss: 0.452892  [304500/467875]\n",
            "loss: 0.742424  [307400/467875]\n",
            "loss: 0.477581  [310300/467875]\n",
            "loss: 0.293288  [313200/467875]\n",
            "loss: 0.397168  [316100/467875]\n",
            "loss: 0.302438  [319000/467875]\n",
            "loss: 0.480107  [321900/467875]\n",
            "loss: 0.427944  [324800/467875]\n",
            "loss: 0.457652  [327700/467875]\n",
            "loss: 0.510373  [330600/467875]\n",
            "loss: 0.451211  [333500/467875]\n",
            "loss: 0.456981  [336400/467875]\n",
            "loss: 0.464043  [339300/467875]\n",
            "loss: 0.304083  [342200/467875]\n",
            "loss: 0.631949  [345100/467875]\n",
            "loss: 0.440393  [348000/467875]\n",
            "loss: 0.399059  [350900/467875]\n",
            "loss: 0.504813  [353800/467875]\n",
            "loss: 0.401510  [356700/467875]\n",
            "loss: 0.442978  [359600/467875]\n",
            "loss: 0.454903  [362500/467875]\n",
            "loss: 0.496424  [365400/467875]\n",
            "loss: 0.593593  [368300/467875]\n",
            "loss: 0.457138  [371200/467875]\n",
            "loss: 0.380590  [374100/467875]\n",
            "loss: 0.372301  [377000/467875]\n",
            "loss: 0.390266  [379900/467875]\n",
            "loss: 0.383928  [382800/467875]\n",
            "loss: 0.385914  [385700/467875]\n",
            "loss: 0.515853  [388600/467875]\n",
            "loss: 0.442178  [391500/467875]\n",
            "loss: 0.523380  [394400/467875]\n",
            "loss: 0.395961  [397300/467875]\n",
            "loss: 0.378483  [400200/467875]\n",
            "loss: 0.583821  [403100/467875]\n",
            "loss: 0.362386  [406000/467875]\n",
            "loss: 0.205642  [408900/467875]\n",
            "loss: 0.482798  [411800/467875]\n",
            "loss: 0.375942  [414700/467875]\n",
            "loss: 0.618425  [417600/467875]\n",
            "loss: 0.553203  [420500/467875]\n",
            "loss: 0.418431  [423400/467875]\n",
            "loss: 0.523800  [426300/467875]\n",
            "loss: 0.350567  [429200/467875]\n",
            "loss: 0.694134  [432100/467875]\n",
            "loss: 0.324220  [435000/467875]\n",
            "loss: 0.628879  [437900/467875]\n",
            "loss: 0.529495  [440800/467875]\n",
            "loss: 0.469342  [443700/467875]\n",
            "loss: 0.499988  [446600/467875]\n",
            "loss: 0.540086  [449500/467875]\n",
            "loss: 0.420656  [452400/467875]\n",
            "loss: 0.513169  [455300/467875]\n",
            "loss: 0.459728  [458200/467875]\n",
            "loss: 0.543362  [461100/467875]\n",
            "loss: 0.345732  [464000/467875]\n",
            "loss: 0.441322  [466900/467875]\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476247 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.443199  [    0/467875]\n",
            "loss: 0.519040  [ 2900/467875]\n",
            "loss: 0.362081  [ 5800/467875]\n",
            "loss: 0.313474  [ 8700/467875]\n",
            "loss: 0.820654  [11600/467875]\n",
            "loss: 0.419380  [14500/467875]\n",
            "loss: 0.550778  [17400/467875]\n",
            "loss: 0.561818  [20300/467875]\n",
            "loss: 0.497565  [23200/467875]\n",
            "loss: 0.546625  [26100/467875]\n",
            "loss: 0.292770  [29000/467875]\n",
            "loss: 0.505798  [31900/467875]\n",
            "loss: 0.494188  [34800/467875]\n",
            "loss: 0.510790  [37700/467875]\n",
            "loss: 0.511117  [40600/467875]\n",
            "loss: 0.522638  [43500/467875]\n",
            "loss: 0.568215  [46400/467875]\n",
            "loss: 0.514600  [49300/467875]\n",
            "loss: 0.575034  [52200/467875]\n",
            "loss: 0.336969  [55100/467875]\n",
            "loss: 0.600248  [58000/467875]\n",
            "loss: 0.344407  [60900/467875]\n",
            "loss: 0.540334  [63800/467875]\n",
            "loss: 0.330859  [66700/467875]\n",
            "loss: 0.490850  [69600/467875]\n",
            "loss: 0.279434  [72500/467875]\n",
            "loss: 0.574364  [75400/467875]\n",
            "loss: 0.593666  [78300/467875]\n",
            "loss: 0.473334  [81200/467875]\n",
            "loss: 0.454704  [84100/467875]\n",
            "loss: 0.445507  [87000/467875]\n",
            "loss: 0.283243  [89900/467875]\n",
            "loss: 0.411095  [92800/467875]\n",
            "loss: 0.617043  [95700/467875]\n",
            "loss: 0.509635  [98600/467875]\n",
            "loss: 0.413622  [101500/467875]\n",
            "loss: 0.518837  [104400/467875]\n",
            "loss: 0.510786  [107300/467875]\n",
            "loss: 0.472231  [110200/467875]\n",
            "loss: 0.295585  [113100/467875]\n",
            "loss: 0.432730  [116000/467875]\n",
            "loss: 0.467116  [118900/467875]\n",
            "loss: 0.409787  [121800/467875]\n",
            "loss: 0.458567  [124700/467875]\n",
            "loss: 0.574819  [127600/467875]\n",
            "loss: 0.629174  [130500/467875]\n",
            "loss: 0.436729  [133400/467875]\n",
            "loss: 0.363859  [136300/467875]\n",
            "loss: 0.452383  [139200/467875]\n",
            "loss: 0.433071  [142100/467875]\n",
            "loss: 0.612085  [145000/467875]\n",
            "loss: 0.442552  [147900/467875]\n",
            "loss: 0.471985  [150800/467875]\n",
            "loss: 0.422746  [153700/467875]\n",
            "loss: 0.418002  [156600/467875]\n",
            "loss: 0.377205  [159500/467875]\n",
            "loss: 0.513325  [162400/467875]\n",
            "loss: 0.485696  [165300/467875]\n",
            "loss: 0.310672  [168200/467875]\n",
            "loss: 0.510929  [171100/467875]\n",
            "loss: 0.248840  [174000/467875]\n",
            "loss: 0.441734  [176900/467875]\n",
            "loss: 0.437615  [179800/467875]\n",
            "loss: 0.432934  [182700/467875]\n",
            "loss: 0.502574  [185600/467875]\n",
            "loss: 0.496559  [188500/467875]\n",
            "loss: 0.373251  [191400/467875]\n",
            "loss: 0.375818  [194300/467875]\n",
            "loss: 0.309519  [197200/467875]\n",
            "loss: 0.526387  [200100/467875]\n",
            "loss: 0.562939  [203000/467875]\n",
            "loss: 0.348844  [205900/467875]\n",
            "loss: 0.510875  [208800/467875]\n",
            "loss: 0.513849  [211700/467875]\n",
            "loss: 0.469418  [214600/467875]\n",
            "loss: 0.540516  [217500/467875]\n",
            "loss: 0.572324  [220400/467875]\n",
            "loss: 0.345250  [223300/467875]\n",
            "loss: 0.405321  [226200/467875]\n",
            "loss: 0.388075  [229100/467875]\n",
            "loss: 0.354149  [232000/467875]\n",
            "loss: 0.533629  [234900/467875]\n",
            "loss: 0.484723  [237800/467875]\n",
            "loss: 0.352049  [240700/467875]\n",
            "loss: 0.456964  [243600/467875]\n",
            "loss: 0.617012  [246500/467875]\n",
            "loss: 0.570768  [249400/467875]\n",
            "loss: 0.279173  [252300/467875]\n",
            "loss: 0.397550  [255200/467875]\n",
            "loss: 0.330436  [258100/467875]\n",
            "loss: 0.445888  [261000/467875]\n",
            "loss: 0.462744  [263900/467875]\n",
            "loss: 0.638125  [266800/467875]\n",
            "loss: 0.298843  [269700/467875]\n",
            "loss: 0.460440  [272600/467875]\n",
            "loss: 0.432785  [275500/467875]\n",
            "loss: 0.529174  [278400/467875]\n",
            "loss: 0.506030  [281300/467875]\n",
            "loss: 0.504308  [284200/467875]\n",
            "loss: 0.533160  [287100/467875]\n",
            "loss: 0.503849  [290000/467875]\n",
            "loss: 0.283977  [292900/467875]\n",
            "loss: 0.570008  [295800/467875]\n",
            "loss: 0.399442  [298700/467875]\n",
            "loss: 0.509736  [301600/467875]\n",
            "loss: 0.523185  [304500/467875]\n",
            "loss: 0.442090  [307400/467875]\n",
            "loss: 0.560664  [310300/467875]\n",
            "loss: 0.415361  [313200/467875]\n",
            "loss: 0.405544  [316100/467875]\n",
            "loss: 0.467210  [319000/467875]\n",
            "loss: 0.612854  [321900/467875]\n",
            "loss: 0.593837  [324800/467875]\n",
            "loss: 0.550994  [327700/467875]\n",
            "loss: 0.574628  [330600/467875]\n",
            "loss: 0.657064  [333500/467875]\n",
            "loss: 0.511381  [336400/467875]\n",
            "loss: 0.540121  [339300/467875]\n",
            "loss: 0.466479  [342200/467875]\n",
            "loss: 0.532056  [345100/467875]\n",
            "loss: 0.463989  [348000/467875]\n",
            "loss: 0.702255  [350900/467875]\n",
            "loss: 0.526674  [353800/467875]\n",
            "loss: 0.473470  [356700/467875]\n",
            "loss: 0.442553  [359600/467875]\n",
            "loss: 0.341190  [362500/467875]\n",
            "loss: 0.541563  [365400/467875]\n",
            "loss: 0.487860  [368300/467875]\n",
            "loss: 0.299685  [371200/467875]\n",
            "loss: 0.466277  [374100/467875]\n",
            "loss: 0.379777  [377000/467875]\n",
            "loss: 0.548280  [379900/467875]\n",
            "loss: 0.494627  [382800/467875]\n",
            "loss: 0.513047  [385700/467875]\n",
            "loss: 0.337657  [388600/467875]\n",
            "loss: 0.509180  [391500/467875]\n",
            "loss: 0.603447  [394400/467875]\n",
            "loss: 0.372603  [397300/467875]\n",
            "loss: 0.411364  [400200/467875]\n",
            "loss: 0.352523  [403100/467875]\n",
            "loss: 0.669269  [406000/467875]\n",
            "loss: 0.610404  [408900/467875]\n",
            "loss: 0.424787  [411800/467875]\n",
            "loss: 0.349144  [414700/467875]\n",
            "loss: 0.428695  [417600/467875]\n",
            "loss: 0.477512  [420500/467875]\n",
            "loss: 0.542411  [423400/467875]\n",
            "loss: 0.524882  [426300/467875]\n",
            "loss: 0.420457  [429200/467875]\n",
            "loss: 0.494638  [432100/467875]\n",
            "loss: 0.277254  [435000/467875]\n",
            "loss: 0.355243  [437900/467875]\n",
            "loss: 0.419211  [440800/467875]\n",
            "loss: 0.552471  [443700/467875]\n",
            "loss: 0.278634  [446600/467875]\n",
            "loss: 0.334764  [449500/467875]\n",
            "loss: 0.542744  [452400/467875]\n",
            "loss: 0.382818  [455300/467875]\n",
            "loss: 0.407590  [458200/467875]\n",
            "loss: 0.469685  [461100/467875]\n",
            "loss: 0.428007  [464000/467875]\n",
            "loss: 0.404950  [466900/467875]\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476060 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 26 s\n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.320037  [    0/467875]\n",
            "loss: 0.589934  [ 2900/467875]\n",
            "loss: 0.339747  [ 5800/467875]\n",
            "loss: 0.337505  [ 8700/467875]\n",
            "loss: 0.787986  [11600/467875]\n",
            "loss: 0.388179  [14500/467875]\n",
            "loss: 0.351117  [17400/467875]\n",
            "loss: 0.459412  [20300/467875]\n",
            "loss: 0.352652  [23200/467875]\n",
            "loss: 0.454616  [26100/467875]\n",
            "loss: 0.408830  [29000/467875]\n",
            "loss: 0.497599  [31900/467875]\n",
            "loss: 0.507195  [34800/467875]\n",
            "loss: 0.492206  [37700/467875]\n",
            "loss: 0.411284  [40600/467875]\n",
            "loss: 0.565138  [43500/467875]\n",
            "loss: 0.500174  [46400/467875]\n",
            "loss: 0.639474  [49300/467875]\n",
            "loss: 0.475211  [52200/467875]\n",
            "loss: 0.666956  [55100/467875]\n",
            "loss: 0.485726  [58000/467875]\n",
            "loss: 0.624023  [60900/467875]\n",
            "loss: 0.493971  [63800/467875]\n",
            "loss: 0.452035  [66700/467875]\n",
            "loss: 0.472654  [69600/467875]\n",
            "loss: 0.432860  [72500/467875]\n",
            "loss: 0.358403  [75400/467875]\n",
            "loss: 0.517228  [78300/467875]\n",
            "loss: 0.605059  [81200/467875]\n",
            "loss: 0.654816  [84100/467875]\n",
            "loss: 0.493610  [87000/467875]\n",
            "loss: 0.521899  [89900/467875]\n",
            "loss: 0.310348  [92800/467875]\n",
            "loss: 0.369929  [95700/467875]\n",
            "loss: 0.576895  [98600/467875]\n",
            "loss: 0.329174  [101500/467875]\n",
            "loss: 0.568550  [104400/467875]\n",
            "loss: 0.536987  [107300/467875]\n",
            "loss: 0.638635  [110200/467875]\n",
            "loss: 0.515365  [113100/467875]\n",
            "loss: 0.528474  [116000/467875]\n",
            "loss: 0.391286  [118900/467875]\n",
            "loss: 0.390728  [121800/467875]\n",
            "loss: 0.435205  [124700/467875]\n",
            "loss: 0.493237  [127600/467875]\n",
            "loss: 0.425760  [130500/467875]\n",
            "loss: 0.304182  [133400/467875]\n",
            "loss: 0.443304  [136300/467875]\n",
            "loss: 0.472240  [139200/467875]\n",
            "loss: 0.465014  [142100/467875]\n",
            "loss: 0.356481  [145000/467875]\n",
            "loss: 0.470674  [147900/467875]\n",
            "loss: 0.455006  [150800/467875]\n",
            "loss: 0.384969  [153700/467875]\n",
            "loss: 0.534314  [156600/467875]\n",
            "loss: 0.611374  [159500/467875]\n",
            "loss: 0.583042  [162400/467875]\n",
            "loss: 0.646901  [165300/467875]\n",
            "loss: 0.403801  [168200/467875]\n",
            "loss: 0.352729  [171100/467875]\n",
            "loss: 0.612185  [174000/467875]\n",
            "loss: 0.474717  [176900/467875]\n",
            "loss: 0.572283  [179800/467875]\n",
            "loss: 0.387642  [182700/467875]\n",
            "loss: 0.415378  [185600/467875]\n",
            "loss: 0.416797  [188500/467875]\n",
            "loss: 0.557900  [191400/467875]\n",
            "loss: 0.392928  [194300/467875]\n",
            "loss: 0.661094  [197200/467875]\n",
            "loss: 0.655864  [200100/467875]\n",
            "loss: 0.393053  [203000/467875]\n",
            "loss: 0.423650  [205900/467875]\n",
            "loss: 0.489645  [208800/467875]\n",
            "loss: 0.577857  [211700/467875]\n",
            "loss: 0.576509  [214600/467875]\n",
            "loss: 0.365905  [217500/467875]\n",
            "loss: 0.438565  [220400/467875]\n",
            "loss: 0.408094  [223300/467875]\n",
            "loss: 0.498874  [226200/467875]\n",
            "loss: 0.644182  [229100/467875]\n",
            "loss: 0.320053  [232000/467875]\n",
            "loss: 0.484377  [234900/467875]\n",
            "loss: 0.451010  [237800/467875]\n",
            "loss: 0.398123  [240700/467875]\n",
            "loss: 0.478295  [243600/467875]\n",
            "loss: 0.549195  [246500/467875]\n",
            "loss: 0.582413  [249400/467875]\n",
            "loss: 0.498041  [252300/467875]\n",
            "loss: 0.612313  [255200/467875]\n",
            "loss: 0.665393  [258100/467875]\n",
            "loss: 0.513633  [261000/467875]\n",
            "loss: 0.515295  [263900/467875]\n",
            "loss: 0.516605  [266800/467875]\n",
            "loss: 0.491870  [269700/467875]\n",
            "loss: 0.556781  [272600/467875]\n",
            "loss: 0.423117  [275500/467875]\n",
            "loss: 0.612943  [278400/467875]\n",
            "loss: 0.408905  [281300/467875]\n",
            "loss: 0.635001  [284200/467875]\n",
            "loss: 0.471461  [287100/467875]\n",
            "loss: 0.457438  [290000/467875]\n",
            "loss: 0.547860  [292900/467875]\n",
            "loss: 0.456084  [295800/467875]\n",
            "loss: 0.414299  [298700/467875]\n",
            "loss: 0.477168  [301600/467875]\n",
            "loss: 0.451584  [304500/467875]\n",
            "loss: 0.630766  [307400/467875]\n",
            "loss: 0.401075  [310300/467875]\n",
            "loss: 0.480740  [313200/467875]\n",
            "loss: 0.372678  [316100/467875]\n",
            "loss: 0.376554  [319000/467875]\n",
            "loss: 0.563885  [321900/467875]\n",
            "loss: 0.511733  [324800/467875]\n",
            "loss: 0.545135  [327700/467875]\n",
            "loss: 0.300316  [330600/467875]\n",
            "loss: 0.410308  [333500/467875]\n",
            "loss: 0.462264  [336400/467875]\n",
            "loss: 0.634727  [339300/467875]\n",
            "loss: 0.414440  [342200/467875]\n",
            "loss: 0.386992  [345100/467875]\n",
            "loss: 0.426165  [348000/467875]\n",
            "loss: 0.351626  [350900/467875]\n",
            "loss: 0.411156  [353800/467875]\n",
            "loss: 0.458917  [356700/467875]\n",
            "loss: 0.499028  [359600/467875]\n",
            "loss: 0.584897  [362500/467875]\n",
            "loss: 0.486769  [365400/467875]\n",
            "loss: 0.652782  [368300/467875]\n",
            "loss: 0.523987  [371200/467875]\n",
            "loss: 0.533165  [374100/467875]\n",
            "loss: 0.501397  [377000/467875]\n",
            "loss: 0.570087  [379900/467875]\n",
            "loss: 0.510228  [382800/467875]\n",
            "loss: 0.498251  [385700/467875]\n",
            "loss: 0.428271  [388600/467875]\n",
            "loss: 0.335809  [391500/467875]\n",
            "loss: 0.493609  [394400/467875]\n",
            "loss: 0.385101  [397300/467875]\n",
            "loss: 0.478538  [400200/467875]\n",
            "loss: 0.368826  [403100/467875]\n",
            "loss: 0.409726  [406000/467875]\n",
            "loss: 0.619983  [408900/467875]\n",
            "loss: 0.438582  [411800/467875]\n",
            "loss: 0.394829  [414700/467875]\n",
            "loss: 0.408418  [417600/467875]\n",
            "loss: 0.498715  [420500/467875]\n",
            "loss: 0.476799  [423400/467875]\n",
            "loss: 0.502164  [426300/467875]\n",
            "loss: 0.412562  [429200/467875]\n",
            "loss: 0.640007  [432100/467875]\n",
            "loss: 0.617543  [435000/467875]\n",
            "loss: 0.472857  [437900/467875]\n",
            "loss: 0.490210  [440800/467875]\n",
            "loss: 0.397644  [443700/467875]\n",
            "loss: 0.236382  [446600/467875]\n",
            "loss: 0.419859  [449500/467875]\n",
            "loss: 0.421998  [452400/467875]\n",
            "loss: 0.404001  [455300/467875]\n",
            "loss: 0.545129  [458200/467875]\n",
            "loss: 0.670574  [461100/467875]\n",
            "loss: 0.294480  [464000/467875]\n",
            "loss: 0.410814  [466900/467875]\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.477813 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.495567  [    0/467875]\n",
            "loss: 0.373249  [ 2900/467875]\n",
            "loss: 0.461349  [ 5800/467875]\n",
            "loss: 0.394786  [ 8700/467875]\n",
            "loss: 0.461308  [11600/467875]\n",
            "loss: 0.537123  [14500/467875]\n",
            "loss: 0.401221  [17400/467875]\n",
            "loss: 0.298875  [20300/467875]\n",
            "loss: 0.406785  [23200/467875]\n",
            "loss: 0.524989  [26100/467875]\n",
            "loss: 0.610393  [29000/467875]\n",
            "loss: 0.481598  [31900/467875]\n",
            "loss: 0.418537  [34800/467875]\n",
            "loss: 0.499950  [37700/467875]\n",
            "loss: 0.404107  [40600/467875]\n",
            "loss: 0.329976  [43500/467875]\n",
            "loss: 0.514993  [46400/467875]\n",
            "loss: 0.287974  [49300/467875]\n",
            "loss: 0.413557  [52200/467875]\n",
            "loss: 0.405285  [55100/467875]\n",
            "loss: 0.551740  [58000/467875]\n",
            "loss: 0.496856  [60900/467875]\n",
            "loss: 0.357982  [63800/467875]\n",
            "loss: 0.387584  [66700/467875]\n",
            "loss: 0.612561  [69600/467875]\n",
            "loss: 0.330562  [72500/467875]\n",
            "loss: 0.498497  [75400/467875]\n",
            "loss: 0.468074  [78300/467875]\n",
            "loss: 0.500838  [81200/467875]\n",
            "loss: 0.694389  [84100/467875]\n",
            "loss: 0.617449  [87000/467875]\n",
            "loss: 0.575642  [89900/467875]\n",
            "loss: 0.449730  [92800/467875]\n",
            "loss: 0.351610  [95700/467875]\n",
            "loss: 0.373813  [98600/467875]\n",
            "loss: 0.527014  [101500/467875]\n",
            "loss: 0.329586  [104400/467875]\n",
            "loss: 0.362047  [107300/467875]\n",
            "loss: 0.513947  [110200/467875]\n",
            "loss: 0.468378  [113100/467875]\n",
            "loss: 0.500080  [116000/467875]\n",
            "loss: 0.670478  [118900/467875]\n",
            "loss: 0.425712  [121800/467875]\n",
            "loss: 0.539241  [124700/467875]\n",
            "loss: 0.370561  [127600/467875]\n",
            "loss: 0.419209  [130500/467875]\n",
            "loss: 0.678078  [133400/467875]\n",
            "loss: 0.492629  [136300/467875]\n",
            "loss: 0.398521  [139200/467875]\n",
            "loss: 0.439420  [142100/467875]\n",
            "loss: 0.275863  [145000/467875]\n",
            "loss: 0.470432  [147900/467875]\n",
            "loss: 0.479929  [150800/467875]\n",
            "loss: 0.522329  [153700/467875]\n",
            "loss: 0.506320  [156600/467875]\n",
            "loss: 0.585915  [159500/467875]\n",
            "loss: 0.397825  [162400/467875]\n",
            "loss: 0.359252  [165300/467875]\n",
            "loss: 0.597548  [168200/467875]\n",
            "loss: 0.548751  [171100/467875]\n",
            "loss: 0.394392  [174000/467875]\n",
            "loss: 0.440352  [176900/467875]\n",
            "loss: 0.490282  [179800/467875]\n",
            "loss: 0.448859  [182700/467875]\n",
            "loss: 0.661360  [185600/467875]\n",
            "loss: 0.530489  [188500/467875]\n",
            "loss: 0.404676  [191400/467875]\n",
            "loss: 0.428119  [194300/467875]\n",
            "loss: 0.544820  [197200/467875]\n",
            "loss: 0.449782  [200100/467875]\n",
            "loss: 0.594896  [203000/467875]\n",
            "loss: 0.277133  [205900/467875]\n",
            "loss: 0.413783  [208800/467875]\n",
            "loss: 0.466198  [211700/467875]\n",
            "loss: 0.337476  [214600/467875]\n",
            "loss: 0.285594  [217500/467875]\n",
            "loss: 0.486052  [220400/467875]\n",
            "loss: 0.435073  [223300/467875]\n",
            "loss: 0.424151  [226200/467875]\n",
            "loss: 0.508976  [229100/467875]\n",
            "loss: 0.317814  [232000/467875]\n",
            "loss: 0.489152  [234900/467875]\n",
            "loss: 0.550800  [237800/467875]\n",
            "loss: 0.558388  [240700/467875]\n",
            "loss: 0.472943  [243600/467875]\n",
            "loss: 0.599135  [246500/467875]\n",
            "loss: 0.405661  [249400/467875]\n",
            "loss: 0.338626  [252300/467875]\n",
            "loss: 0.474757  [255200/467875]\n",
            "loss: 0.590812  [258100/467875]\n",
            "loss: 0.469565  [261000/467875]\n",
            "loss: 0.624209  [263900/467875]\n",
            "loss: 0.646227  [266800/467875]\n",
            "loss: 0.439774  [269700/467875]\n",
            "loss: 0.477615  [272600/467875]\n",
            "loss: 0.304302  [275500/467875]\n",
            "loss: 0.496561  [278400/467875]\n",
            "loss: 0.537974  [281300/467875]\n",
            "loss: 0.428244  [284200/467875]\n",
            "loss: 0.517433  [287100/467875]\n",
            "loss: 0.511667  [290000/467875]\n",
            "loss: 0.476657  [292900/467875]\n",
            "loss: 0.557333  [295800/467875]\n",
            "loss: 0.381831  [298700/467875]\n",
            "loss: 0.602933  [301600/467875]\n",
            "loss: 0.521335  [304500/467875]\n",
            "loss: 0.346561  [307400/467875]\n",
            "loss: 0.531579  [310300/467875]\n",
            "loss: 0.492320  [313200/467875]\n",
            "loss: 0.482965  [316100/467875]\n",
            "loss: 0.442158  [319000/467875]\n",
            "loss: 0.633619  [321900/467875]\n",
            "loss: 0.557655  [324800/467875]\n",
            "loss: 0.499996  [327700/467875]\n",
            "loss: 0.533829  [330600/467875]\n",
            "loss: 0.428964  [333500/467875]\n",
            "loss: 0.413480  [336400/467875]\n",
            "loss: 0.466904  [339300/467875]\n",
            "loss: 0.395112  [342200/467875]\n",
            "loss: 0.347837  [345100/467875]\n",
            "loss: 0.476308  [348000/467875]\n",
            "loss: 0.526772  [350900/467875]\n",
            "loss: 0.332939  [353800/467875]\n",
            "loss: 0.544888  [356700/467875]\n",
            "loss: 0.416220  [359600/467875]\n",
            "loss: 0.418613  [362500/467875]\n",
            "loss: 0.357465  [365400/467875]\n",
            "loss: 0.538861  [368300/467875]\n",
            "loss: 0.636321  [371200/467875]\n",
            "loss: 0.437864  [374100/467875]\n",
            "loss: 0.470226  [377000/467875]\n",
            "loss: 0.418635  [379900/467875]\n",
            "loss: 0.374080  [382800/467875]\n",
            "loss: 0.545406  [385700/467875]\n",
            "loss: 0.453870  [388600/467875]\n",
            "loss: 0.459610  [391500/467875]\n",
            "loss: 0.415134  [394400/467875]\n",
            "loss: 0.319361  [397300/467875]\n",
            "loss: 0.522027  [400200/467875]\n",
            "loss: 0.439543  [403100/467875]\n",
            "loss: 0.629142  [406000/467875]\n",
            "loss: 0.413037  [408900/467875]\n",
            "loss: 0.502788  [411800/467875]\n",
            "loss: 0.423328  [414700/467875]\n",
            "loss: 0.479904  [417600/467875]\n",
            "loss: 0.385619  [420500/467875]\n",
            "loss: 0.597999  [423400/467875]\n",
            "loss: 0.520825  [426300/467875]\n",
            "loss: 0.728951  [429200/467875]\n",
            "loss: 0.489009  [432100/467875]\n",
            "loss: 0.416644  [435000/467875]\n",
            "loss: 0.565675  [437900/467875]\n",
            "loss: 0.367298  [440800/467875]\n",
            "loss: 0.634944  [443700/467875]\n",
            "loss: 0.463767  [446600/467875]\n",
            "loss: 0.504499  [449500/467875]\n",
            "loss: 0.372551  [452400/467875]\n",
            "loss: 0.436665  [455300/467875]\n",
            "loss: 0.497200  [458200/467875]\n",
            "loss: 0.588108  [461100/467875]\n",
            "loss: 0.373949  [464000/467875]\n",
            "loss: 0.621391  [466900/467875]\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.476530 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.503895  [    0/467875]\n",
            "loss: 0.496961  [ 2900/467875]\n",
            "loss: 0.354754  [ 5800/467875]\n",
            "loss: 0.495340  [ 8700/467875]\n",
            "loss: 0.508115  [11600/467875]\n",
            "loss: 0.552069  [14500/467875]\n",
            "loss: 0.647860  [17400/467875]\n",
            "loss: 0.488222  [20300/467875]\n",
            "loss: 0.622344  [23200/467875]\n",
            "loss: 0.511303  [26100/467875]\n",
            "loss: 0.345428  [29000/467875]\n",
            "loss: 0.489092  [31900/467875]\n",
            "loss: 0.458074  [34800/467875]\n",
            "loss: 0.384935  [37700/467875]\n",
            "loss: 0.477992  [40600/467875]\n",
            "loss: 0.594088  [43500/467875]\n",
            "loss: 0.487677  [46400/467875]\n",
            "loss: 0.472230  [49300/467875]\n",
            "loss: 0.417498  [52200/467875]\n",
            "loss: 0.364183  [55100/467875]\n",
            "loss: 0.363348  [58000/467875]\n",
            "loss: 0.490409  [60900/467875]\n",
            "loss: 0.374459  [63800/467875]\n",
            "loss: 0.493753  [66700/467875]\n",
            "loss: 0.471861  [69600/467875]\n",
            "loss: 0.462792  [72500/467875]\n",
            "loss: 0.511651  [75400/467875]\n",
            "loss: 0.414295  [78300/467875]\n",
            "loss: 0.472098  [81200/467875]\n",
            "loss: 0.504279  [84100/467875]\n",
            "loss: 0.410148  [87000/467875]\n",
            "loss: 0.382364  [89900/467875]\n",
            "loss: 0.304663  [92800/467875]\n",
            "loss: 0.474029  [95700/467875]\n",
            "loss: 0.455858  [98600/467875]\n",
            "loss: 0.477345  [101500/467875]\n",
            "loss: 0.463388  [104400/467875]\n",
            "loss: 0.451636  [107300/467875]\n",
            "loss: 0.457240  [110200/467875]\n",
            "loss: 0.326689  [113100/467875]\n",
            "loss: 0.473926  [116000/467875]\n",
            "loss: 0.521870  [118900/467875]\n",
            "loss: 0.452757  [121800/467875]\n",
            "loss: 0.308705  [124700/467875]\n",
            "loss: 0.446828  [127600/467875]\n",
            "loss: 0.512953  [130500/467875]\n",
            "loss: 0.403130  [133400/467875]\n",
            "loss: 0.475732  [136300/467875]\n",
            "loss: 0.627902  [139200/467875]\n",
            "loss: 0.434896  [142100/467875]\n",
            "loss: 0.469228  [145000/467875]\n",
            "loss: 0.343111  [147900/467875]\n",
            "loss: 0.576788  [150800/467875]\n",
            "loss: 0.385383  [153700/467875]\n",
            "loss: 0.563815  [156600/467875]\n",
            "loss: 0.423642  [159500/467875]\n",
            "loss: 0.387260  [162400/467875]\n",
            "loss: 0.480203  [165300/467875]\n",
            "loss: 0.393868  [168200/467875]\n",
            "loss: 0.407860  [171100/467875]\n",
            "loss: 0.635229  [174000/467875]\n",
            "loss: 0.410782  [176900/467875]\n",
            "loss: 0.499327  [179800/467875]\n",
            "loss: 0.420332  [182700/467875]\n",
            "loss: 0.393762  [185600/467875]\n",
            "loss: 0.531806  [188500/467875]\n",
            "loss: 0.693855  [191400/467875]\n",
            "loss: 0.356050  [194300/467875]\n",
            "loss: 0.265335  [197200/467875]\n",
            "loss: 0.516893  [200100/467875]\n",
            "loss: 0.188038  [203000/467875]\n",
            "loss: 0.489938  [205900/467875]\n",
            "loss: 0.316212  [208800/467875]\n",
            "loss: 0.387696  [211700/467875]\n",
            "loss: 0.403638  [214600/467875]\n",
            "loss: 0.434041  [217500/467875]\n",
            "loss: 0.385920  [220400/467875]\n",
            "loss: 0.430074  [223300/467875]\n",
            "loss: 0.630495  [226200/467875]\n",
            "loss: 0.508526  [229100/467875]\n",
            "loss: 0.451223  [232000/467875]\n",
            "loss: 0.595346  [234900/467875]\n",
            "loss: 0.531724  [237800/467875]\n",
            "loss: 0.640706  [240700/467875]\n",
            "loss: 0.358547  [243600/467875]\n",
            "loss: 0.478248  [246500/467875]\n",
            "loss: 0.563769  [249400/467875]\n",
            "loss: 0.599936  [252300/467875]\n",
            "loss: 0.419269  [255200/467875]\n",
            "loss: 0.414701  [258100/467875]\n",
            "loss: 0.524855  [261000/467875]\n",
            "loss: 0.555648  [263900/467875]\n",
            "loss: 0.442304  [266800/467875]\n",
            "loss: 0.442672  [269700/467875]\n",
            "loss: 0.452050  [272600/467875]\n",
            "loss: 0.581795  [275500/467875]\n",
            "loss: 0.420202  [278400/467875]\n",
            "loss: 0.614280  [281300/467875]\n",
            "loss: 0.568685  [284200/467875]\n",
            "loss: 0.372714  [287100/467875]\n",
            "loss: 0.726038  [290000/467875]\n",
            "loss: 0.570567  [292900/467875]\n",
            "loss: 0.339889  [295800/467875]\n",
            "loss: 0.386908  [298700/467875]\n",
            "loss: 0.389823  [301600/467875]\n",
            "loss: 0.358307  [304500/467875]\n",
            "loss: 0.423043  [307400/467875]\n",
            "loss: 0.501645  [310300/467875]\n",
            "loss: 0.566601  [313200/467875]\n",
            "loss: 0.396809  [316100/467875]\n",
            "loss: 0.396485  [319000/467875]\n",
            "loss: 0.547212  [321900/467875]\n",
            "loss: 0.569729  [324800/467875]\n",
            "loss: 0.653808  [327700/467875]\n",
            "loss: 0.420115  [330600/467875]\n",
            "loss: 0.445806  [333500/467875]\n",
            "loss: 0.404589  [336400/467875]\n",
            "loss: 0.405624  [339300/467875]\n",
            "loss: 0.663424  [342200/467875]\n",
            "loss: 0.346059  [345100/467875]\n",
            "loss: 0.440687  [348000/467875]\n",
            "loss: 0.538605  [350900/467875]\n",
            "loss: 0.352445  [353800/467875]\n",
            "loss: 0.388787  [356700/467875]\n",
            "loss: 0.514371  [359600/467875]\n",
            "loss: 0.430728  [362500/467875]\n",
            "loss: 0.472142  [365400/467875]\n",
            "loss: 0.575769  [368300/467875]\n",
            "loss: 0.320234  [371200/467875]\n",
            "loss: 0.293421  [374100/467875]\n",
            "loss: 0.543252  [377000/467875]\n",
            "loss: 0.353427  [379900/467875]\n",
            "loss: 0.345978  [382800/467875]\n",
            "loss: 0.286499  [385700/467875]\n",
            "loss: 0.701588  [388600/467875]\n",
            "loss: 0.473268  [391500/467875]\n",
            "loss: 0.535204  [394400/467875]\n",
            "loss: 0.395368  [397300/467875]\n",
            "loss: 0.415270  [400200/467875]\n",
            "loss: 0.365220  [403100/467875]\n",
            "loss: 0.743180  [406000/467875]\n",
            "loss: 0.468389  [408900/467875]\n",
            "loss: 0.501599  [411800/467875]\n",
            "loss: 0.423449  [414700/467875]\n",
            "loss: 0.530829  [417600/467875]\n",
            "loss: 0.312833  [420500/467875]\n",
            "loss: 0.630808  [423400/467875]\n",
            "loss: 0.493315  [426300/467875]\n",
            "loss: 0.459947  [429200/467875]\n",
            "loss: 0.659702  [432100/467875]\n",
            "loss: 0.493455  [435000/467875]\n",
            "loss: 0.664612  [437900/467875]\n",
            "loss: 0.561343  [440800/467875]\n",
            "loss: 0.570224  [443700/467875]\n",
            "loss: 0.418206  [446600/467875]\n",
            "loss: 0.362548  [449500/467875]\n",
            "loss: 0.380700  [452400/467875]\n",
            "loss: 0.490657  [455300/467875]\n",
            "loss: 0.365002  [458200/467875]\n",
            "loss: 0.539003  [461100/467875]\n",
            "loss: 0.412368  [464000/467875]\n",
            "loss: 0.496877  [466900/467875]\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476181 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 26 s\n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.474237  [    0/467875]\n",
            "loss: 0.380865  [ 2900/467875]\n",
            "loss: 0.377891  [ 5800/467875]\n",
            "loss: 0.360682  [ 8700/467875]\n",
            "loss: 0.475506  [11600/467875]\n",
            "loss: 0.354774  [14500/467875]\n",
            "loss: 0.433721  [17400/467875]\n",
            "loss: 0.547509  [20300/467875]\n",
            "loss: 0.441436  [23200/467875]\n",
            "loss: 0.529505  [26100/467875]\n",
            "loss: 0.569616  [29000/467875]\n",
            "loss: 0.526343  [31900/467875]\n",
            "loss: 0.360048  [34800/467875]\n",
            "loss: 0.462657  [37700/467875]\n",
            "loss: 0.468269  [40600/467875]\n",
            "loss: 0.368825  [43500/467875]\n",
            "loss: 0.690093  [46400/467875]\n",
            "loss: 0.598091  [49300/467875]\n",
            "loss: 0.403848  [52200/467875]\n",
            "loss: 0.672438  [55100/467875]\n",
            "loss: 0.618706  [58000/467875]\n",
            "loss: 0.502968  [60900/467875]\n",
            "loss: 0.431319  [63800/467875]\n",
            "loss: 0.574265  [66700/467875]\n",
            "loss: 0.338969  [69600/467875]\n",
            "loss: 0.338063  [72500/467875]\n",
            "loss: 0.385722  [75400/467875]\n",
            "loss: 0.546123  [78300/467875]\n",
            "loss: 0.391806  [81200/467875]\n",
            "loss: 0.576265  [84100/467875]\n",
            "loss: 0.488390  [87000/467875]\n",
            "loss: 0.437781  [89900/467875]\n",
            "loss: 0.427580  [92800/467875]\n",
            "loss: 0.325858  [95700/467875]\n",
            "loss: 0.317961  [98600/467875]\n",
            "loss: 0.320063  [101500/467875]\n",
            "loss: 0.454976  [104400/467875]\n",
            "loss: 0.578275  [107300/467875]\n",
            "loss: 0.253601  [110200/467875]\n",
            "loss: 0.360506  [113100/467875]\n",
            "loss: 0.542220  [116000/467875]\n",
            "loss: 0.730729  [118900/467875]\n",
            "loss: 0.298240  [121800/467875]\n",
            "loss: 0.671441  [124700/467875]\n",
            "loss: 0.467455  [127600/467875]\n",
            "loss: 0.392273  [130500/467875]\n",
            "loss: 0.518154  [133400/467875]\n",
            "loss: 0.647912  [136300/467875]\n",
            "loss: 0.441548  [139200/467875]\n",
            "loss: 0.434872  [142100/467875]\n",
            "loss: 0.377012  [145000/467875]\n",
            "loss: 0.367754  [147900/467875]\n",
            "loss: 0.566252  [150800/467875]\n",
            "loss: 0.461553  [153700/467875]\n",
            "loss: 0.550815  [156600/467875]\n",
            "loss: 0.341523  [159500/467875]\n",
            "loss: 0.463199  [162400/467875]\n",
            "loss: 0.421292  [165300/467875]\n",
            "loss: 0.521165  [168200/467875]\n",
            "loss: 0.396949  [171100/467875]\n",
            "loss: 0.424966  [174000/467875]\n",
            "loss: 0.418300  [176900/467875]\n",
            "loss: 0.634335  [179800/467875]\n",
            "loss: 0.615259  [182700/467875]\n",
            "loss: 0.538111  [185600/467875]\n",
            "loss: 0.308320  [188500/467875]\n",
            "loss: 0.440145  [191400/467875]\n",
            "loss: 0.574745  [194300/467875]\n",
            "loss: 0.409544  [197200/467875]\n",
            "loss: 0.397193  [200100/467875]\n",
            "loss: 0.489615  [203000/467875]\n",
            "loss: 0.520525  [205900/467875]\n",
            "loss: 0.408018  [208800/467875]\n",
            "loss: 0.631637  [211700/467875]\n",
            "loss: 0.458702  [214600/467875]\n",
            "loss: 0.551577  [217500/467875]\n",
            "loss: 0.340521  [220400/467875]\n",
            "loss: 0.494923  [223300/467875]\n",
            "loss: 0.381258  [226200/467875]\n",
            "loss: 0.451232  [229100/467875]\n",
            "loss: 0.560662  [232000/467875]\n",
            "loss: 0.540774  [234900/467875]\n",
            "loss: 0.450012  [237800/467875]\n",
            "loss: 0.469630  [240700/467875]\n",
            "loss: 0.426297  [243600/467875]\n",
            "loss: 0.400630  [246500/467875]\n",
            "loss: 0.640551  [249400/467875]\n",
            "loss: 0.391409  [252300/467875]\n",
            "loss: 0.281340  [255200/467875]\n",
            "loss: 0.378165  [258100/467875]\n",
            "loss: 0.381119  [261000/467875]\n",
            "loss: 0.354660  [263900/467875]\n",
            "loss: 0.449604  [266800/467875]\n",
            "loss: 0.394160  [269700/467875]\n",
            "loss: 0.538331  [272600/467875]\n",
            "loss: 0.469992  [275500/467875]\n",
            "loss: 0.491917  [278400/467875]\n",
            "loss: 0.533686  [281300/467875]\n",
            "loss: 0.629102  [284200/467875]\n",
            "loss: 0.556353  [287100/467875]\n",
            "loss: 0.306881  [290000/467875]\n",
            "loss: 0.445077  [292900/467875]\n",
            "loss: 0.387022  [295800/467875]\n",
            "loss: 0.589361  [298700/467875]\n",
            "loss: 0.577751  [301600/467875]\n",
            "loss: 0.532527  [304500/467875]\n",
            "loss: 0.481659  [307400/467875]\n",
            "loss: 0.471666  [310300/467875]\n",
            "loss: 0.494719  [313200/467875]\n",
            "loss: 0.478868  [316100/467875]\n",
            "loss: 0.532054  [319000/467875]\n",
            "loss: 0.290672  [321900/467875]\n",
            "loss: 0.450555  [324800/467875]\n",
            "loss: 0.564586  [327700/467875]\n",
            "loss: 0.549822  [330600/467875]\n",
            "loss: 0.442514  [333500/467875]\n",
            "loss: 0.465225  [336400/467875]\n",
            "loss: 0.429090  [339300/467875]\n",
            "loss: 0.578055  [342200/467875]\n",
            "loss: 0.463825  [345100/467875]\n",
            "loss: 0.620250  [348000/467875]\n",
            "loss: 0.365236  [350900/467875]\n",
            "loss: 0.359717  [353800/467875]\n",
            "loss: 0.436770  [356700/467875]\n",
            "loss: 0.543938  [359600/467875]\n",
            "loss: 0.357714  [362500/467875]\n",
            "loss: 0.520971  [365400/467875]\n",
            "loss: 0.309583  [368300/467875]\n",
            "loss: 0.617094  [371200/467875]\n",
            "loss: 0.549211  [374100/467875]\n",
            "loss: 0.476278  [377000/467875]\n",
            "loss: 0.537962  [379900/467875]\n",
            "loss: 0.369405  [382800/467875]\n",
            "loss: 0.329259  [385700/467875]\n",
            "loss: 0.435645  [388600/467875]\n",
            "loss: 0.580783  [391500/467875]\n",
            "loss: 0.411247  [394400/467875]\n",
            "loss: 0.367313  [397300/467875]\n",
            "loss: 0.504448  [400200/467875]\n",
            "loss: 0.321737  [403100/467875]\n",
            "loss: 0.306395  [406000/467875]\n",
            "loss: 0.527931  [408900/467875]\n",
            "loss: 0.436761  [411800/467875]\n",
            "loss: 0.671868  [414700/467875]\n",
            "loss: 0.517632  [417600/467875]\n",
            "loss: 0.774707  [420500/467875]\n",
            "loss: 0.491487  [423400/467875]\n",
            "loss: 0.487237  [426300/467875]\n",
            "loss: 0.375429  [429200/467875]\n",
            "loss: 0.477937  [432100/467875]\n",
            "loss: 0.510962  [435000/467875]\n",
            "loss: 0.232949  [437900/467875]\n",
            "loss: 0.432352  [440800/467875]\n",
            "loss: 0.460998  [443700/467875]\n",
            "loss: 0.589395  [446600/467875]\n",
            "loss: 0.507019  [449500/467875]\n",
            "loss: 0.649114  [452400/467875]\n",
            "loss: 0.465080  [455300/467875]\n",
            "loss: 0.452575  [458200/467875]\n",
            "loss: 0.554836  [461100/467875]\n",
            "loss: 0.348492  [464000/467875]\n",
            "loss: 0.489887  [466900/467875]\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.477337 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.473731  [    0/467875]\n",
            "loss: 0.529291  [ 2900/467875]\n",
            "loss: 0.511891  [ 5800/467875]\n",
            "loss: 0.416870  [ 8700/467875]\n",
            "loss: 0.402162  [11600/467875]\n",
            "loss: 0.429753  [14500/467875]\n",
            "loss: 0.343895  [17400/467875]\n",
            "loss: 0.445566  [20300/467875]\n",
            "loss: 0.446217  [23200/467875]\n",
            "loss: 0.305435  [26100/467875]\n",
            "loss: 0.511561  [29000/467875]\n",
            "loss: 0.447675  [31900/467875]\n",
            "loss: 0.487474  [34800/467875]\n",
            "loss: 0.567668  [37700/467875]\n",
            "loss: 0.372485  [40600/467875]\n",
            "loss: 0.352928  [43500/467875]\n",
            "loss: 0.515712  [46400/467875]\n",
            "loss: 0.384013  [49300/467875]\n",
            "loss: 0.674821  [52200/467875]\n",
            "loss: 0.559389  [55100/467875]\n",
            "loss: 0.530578  [58000/467875]\n",
            "loss: 0.484772  [60900/467875]\n",
            "loss: 0.429827  [63800/467875]\n",
            "loss: 0.376662  [66700/467875]\n",
            "loss: 0.581741  [69600/467875]\n",
            "loss: 0.542230  [72500/467875]\n",
            "loss: 0.545111  [75400/467875]\n",
            "loss: 0.595325  [78300/467875]\n",
            "loss: 0.430699  [81200/467875]\n",
            "loss: 0.511372  [84100/467875]\n",
            "loss: 0.342704  [87000/467875]\n",
            "loss: 0.424917  [89900/467875]\n",
            "loss: 0.419114  [92800/467875]\n",
            "loss: 0.439074  [95700/467875]\n",
            "loss: 0.419938  [98600/467875]\n",
            "loss: 0.498899  [101500/467875]\n",
            "loss: 0.377882  [104400/467875]\n",
            "loss: 0.392248  [107300/467875]\n",
            "loss: 0.511588  [110200/467875]\n",
            "loss: 0.389308  [113100/467875]\n",
            "loss: 0.530714  [116000/467875]\n",
            "loss: 0.388018  [118900/467875]\n",
            "loss: 0.510837  [121800/467875]\n",
            "loss: 0.523483  [124700/467875]\n",
            "loss: 0.423228  [127600/467875]\n",
            "loss: 0.401747  [130500/467875]\n",
            "loss: 0.677224  [133400/467875]\n",
            "loss: 0.438966  [136300/467875]\n",
            "loss: 0.428582  [139200/467875]\n",
            "loss: 0.863310  [142100/467875]\n",
            "loss: 0.475997  [145000/467875]\n",
            "loss: 0.447747  [147900/467875]\n",
            "loss: 0.397518  [150800/467875]\n",
            "loss: 0.412527  [153700/467875]\n",
            "loss: 0.393893  [156600/467875]\n",
            "loss: 0.552716  [159500/467875]\n",
            "loss: 0.480599  [162400/467875]\n",
            "loss: 0.470649  [165300/467875]\n",
            "loss: 0.641437  [168200/467875]\n",
            "loss: 0.422340  [171100/467875]\n",
            "loss: 0.456882  [174000/467875]\n",
            "loss: 0.615040  [176900/467875]\n",
            "loss: 0.353857  [179800/467875]\n",
            "loss: 0.535321  [182700/467875]\n",
            "loss: 0.476918  [185600/467875]\n",
            "loss: 0.389679  [188500/467875]\n",
            "loss: 0.411065  [191400/467875]\n",
            "loss: 0.517981  [194300/467875]\n",
            "loss: 0.608447  [197200/467875]\n",
            "loss: 0.428811  [200100/467875]\n",
            "loss: 0.420304  [203000/467875]\n",
            "loss: 0.411039  [205900/467875]\n",
            "loss: 0.532287  [208800/467875]\n",
            "loss: 0.318946  [211700/467875]\n",
            "loss: 0.406607  [214600/467875]\n",
            "loss: 0.521489  [217500/467875]\n",
            "loss: 0.558668  [220400/467875]\n",
            "loss: 0.728203  [223300/467875]\n",
            "loss: 0.444299  [226200/467875]\n",
            "loss: 0.414638  [229100/467875]\n",
            "loss: 0.472376  [232000/467875]\n",
            "loss: 0.476591  [234900/467875]\n",
            "loss: 0.426619  [237800/467875]\n",
            "loss: 0.408087  [240700/467875]\n",
            "loss: 0.399026  [243600/467875]\n",
            "loss: 0.450073  [246500/467875]\n",
            "loss: 0.496968  [249400/467875]\n",
            "loss: 0.436928  [252300/467875]\n",
            "loss: 0.383521  [255200/467875]\n",
            "loss: 0.419770  [258100/467875]\n",
            "loss: 0.562203  [261000/467875]\n",
            "loss: 0.289901  [263900/467875]\n",
            "loss: 0.682247  [266800/467875]\n",
            "loss: 0.622460  [269700/467875]\n",
            "loss: 0.653391  [272600/467875]\n",
            "loss: 0.451320  [275500/467875]\n",
            "loss: 0.637226  [278400/467875]\n",
            "loss: 0.576608  [281300/467875]\n",
            "loss: 0.359180  [284200/467875]\n",
            "loss: 0.471731  [287100/467875]\n",
            "loss: 0.355393  [290000/467875]\n",
            "loss: 0.489824  [292900/467875]\n",
            "loss: 0.604457  [295800/467875]\n",
            "loss: 0.510076  [298700/467875]\n",
            "loss: 0.369974  [301600/467875]\n",
            "loss: 0.315255  [304500/467875]\n",
            "loss: 0.338088  [307400/467875]\n",
            "loss: 0.342867  [310300/467875]\n",
            "loss: 0.366909  [313200/467875]\n",
            "loss: 0.471925  [316100/467875]\n",
            "loss: 0.436827  [319000/467875]\n",
            "loss: 0.457248  [321900/467875]\n",
            "loss: 0.297617  [324800/467875]\n",
            "loss: 0.452799  [327700/467875]\n",
            "loss: 0.578718  [330600/467875]\n",
            "loss: 0.495956  [333500/467875]\n",
            "loss: 0.422080  [336400/467875]\n",
            "loss: 0.404106  [339300/467875]\n",
            "loss: 0.386858  [342200/467875]\n",
            "loss: 0.507617  [345100/467875]\n",
            "loss: 0.406601  [348000/467875]\n",
            "loss: 0.485507  [350900/467875]\n",
            "loss: 0.448076  [353800/467875]\n",
            "loss: 0.427171  [356700/467875]\n",
            "loss: 0.581500  [359600/467875]\n",
            "loss: 0.385292  [362500/467875]\n",
            "loss: 0.648570  [365400/467875]\n",
            "loss: 0.594539  [368300/467875]\n",
            "loss: 0.473105  [371200/467875]\n",
            "loss: 0.440081  [374100/467875]\n",
            "loss: 0.370321  [377000/467875]\n",
            "loss: 0.389053  [379900/467875]\n",
            "loss: 0.410965  [382800/467875]\n",
            "loss: 0.526836  [385700/467875]\n",
            "loss: 0.438204  [388600/467875]\n",
            "loss: 0.337317  [391500/467875]\n",
            "loss: 0.564172  [394400/467875]\n",
            "loss: 0.491908  [397300/467875]\n",
            "loss: 0.394973  [400200/467875]\n",
            "loss: 0.448599  [403100/467875]\n",
            "loss: 0.427217  [406000/467875]\n",
            "loss: 0.419552  [408900/467875]\n",
            "loss: 0.599441  [411800/467875]\n",
            "loss: 0.326095  [414700/467875]\n",
            "loss: 0.500385  [417600/467875]\n",
            "loss: 0.304664  [420500/467875]\n",
            "loss: 0.509206  [423400/467875]\n",
            "loss: 0.288156  [426300/467875]\n",
            "loss: 0.321183  [429200/467875]\n",
            "loss: 0.405143  [432100/467875]\n",
            "loss: 0.404198  [435000/467875]\n",
            "loss: 0.408162  [437900/467875]\n",
            "loss: 0.672944  [440800/467875]\n",
            "loss: 0.402137  [443700/467875]\n",
            "loss: 0.347692  [446600/467875]\n",
            "loss: 0.474583  [449500/467875]\n",
            "loss: 0.632268  [452400/467875]\n",
            "loss: 0.462924  [455300/467875]\n",
            "loss: 0.712939  [458200/467875]\n",
            "loss: 0.298813  [461100/467875]\n",
            "loss: 0.519945  [464000/467875]\n",
            "loss: 0.397731  [466900/467875]\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.475921 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 24 s\n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.323317  [    0/467875]\n",
            "loss: 0.381700  [ 2900/467875]\n",
            "loss: 0.620484  [ 5800/467875]\n",
            "loss: 0.372172  [ 8700/467875]\n",
            "loss: 0.387170  [11600/467875]\n",
            "loss: 0.619006  [14500/467875]\n",
            "loss: 0.546047  [17400/467875]\n",
            "loss: 0.474271  [20300/467875]\n",
            "loss: 0.419343  [23200/467875]\n",
            "loss: 0.470533  [26100/467875]\n",
            "loss: 0.519488  [29000/467875]\n",
            "loss: 0.407834  [31900/467875]\n",
            "loss: 0.376798  [34800/467875]\n",
            "loss: 0.435418  [37700/467875]\n",
            "loss: 0.479018  [40600/467875]\n",
            "loss: 0.499519  [43500/467875]\n",
            "loss: 0.544001  [46400/467875]\n",
            "loss: 0.470809  [49300/467875]\n",
            "loss: 0.492027  [52200/467875]\n",
            "loss: 0.457463  [55100/467875]\n",
            "loss: 0.435182  [58000/467875]\n",
            "loss: 0.340105  [60900/467875]\n",
            "loss: 0.430600  [63800/467875]\n",
            "loss: 0.631886  [66700/467875]\n",
            "loss: 0.427251  [69600/467875]\n",
            "loss: 0.336400  [72500/467875]\n",
            "loss: 0.407195  [75400/467875]\n",
            "loss: 0.408547  [78300/467875]\n",
            "loss: 0.521631  [81200/467875]\n",
            "loss: 0.551568  [84100/467875]\n",
            "loss: 0.520354  [87000/467875]\n",
            "loss: 0.486804  [89900/467875]\n",
            "loss: 0.503436  [92800/467875]\n",
            "loss: 0.488406  [95700/467875]\n",
            "loss: 0.533936  [98600/467875]\n",
            "loss: 0.285483  [101500/467875]\n",
            "loss: 0.330798  [104400/467875]\n",
            "loss: 0.706081  [107300/467875]\n",
            "loss: 0.342329  [110200/467875]\n",
            "loss: 0.346929  [113100/467875]\n",
            "loss: 0.284585  [116000/467875]\n",
            "loss: 0.440964  [118900/467875]\n",
            "loss: 0.448580  [121800/467875]\n",
            "loss: 0.458674  [124700/467875]\n",
            "loss: 0.602108  [127600/467875]\n",
            "loss: 0.442396  [130500/467875]\n",
            "loss: 0.566275  [133400/467875]\n",
            "loss: 0.469083  [136300/467875]\n",
            "loss: 0.690011  [139200/467875]\n",
            "loss: 0.442433  [142100/467875]\n",
            "loss: 0.401325  [145000/467875]\n",
            "loss: 0.478830  [147900/467875]\n",
            "loss: 0.350974  [150800/467875]\n",
            "loss: 0.387871  [153700/467875]\n",
            "loss: 0.520443  [156600/467875]\n",
            "loss: 0.461810  [159500/467875]\n",
            "loss: 0.504527  [162400/467875]\n",
            "loss: 0.381905  [165300/467875]\n",
            "loss: 0.318492  [168200/467875]\n",
            "loss: 0.621185  [171100/467875]\n",
            "loss: 0.257644  [174000/467875]\n",
            "loss: 0.357564  [176900/467875]\n",
            "loss: 0.300298  [179800/467875]\n",
            "loss: 0.378044  [182700/467875]\n",
            "loss: 0.432197  [185600/467875]\n",
            "loss: 0.451247  [188500/467875]\n",
            "loss: 0.429970  [191400/467875]\n",
            "loss: 0.561563  [194300/467875]\n",
            "loss: 0.423000  [197200/467875]\n",
            "loss: 0.286389  [200100/467875]\n",
            "loss: 0.442647  [203000/467875]\n",
            "loss: 0.653840  [205900/467875]\n",
            "loss: 0.420340  [208800/467875]\n",
            "loss: 0.664952  [211700/467875]\n",
            "loss: 0.517801  [214600/467875]\n",
            "loss: 0.731359  [217500/467875]\n",
            "loss: 0.388909  [220400/467875]\n",
            "loss: 0.623535  [223300/467875]\n",
            "loss: 0.440245  [226200/467875]\n",
            "loss: 0.606515  [229100/467875]\n",
            "loss: 0.566834  [232000/467875]\n",
            "loss: 0.474707  [234900/467875]\n",
            "loss: 0.367584  [237800/467875]\n",
            "loss: 0.709991  [240700/467875]\n",
            "loss: 0.832527  [243600/467875]\n",
            "loss: 0.512956  [246500/467875]\n",
            "loss: 0.574143  [249400/467875]\n",
            "loss: 0.406070  [252300/467875]\n",
            "loss: 0.450705  [255200/467875]\n",
            "loss: 0.362794  [258100/467875]\n",
            "loss: 0.675033  [261000/467875]\n",
            "loss: 0.394202  [263900/467875]\n",
            "loss: 0.406795  [266800/467875]\n",
            "loss: 0.330524  [269700/467875]\n",
            "loss: 0.599794  [272600/467875]\n",
            "loss: 0.360967  [275500/467875]\n",
            "loss: 0.492965  [278400/467875]\n",
            "loss: 0.469450  [281300/467875]\n",
            "loss: 0.587611  [284200/467875]\n",
            "loss: 0.400404  [287100/467875]\n",
            "loss: 0.501892  [290000/467875]\n",
            "loss: 0.512300  [292900/467875]\n",
            "loss: 0.439441  [295800/467875]\n",
            "loss: 0.530419  [298700/467875]\n",
            "loss: 0.459188  [301600/467875]\n",
            "loss: 0.656400  [304500/467875]\n",
            "loss: 0.441339  [307400/467875]\n",
            "loss: 0.307008  [310300/467875]\n",
            "loss: 0.605771  [313200/467875]\n",
            "loss: 0.340881  [316100/467875]\n",
            "loss: 0.552869  [319000/467875]\n",
            "loss: 0.534084  [321900/467875]\n",
            "loss: 0.178603  [324800/467875]\n",
            "loss: 0.549264  [327700/467875]\n",
            "loss: 0.317350  [330600/467875]\n",
            "loss: 0.470891  [333500/467875]\n",
            "loss: 0.447424  [336400/467875]\n",
            "loss: 0.710107  [339300/467875]\n",
            "loss: 0.487897  [342200/467875]\n",
            "loss: 0.488866  [345100/467875]\n",
            "loss: 0.606807  [348000/467875]\n",
            "loss: 0.596348  [350900/467875]\n",
            "loss: 0.498404  [353800/467875]\n",
            "loss: 0.471198  [356700/467875]\n",
            "loss: 0.400284  [359600/467875]\n",
            "loss: 0.443374  [362500/467875]\n",
            "loss: 0.462068  [365400/467875]\n",
            "loss: 0.527911  [368300/467875]\n",
            "loss: 0.497904  [371200/467875]\n",
            "loss: 0.482654  [374100/467875]\n",
            "loss: 0.456762  [377000/467875]\n",
            "loss: 0.525849  [379900/467875]\n",
            "loss: 0.386883  [382800/467875]\n",
            "loss: 0.604688  [385700/467875]\n",
            "loss: 0.435908  [388600/467875]\n",
            "loss: 0.563915  [391500/467875]\n",
            "loss: 0.387009  [394400/467875]\n",
            "loss: 0.516766  [397300/467875]\n",
            "loss: 0.498941  [400200/467875]\n",
            "loss: 0.442750  [403100/467875]\n",
            "loss: 0.437402  [406000/467875]\n",
            "loss: 0.403092  [408900/467875]\n",
            "loss: 0.551672  [411800/467875]\n",
            "loss: 0.412135  [414700/467875]\n",
            "loss: 0.551360  [417600/467875]\n",
            "loss: 0.405138  [420500/467875]\n",
            "loss: 0.483657  [423400/467875]\n",
            "loss: 0.482664  [426300/467875]\n",
            "loss: 0.267404  [429200/467875]\n",
            "loss: 0.480358  [432100/467875]\n",
            "loss: 0.607872  [435000/467875]\n",
            "loss: 0.312424  [437900/467875]\n",
            "loss: 0.370500  [440800/467875]\n",
            "loss: 0.334997  [443700/467875]\n",
            "loss: 0.544931  [446600/467875]\n",
            "loss: 0.438831  [449500/467875]\n",
            "loss: 0.523801  [452400/467875]\n",
            "loss: 0.495181  [455300/467875]\n",
            "loss: 0.488073  [458200/467875]\n",
            "loss: 0.588064  [461100/467875]\n",
            "loss: 0.327668  [464000/467875]\n",
            "loss: 0.554096  [466900/467875]\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.476300 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 26 s\n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.447951  [    0/467875]\n",
            "loss: 0.386256  [ 2900/467875]\n",
            "loss: 0.448986  [ 5800/467875]\n",
            "loss: 0.385321  [ 8700/467875]\n",
            "loss: 0.431851  [11600/467875]\n",
            "loss: 0.876320  [14500/467875]\n",
            "loss: 0.459053  [17400/467875]\n",
            "loss: 0.450563  [20300/467875]\n",
            "loss: 0.458176  [23200/467875]\n",
            "loss: 0.585460  [26100/467875]\n",
            "loss: 0.452406  [29000/467875]\n",
            "loss: 0.473356  [31900/467875]\n",
            "loss: 0.331437  [34800/467875]\n",
            "loss: 0.365091  [37700/467875]\n",
            "loss: 0.467894  [40600/467875]\n",
            "loss: 0.682611  [43500/467875]\n",
            "loss: 0.422569  [46400/467875]\n",
            "loss: 0.435358  [49300/467875]\n",
            "loss: 0.523116  [52200/467875]\n",
            "loss: 0.814581  [55100/467875]\n",
            "loss: 0.395958  [58000/467875]\n",
            "loss: 0.519718  [60900/467875]\n",
            "loss: 0.646968  [63800/467875]\n",
            "loss: 0.541004  [66700/467875]\n",
            "loss: 0.462242  [69600/467875]\n",
            "loss: 0.536677  [72500/467875]\n",
            "loss: 0.477707  [75400/467875]\n",
            "loss: 0.439850  [78300/467875]\n",
            "loss: 0.329631  [81200/467875]\n",
            "loss: 0.439495  [84100/467875]\n",
            "loss: 0.527519  [87000/467875]\n",
            "loss: 0.542190  [89900/467875]\n",
            "loss: 0.405934  [92800/467875]\n",
            "loss: 0.428156  [95700/467875]\n",
            "loss: 0.513267  [98600/467875]\n",
            "loss: 0.432429  [101500/467875]\n",
            "loss: 0.435939  [104400/467875]\n",
            "loss: 0.445085  [107300/467875]\n",
            "loss: 0.553067  [110200/467875]\n",
            "loss: 0.462678  [113100/467875]\n",
            "loss: 0.433571  [116000/467875]\n",
            "loss: 0.545873  [118900/467875]\n",
            "loss: 0.502693  [121800/467875]\n",
            "loss: 0.654415  [124700/467875]\n",
            "loss: 0.429027  [127600/467875]\n",
            "loss: 0.453479  [130500/467875]\n",
            "loss: 0.573505  [133400/467875]\n",
            "loss: 0.522356  [136300/467875]\n",
            "loss: 0.410497  [139200/467875]\n",
            "loss: 0.434739  [142100/467875]\n",
            "loss: 0.529268  [145000/467875]\n",
            "loss: 0.450365  [147900/467875]\n",
            "loss: 0.456409  [150800/467875]\n",
            "loss: 0.390538  [153700/467875]\n",
            "loss: 0.458101  [156600/467875]\n",
            "loss: 0.509072  [159500/467875]\n",
            "loss: 0.693733  [162400/467875]\n",
            "loss: 0.554705  [165300/467875]\n",
            "loss: 0.491322  [168200/467875]\n",
            "loss: 0.528622  [171100/467875]\n",
            "loss: 0.478186  [174000/467875]\n",
            "loss: 0.508043  [176900/467875]\n",
            "loss: 0.399106  [179800/467875]\n",
            "loss: 0.694912  [182700/467875]\n",
            "loss: 0.389405  [185600/467875]\n",
            "loss: 0.464249  [188500/467875]\n",
            "loss: 0.360771  [191400/467875]\n",
            "loss: 0.358121  [194300/467875]\n",
            "loss: 0.547805  [197200/467875]\n",
            "loss: 0.487449  [200100/467875]\n",
            "loss: 0.506216  [203000/467875]\n",
            "loss: 0.576056  [205900/467875]\n",
            "loss: 0.443502  [208800/467875]\n",
            "loss: 0.418466  [211700/467875]\n",
            "loss: 0.460133  [214600/467875]\n",
            "loss: 0.370439  [217500/467875]\n",
            "loss: 0.407298  [220400/467875]\n",
            "loss: 0.399656  [223300/467875]\n",
            "loss: 0.600923  [226200/467875]\n",
            "loss: 0.558407  [229100/467875]\n",
            "loss: 0.667295  [232000/467875]\n",
            "loss: 0.499941  [234900/467875]\n",
            "loss: 0.432095  [237800/467875]\n",
            "loss: 0.502754  [240700/467875]\n",
            "loss: 0.357787  [243600/467875]\n",
            "loss: 0.269182  [246500/467875]\n",
            "loss: 0.362375  [249400/467875]\n",
            "loss: 0.629588  [252300/467875]\n",
            "loss: 0.389296  [255200/467875]\n",
            "loss: 0.624337  [258100/467875]\n",
            "loss: 0.390166  [261000/467875]\n",
            "loss: 0.406636  [263900/467875]\n",
            "loss: 0.538014  [266800/467875]\n",
            "loss: 0.591819  [269700/467875]\n",
            "loss: 0.386772  [272600/467875]\n",
            "loss: 0.376888  [275500/467875]\n",
            "loss: 0.621491  [278400/467875]\n",
            "loss: 0.510571  [281300/467875]\n",
            "loss: 0.361369  [284200/467875]\n",
            "loss: 0.445281  [287100/467875]\n",
            "loss: 0.498094  [290000/467875]\n",
            "loss: 0.297521  [292900/467875]\n",
            "loss: 0.552304  [295800/467875]\n",
            "loss: 0.436982  [298700/467875]\n",
            "loss: 0.442182  [301600/467875]\n",
            "loss: 0.303821  [304500/467875]\n",
            "loss: 0.619745  [307400/467875]\n",
            "loss: 0.387271  [310300/467875]\n",
            "loss: 0.541120  [313200/467875]\n",
            "loss: 0.379262  [316100/467875]\n",
            "loss: 0.548863  [319000/467875]\n",
            "loss: 0.420890  [321900/467875]\n",
            "loss: 0.358113  [324800/467875]\n",
            "loss: 0.442279  [327700/467875]\n",
            "loss: 0.683350  [330600/467875]\n",
            "loss: 0.610483  [333500/467875]\n",
            "loss: 0.493340  [336400/467875]\n",
            "loss: 0.510650  [339300/467875]\n",
            "loss: 0.537964  [342200/467875]\n",
            "loss: 0.473170  [345100/467875]\n",
            "loss: 0.577195  [348000/467875]\n",
            "loss: 0.764070  [350900/467875]\n",
            "loss: 0.403783  [353800/467875]\n",
            "loss: 0.656272  [356700/467875]\n",
            "loss: 0.541172  [359600/467875]\n",
            "loss: 0.453760  [362500/467875]\n",
            "loss: 0.400063  [365400/467875]\n",
            "loss: 0.346006  [368300/467875]\n",
            "loss: 0.352030  [371200/467875]\n",
            "loss: 0.341678  [374100/467875]\n",
            "loss: 0.581948  [377000/467875]\n",
            "loss: 0.422808  [379900/467875]\n",
            "loss: 0.315563  [382800/467875]\n",
            "loss: 0.678424  [385700/467875]\n",
            "loss: 0.326592  [388600/467875]\n",
            "loss: 0.534798  [391500/467875]\n",
            "loss: 0.521180  [394400/467875]\n",
            "loss: 0.341892  [397300/467875]\n",
            "loss: 0.347970  [400200/467875]\n",
            "loss: 0.484940  [403100/467875]\n",
            "loss: 0.545542  [406000/467875]\n",
            "loss: 0.617931  [408900/467875]\n",
            "loss: 0.574675  [411800/467875]\n",
            "loss: 0.498425  [414700/467875]\n",
            "loss: 0.405472  [417600/467875]\n",
            "loss: 0.442332  [420500/467875]\n",
            "loss: 0.403405  [423400/467875]\n",
            "loss: 0.332538  [426300/467875]\n",
            "loss: 0.438847  [429200/467875]\n",
            "loss: 0.373531  [432100/467875]\n",
            "loss: 0.444816  [435000/467875]\n",
            "loss: 0.680345  [437900/467875]\n",
            "loss: 0.529466  [440800/467875]\n",
            "loss: 0.570098  [443700/467875]\n",
            "loss: 0.360256  [446600/467875]\n",
            "loss: 0.489715  [449500/467875]\n",
            "loss: 0.542132  [452400/467875]\n",
            "loss: 0.559074  [455300/467875]\n",
            "loss: 0.487386  [458200/467875]\n",
            "loss: 0.796313  [461100/467875]\n",
            "loss: 0.723597  [464000/467875]\n",
            "loss: 0.549538  [466900/467875]\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.477745 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.571626  [    0/467875]\n",
            "loss: 0.466274  [ 2900/467875]\n",
            "loss: 0.518821  [ 5800/467875]\n",
            "loss: 0.624070  [ 8700/467875]\n",
            "loss: 0.389889  [11600/467875]\n",
            "loss: 0.652270  [14500/467875]\n",
            "loss: 0.564400  [17400/467875]\n",
            "loss: 0.825935  [20300/467875]\n",
            "loss: 0.378304  [23200/467875]\n",
            "loss: 0.535759  [26100/467875]\n",
            "loss: 0.453975  [29000/467875]\n",
            "loss: 0.378141  [31900/467875]\n",
            "loss: 0.416477  [34800/467875]\n",
            "loss: 0.470575  [37700/467875]\n",
            "loss: 0.440117  [40600/467875]\n",
            "loss: 0.603164  [43500/467875]\n",
            "loss: 0.578816  [46400/467875]\n",
            "loss: 0.430423  [49300/467875]\n",
            "loss: 0.569239  [52200/467875]\n",
            "loss: 0.501633  [55100/467875]\n",
            "loss: 0.539080  [58000/467875]\n",
            "loss: 0.386060  [60900/467875]\n",
            "loss: 0.468014  [63800/467875]\n",
            "loss: 0.446924  [66700/467875]\n",
            "loss: 0.579531  [69600/467875]\n",
            "loss: 0.340918  [72500/467875]\n",
            "loss: 0.457313  [75400/467875]\n",
            "loss: 0.492305  [78300/467875]\n",
            "loss: 0.382570  [81200/467875]\n",
            "loss: 0.540422  [84100/467875]\n",
            "loss: 0.520523  [87000/467875]\n",
            "loss: 0.616244  [89900/467875]\n",
            "loss: 0.400207  [92800/467875]\n",
            "loss: 0.477371  [95700/467875]\n",
            "loss: 0.367210  [98600/467875]\n",
            "loss: 0.418231  [101500/467875]\n",
            "loss: 0.450102  [104400/467875]\n",
            "loss: 0.404194  [107300/467875]\n",
            "loss: 0.475550  [110200/467875]\n",
            "loss: 0.416592  [113100/467875]\n",
            "loss: 0.494156  [116000/467875]\n",
            "loss: 0.418653  [118900/467875]\n",
            "loss: 0.413140  [121800/467875]\n",
            "loss: 0.368007  [124700/467875]\n",
            "loss: 0.518003  [127600/467875]\n",
            "loss: 0.504272  [130500/467875]\n",
            "loss: 0.470303  [133400/467875]\n",
            "loss: 0.449458  [136300/467875]\n",
            "loss: 0.451608  [139200/467875]\n",
            "loss: 0.418494  [142100/467875]\n",
            "loss: 0.513870  [145000/467875]\n",
            "loss: 0.763032  [147900/467875]\n",
            "loss: 0.487523  [150800/467875]\n",
            "loss: 0.639928  [153700/467875]\n",
            "loss: 0.390200  [156600/467875]\n",
            "loss: 0.409081  [159500/467875]\n",
            "loss: 0.524217  [162400/467875]\n",
            "loss: 0.570665  [165300/467875]\n",
            "loss: 0.251003  [168200/467875]\n",
            "loss: 0.458646  [171100/467875]\n",
            "loss: 0.575308  [174000/467875]\n",
            "loss: 0.558701  [176900/467875]\n",
            "loss: 0.283441  [179800/467875]\n",
            "loss: 0.554112  [182700/467875]\n",
            "loss: 0.532259  [185600/467875]\n",
            "loss: 0.477789  [188500/467875]\n",
            "loss: 0.459394  [191400/467875]\n",
            "loss: 0.395322  [194300/467875]\n",
            "loss: 0.636290  [197200/467875]\n",
            "loss: 0.475844  [200100/467875]\n",
            "loss: 0.684646  [203000/467875]\n",
            "loss: 0.527625  [205900/467875]\n",
            "loss: 0.357362  [208800/467875]\n",
            "loss: 0.585874  [211700/467875]\n",
            "loss: 0.442942  [214600/467875]\n",
            "loss: 0.426416  [217500/467875]\n",
            "loss: 0.401843  [220400/467875]\n",
            "loss: 0.510042  [223300/467875]\n",
            "loss: 0.583364  [226200/467875]\n",
            "loss: 0.485225  [229100/467875]\n",
            "loss: 0.527334  [232000/467875]\n",
            "loss: 0.485216  [234900/467875]\n",
            "loss: 0.344254  [237800/467875]\n",
            "loss: 0.504757  [240700/467875]\n",
            "loss: 0.657341  [243600/467875]\n",
            "loss: 0.643934  [246500/467875]\n",
            "loss: 0.621265  [249400/467875]\n",
            "loss: 0.461567  [252300/467875]\n",
            "loss: 0.344583  [255200/467875]\n",
            "loss: 0.435025  [258100/467875]\n",
            "loss: 0.445858  [261000/467875]\n",
            "loss: 0.357850  [263900/467875]\n",
            "loss: 0.452173  [266800/467875]\n",
            "loss: 0.397841  [269700/467875]\n",
            "loss: 0.683356  [272600/467875]\n",
            "loss: 0.479408  [275500/467875]\n",
            "loss: 0.577237  [278400/467875]\n",
            "loss: 0.559861  [281300/467875]\n",
            "loss: 0.795208  [284200/467875]\n",
            "loss: 0.353225  [287100/467875]\n",
            "loss: 0.464043  [290000/467875]\n",
            "loss: 0.428909  [292900/467875]\n",
            "loss: 0.490191  [295800/467875]\n",
            "loss: 0.380911  [298700/467875]\n",
            "loss: 0.678358  [301600/467875]\n",
            "loss: 0.627918  [304500/467875]\n",
            "loss: 0.485198  [307400/467875]\n",
            "loss: 0.426270  [310300/467875]\n",
            "loss: 0.462063  [313200/467875]\n",
            "loss: 0.570999  [316100/467875]\n",
            "loss: 0.531971  [319000/467875]\n",
            "loss: 0.575988  [321900/467875]\n",
            "loss: 0.363194  [324800/467875]\n",
            "loss: 0.649599  [327700/467875]\n",
            "loss: 0.591262  [330600/467875]\n",
            "loss: 0.393165  [333500/467875]\n",
            "loss: 0.376509  [336400/467875]\n",
            "loss: 0.426246  [339300/467875]\n",
            "loss: 0.554845  [342200/467875]\n",
            "loss: 0.408993  [345100/467875]\n",
            "loss: 0.431756  [348000/467875]\n",
            "loss: 0.491401  [350900/467875]\n",
            "loss: 0.648101  [353800/467875]\n",
            "loss: 0.675001  [356700/467875]\n",
            "loss: 0.474622  [359600/467875]\n",
            "loss: 0.324441  [362500/467875]\n",
            "loss: 0.522551  [365400/467875]\n",
            "loss: 0.565020  [368300/467875]\n",
            "loss: 0.384825  [371200/467875]\n",
            "loss: 0.576082  [374100/467875]\n",
            "loss: 0.477345  [377000/467875]\n",
            "loss: 0.479128  [379900/467875]\n",
            "loss: 0.434312  [382800/467875]\n",
            "loss: 0.422333  [385700/467875]\n",
            "loss: 0.370428  [388600/467875]\n",
            "loss: 0.628588  [391500/467875]\n",
            "loss: 0.598353  [394400/467875]\n",
            "loss: 0.379145  [397300/467875]\n",
            "loss: 0.526079  [400200/467875]\n",
            "loss: 0.513775  [403100/467875]\n",
            "loss: 0.397101  [406000/467875]\n",
            "loss: 0.651802  [408900/467875]\n",
            "loss: 0.551623  [411800/467875]\n",
            "loss: 0.476068  [414700/467875]\n",
            "loss: 0.654033  [417600/467875]\n",
            "loss: 0.518705  [420500/467875]\n",
            "loss: 0.307672  [423400/467875]\n",
            "loss: 0.306083  [426300/467875]\n",
            "loss: 0.550034  [429200/467875]\n",
            "loss: 0.540173  [432100/467875]\n",
            "loss: 0.457566  [435000/467875]\n",
            "loss: 0.465349  [437900/467875]\n",
            "loss: 0.457799  [440800/467875]\n",
            "loss: 0.600067  [443700/467875]\n",
            "loss: 0.500156  [446600/467875]\n",
            "loss: 0.549701  [449500/467875]\n",
            "loss: 0.459391  [452400/467875]\n",
            "loss: 0.596223  [455300/467875]\n",
            "loss: 0.481704  [458200/467875]\n",
            "loss: 0.531939  [461100/467875]\n",
            "loss: 0.515289  [464000/467875]\n",
            "loss: 0.427586  [466900/467875]\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476041 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.576844  [    0/467875]\n",
            "loss: 0.418144  [ 2900/467875]\n",
            "loss: 0.493183  [ 5800/467875]\n",
            "loss: 0.471476  [ 8700/467875]\n",
            "loss: 0.681242  [11600/467875]\n",
            "loss: 0.372804  [14500/467875]\n",
            "loss: 0.529506  [17400/467875]\n",
            "loss: 0.326380  [20300/467875]\n",
            "loss: 0.399902  [23200/467875]\n",
            "loss: 0.357871  [26100/467875]\n",
            "loss: 0.625053  [29000/467875]\n",
            "loss: 0.260741  [31900/467875]\n",
            "loss: 0.378027  [34800/467875]\n",
            "loss: 0.413123  [37700/467875]\n",
            "loss: 0.537153  [40600/467875]\n",
            "loss: 0.301423  [43500/467875]\n",
            "loss: 0.624642  [46400/467875]\n",
            "loss: 0.432343  [49300/467875]\n",
            "loss: 0.509528  [52200/467875]\n",
            "loss: 0.521498  [55100/467875]\n",
            "loss: 0.644652  [58000/467875]\n",
            "loss: 0.396978  [60900/467875]\n",
            "loss: 0.420097  [63800/467875]\n",
            "loss: 0.501507  [66700/467875]\n",
            "loss: 0.379821  [69600/467875]\n",
            "loss: 0.483306  [72500/467875]\n",
            "loss: 0.278839  [75400/467875]\n",
            "loss: 0.385984  [78300/467875]\n",
            "loss: 0.558907  [81200/467875]\n",
            "loss: 0.523049  [84100/467875]\n",
            "loss: 0.263810  [87000/467875]\n",
            "loss: 0.550367  [89900/467875]\n",
            "loss: 0.349327  [92800/467875]\n",
            "loss: 0.358163  [95700/467875]\n",
            "loss: 0.432119  [98600/467875]\n",
            "loss: 0.344713  [101500/467875]\n",
            "loss: 0.337306  [104400/467875]\n",
            "loss: 0.520248  [107300/467875]\n",
            "loss: 0.369632  [110200/467875]\n",
            "loss: 0.376566  [113100/467875]\n",
            "loss: 0.381652  [116000/467875]\n",
            "loss: 0.392265  [118900/467875]\n",
            "loss: 0.410726  [121800/467875]\n",
            "loss: 0.415737  [124700/467875]\n",
            "loss: 0.392689  [127600/467875]\n",
            "loss: 0.409241  [130500/467875]\n",
            "loss: 0.447019  [133400/467875]\n",
            "loss: 0.561224  [136300/467875]\n",
            "loss: 0.448239  [139200/467875]\n",
            "loss: 0.453222  [142100/467875]\n",
            "loss: 0.489011  [145000/467875]\n",
            "loss: 0.452251  [147900/467875]\n",
            "loss: 0.399604  [150800/467875]\n",
            "loss: 0.607868  [153700/467875]\n",
            "loss: 0.452707  [156600/467875]\n",
            "loss: 0.292944  [159500/467875]\n",
            "loss: 0.390247  [162400/467875]\n",
            "loss: 0.330763  [165300/467875]\n",
            "loss: 0.439994  [168200/467875]\n",
            "loss: 0.436684  [171100/467875]\n",
            "loss: 0.413941  [174000/467875]\n",
            "loss: 0.527450  [176900/467875]\n",
            "loss: 0.394670  [179800/467875]\n",
            "loss: 0.418612  [182700/467875]\n",
            "loss: 0.534358  [185600/467875]\n",
            "loss: 0.538371  [188500/467875]\n",
            "loss: 0.448621  [191400/467875]\n",
            "loss: 0.395486  [194300/467875]\n",
            "loss: 0.436848  [197200/467875]\n",
            "loss: 0.448880  [200100/467875]\n",
            "loss: 0.318186  [203000/467875]\n",
            "loss: 0.426923  [205900/467875]\n",
            "loss: 0.596216  [208800/467875]\n",
            "loss: 0.485279  [211700/467875]\n",
            "loss: 0.568676  [214600/467875]\n",
            "loss: 0.482976  [217500/467875]\n",
            "loss: 0.387551  [220400/467875]\n",
            "loss: 0.582633  [223300/467875]\n",
            "loss: 0.404659  [226200/467875]\n",
            "loss: 0.518027  [229100/467875]\n",
            "loss: 0.592081  [232000/467875]\n",
            "loss: 0.411595  [234900/467875]\n",
            "loss: 0.337813  [237800/467875]\n",
            "loss: 0.282035  [240700/467875]\n",
            "loss: 0.440769  [243600/467875]\n",
            "loss: 0.540624  [246500/467875]\n",
            "loss: 0.376594  [249400/467875]\n",
            "loss: 0.453061  [252300/467875]\n",
            "loss: 0.525526  [255200/467875]\n",
            "loss: 0.573873  [258100/467875]\n",
            "loss: 0.453810  [261000/467875]\n",
            "loss: 0.375271  [263900/467875]\n",
            "loss: 0.579807  [266800/467875]\n",
            "loss: 0.361317  [269700/467875]\n",
            "loss: 0.419040  [272600/467875]\n",
            "loss: 0.413450  [275500/467875]\n",
            "loss: 0.224863  [278400/467875]\n",
            "loss: 0.467062  [281300/467875]\n",
            "loss: 0.303880  [284200/467875]\n",
            "loss: 0.438560  [287100/467875]\n",
            "loss: 0.526819  [290000/467875]\n",
            "loss: 0.507241  [292900/467875]\n",
            "loss: 0.421052  [295800/467875]\n",
            "loss: 0.483234  [298700/467875]\n",
            "loss: 0.567966  [301600/467875]\n",
            "loss: 0.468561  [304500/467875]\n",
            "loss: 0.535030  [307400/467875]\n",
            "loss: 0.605379  [310300/467875]\n",
            "loss: 0.744916  [313200/467875]\n",
            "loss: 0.549370  [316100/467875]\n",
            "loss: 0.385230  [319000/467875]\n",
            "loss: 0.518240  [321900/467875]\n",
            "loss: 0.586444  [324800/467875]\n",
            "loss: 0.339742  [327700/467875]\n",
            "loss: 0.492459  [330600/467875]\n",
            "loss: 0.539045  [333500/467875]\n",
            "loss: 0.426163  [336400/467875]\n",
            "loss: 0.646678  [339300/467875]\n",
            "loss: 0.425833  [342200/467875]\n",
            "loss: 0.475663  [345100/467875]\n",
            "loss: 0.663522  [348000/467875]\n",
            "loss: 0.444951  [350900/467875]\n",
            "loss: 0.605460  [353800/467875]\n",
            "loss: 0.501224  [356700/467875]\n",
            "loss: 0.417558  [359600/467875]\n",
            "loss: 0.540653  [362500/467875]\n",
            "loss: 0.436734  [365400/467875]\n",
            "loss: 0.503410  [368300/467875]\n",
            "loss: 0.681416  [371200/467875]\n",
            "loss: 0.706577  [374100/467875]\n",
            "loss: 0.389613  [377000/467875]\n",
            "loss: 0.377111  [379900/467875]\n",
            "loss: 0.521929  [382800/467875]\n",
            "loss: 0.392256  [385700/467875]\n",
            "loss: 0.463482  [388600/467875]\n",
            "loss: 0.576651  [391500/467875]\n",
            "loss: 0.495278  [394400/467875]\n",
            "loss: 0.487325  [397300/467875]\n",
            "loss: 0.405301  [400200/467875]\n",
            "loss: 0.508107  [403100/467875]\n",
            "loss: 0.450021  [406000/467875]\n",
            "loss: 0.610807  [408900/467875]\n",
            "loss: 0.384531  [411800/467875]\n",
            "loss: 0.634880  [414700/467875]\n",
            "loss: 0.391712  [417600/467875]\n",
            "loss: 0.584899  [420500/467875]\n",
            "loss: 0.618724  [423400/467875]\n",
            "loss: 0.664431  [426300/467875]\n",
            "loss: 0.442325  [429200/467875]\n",
            "loss: 0.340345  [432100/467875]\n",
            "loss: 0.335403  [435000/467875]\n",
            "loss: 0.437305  [437900/467875]\n",
            "loss: 0.478958  [440800/467875]\n",
            "loss: 0.527657  [443700/467875]\n",
            "loss: 0.568530  [446600/467875]\n",
            "loss: 0.376681  [449500/467875]\n",
            "loss: 0.434153  [452400/467875]\n",
            "loss: 0.434639  [455300/467875]\n",
            "loss: 0.564043  [458200/467875]\n",
            "loss: 0.486207  [461100/467875]\n",
            "loss: 0.539267  [464000/467875]\n",
            "loss: 0.506537  [466900/467875]\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.475582 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.442744  [    0/467875]\n",
            "loss: 0.452849  [ 2900/467875]\n",
            "loss: 0.570057  [ 5800/467875]\n",
            "loss: 0.587691  [ 8700/467875]\n",
            "loss: 0.453345  [11600/467875]\n",
            "loss: 0.673245  [14500/467875]\n",
            "loss: 0.351902  [17400/467875]\n",
            "loss: 0.533776  [20300/467875]\n",
            "loss: 0.273835  [23200/467875]\n",
            "loss: 0.546297  [26100/467875]\n",
            "loss: 0.409655  [29000/467875]\n",
            "loss: 0.430079  [31900/467875]\n",
            "loss: 0.362792  [34800/467875]\n",
            "loss: 0.417579  [37700/467875]\n",
            "loss: 0.497405  [40600/467875]\n",
            "loss: 0.727858  [43500/467875]\n",
            "loss: 0.451261  [46400/467875]\n",
            "loss: 0.370646  [49300/467875]\n",
            "loss: 0.396075  [52200/467875]\n",
            "loss: 0.506463  [55100/467875]\n",
            "loss: 0.414497  [58000/467875]\n",
            "loss: 0.444499  [60900/467875]\n",
            "loss: 0.331209  [63800/467875]\n",
            "loss: 0.514682  [66700/467875]\n",
            "loss: 0.413280  [69600/467875]\n",
            "loss: 0.395937  [72500/467875]\n",
            "loss: 0.357706  [75400/467875]\n",
            "loss: 0.280212  [78300/467875]\n",
            "loss: 0.472491  [81200/467875]\n",
            "loss: 0.599725  [84100/467875]\n",
            "loss: 0.376686  [87000/467875]\n",
            "loss: 0.641750  [89900/467875]\n",
            "loss: 0.541419  [92800/467875]\n",
            "loss: 0.362798  [95700/467875]\n",
            "loss: 0.435664  [98600/467875]\n",
            "loss: 0.484289  [101500/467875]\n",
            "loss: 0.652668  [104400/467875]\n",
            "loss: 0.656293  [107300/467875]\n",
            "loss: 0.517054  [110200/467875]\n",
            "loss: 0.359029  [113100/467875]\n",
            "loss: 0.382699  [116000/467875]\n",
            "loss: 0.517337  [118900/467875]\n",
            "loss: 0.618236  [121800/467875]\n",
            "loss: 0.593159  [124700/467875]\n",
            "loss: 0.485692  [127600/467875]\n",
            "loss: 0.412281  [130500/467875]\n",
            "loss: 0.398875  [133400/467875]\n",
            "loss: 0.804367  [136300/467875]\n",
            "loss: 0.360936  [139200/467875]\n",
            "loss: 0.368696  [142100/467875]\n",
            "loss: 0.406476  [145000/467875]\n",
            "loss: 0.593467  [147900/467875]\n",
            "loss: 0.318731  [150800/467875]\n",
            "loss: 0.510393  [153700/467875]\n",
            "loss: 0.453509  [156600/467875]\n",
            "loss: 0.559352  [159500/467875]\n",
            "loss: 0.410264  [162400/467875]\n",
            "loss: 0.386785  [165300/467875]\n",
            "loss: 0.334312  [168200/467875]\n",
            "loss: 0.352259  [171100/467875]\n",
            "loss: 0.534693  [174000/467875]\n",
            "loss: 0.552345  [176900/467875]\n",
            "loss: 0.448805  [179800/467875]\n",
            "loss: 0.533415  [182700/467875]\n",
            "loss: 0.598434  [185600/467875]\n",
            "loss: 0.459912  [188500/467875]\n",
            "loss: 0.535593  [191400/467875]\n",
            "loss: 0.587503  [194300/467875]\n",
            "loss: 0.674737  [197200/467875]\n",
            "loss: 0.457930  [200100/467875]\n",
            "loss: 0.320867  [203000/467875]\n",
            "loss: 0.605138  [205900/467875]\n",
            "loss: 0.299649  [208800/467875]\n",
            "loss: 0.524247  [211700/467875]\n",
            "loss: 0.296076  [214600/467875]\n",
            "loss: 0.634125  [217500/467875]\n",
            "loss: 0.528367  [220400/467875]\n",
            "loss: 0.647626  [223300/467875]\n",
            "loss: 0.583151  [226200/467875]\n",
            "loss: 0.489651  [229100/467875]\n",
            "loss: 0.564718  [232000/467875]\n",
            "loss: 0.297295  [234900/467875]\n",
            "loss: 0.424816  [237800/467875]\n",
            "loss: 0.410333  [240700/467875]\n",
            "loss: 0.488076  [243600/467875]\n",
            "loss: 0.480751  [246500/467875]\n",
            "loss: 0.492600  [249400/467875]\n",
            "loss: 0.401244  [252300/467875]\n",
            "loss: 0.478017  [255200/467875]\n",
            "loss: 0.516981  [258100/467875]\n",
            "loss: 0.425917  [261000/467875]\n",
            "loss: 0.459324  [263900/467875]\n",
            "loss: 0.401033  [266800/467875]\n",
            "loss: 0.407532  [269700/467875]\n",
            "loss: 0.448616  [272600/467875]\n",
            "loss: 0.614272  [275500/467875]\n",
            "loss: 0.471090  [278400/467875]\n",
            "loss: 0.458797  [281300/467875]\n",
            "loss: 0.637545  [284200/467875]\n",
            "loss: 0.615013  [287100/467875]\n",
            "loss: 0.507064  [290000/467875]\n",
            "loss: 0.371285  [292900/467875]\n",
            "loss: 0.578707  [295800/467875]\n",
            "loss: 0.436000  [298700/467875]\n",
            "loss: 0.494370  [301600/467875]\n",
            "loss: 0.456078  [304500/467875]\n",
            "loss: 0.482767  [307400/467875]\n",
            "loss: 0.347003  [310300/467875]\n",
            "loss: 0.493955  [313200/467875]\n",
            "loss: 0.425105  [316100/467875]\n",
            "loss: 0.647940  [319000/467875]\n",
            "loss: 0.339663  [321900/467875]\n",
            "loss: 0.586927  [324800/467875]\n",
            "loss: 0.414207  [327700/467875]\n",
            "loss: 0.399499  [330600/467875]\n",
            "loss: 0.328559  [333500/467875]\n",
            "loss: 0.341140  [336400/467875]\n",
            "loss: 0.516448  [339300/467875]\n",
            "loss: 0.378786  [342200/467875]\n",
            "loss: 0.334237  [345100/467875]\n",
            "loss: 0.552457  [348000/467875]\n",
            "loss: 0.551000  [350900/467875]\n",
            "loss: 0.536597  [353800/467875]\n",
            "loss: 0.419496  [356700/467875]\n",
            "loss: 0.421600  [359600/467875]\n",
            "loss: 0.451634  [362500/467875]\n",
            "loss: 0.490285  [365400/467875]\n",
            "loss: 0.453750  [368300/467875]\n",
            "loss: 0.409695  [371200/467875]\n",
            "loss: 0.552210  [374100/467875]\n",
            "loss: 0.535600  [377000/467875]\n",
            "loss: 0.470023  [379900/467875]\n",
            "loss: 0.513442  [382800/467875]\n",
            "loss: 0.499641  [385700/467875]\n",
            "loss: 0.565777  [388600/467875]\n",
            "loss: 0.481291  [391500/467875]\n",
            "loss: 0.494761  [394400/467875]\n",
            "loss: 0.581419  [397300/467875]\n",
            "loss: 0.374561  [400200/467875]\n",
            "loss: 0.606076  [403100/467875]\n",
            "loss: 0.787569  [406000/467875]\n",
            "loss: 0.210031  [408900/467875]\n",
            "loss: 0.368147  [411800/467875]\n",
            "loss: 0.480887  [414700/467875]\n",
            "loss: 0.436963  [417600/467875]\n",
            "loss: 0.414857  [420500/467875]\n",
            "loss: 0.389614  [423400/467875]\n",
            "loss: 0.443281  [426300/467875]\n",
            "loss: 0.339572  [429200/467875]\n",
            "loss: 0.531772  [432100/467875]\n",
            "loss: 0.268494  [435000/467875]\n",
            "loss: 0.602730  [437900/467875]\n",
            "loss: 0.403919  [440800/467875]\n",
            "loss: 0.444799  [443700/467875]\n",
            "loss: 0.432665  [446600/467875]\n",
            "loss: 0.404465  [449500/467875]\n",
            "loss: 0.477734  [452400/467875]\n",
            "loss: 0.486020  [455300/467875]\n",
            "loss: 0.520510  [458200/467875]\n",
            "loss: 0.443761  [461100/467875]\n",
            "loss: 0.362800  [464000/467875]\n",
            "loss: 0.439957  [466900/467875]\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476684 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 24 s\n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.639778  [    0/467875]\n",
            "loss: 0.393048  [ 2900/467875]\n",
            "loss: 0.539852  [ 5800/467875]\n",
            "loss: 0.339539  [ 8700/467875]\n",
            "loss: 0.280398  [11600/467875]\n",
            "loss: 0.624674  [14500/467875]\n",
            "loss: 0.413042  [17400/467875]\n",
            "loss: 0.545226  [20300/467875]\n",
            "loss: 0.600769  [23200/467875]\n",
            "loss: 0.445330  [26100/467875]\n",
            "loss: 0.533571  [29000/467875]\n",
            "loss: 0.404206  [31900/467875]\n",
            "loss: 0.596704  [34800/467875]\n",
            "loss: 0.544990  [37700/467875]\n",
            "loss: 0.429932  [40600/467875]\n",
            "loss: 0.666816  [43500/467875]\n",
            "loss: 0.399647  [46400/467875]\n",
            "loss: 0.502790  [49300/467875]\n",
            "loss: 0.455014  [52200/467875]\n",
            "loss: 0.513213  [55100/467875]\n",
            "loss: 0.390006  [58000/467875]\n",
            "loss: 0.501856  [60900/467875]\n",
            "loss: 0.438417  [63800/467875]\n",
            "loss: 0.599046  [66700/467875]\n",
            "loss: 0.466662  [69600/467875]\n",
            "loss: 0.572642  [72500/467875]\n",
            "loss: 0.435330  [75400/467875]\n",
            "loss: 0.507241  [78300/467875]\n",
            "loss: 0.530752  [81200/467875]\n",
            "loss: 0.539245  [84100/467875]\n",
            "loss: 0.558562  [87000/467875]\n",
            "loss: 0.472036  [89900/467875]\n",
            "loss: 0.665896  [92800/467875]\n",
            "loss: 0.384500  [95700/467875]\n",
            "loss: 0.493867  [98600/467875]\n",
            "loss: 0.409220  [101500/467875]\n",
            "loss: 0.599189  [104400/467875]\n",
            "loss: 0.546815  [107300/467875]\n",
            "loss: 0.587064  [110200/467875]\n",
            "loss: 0.426345  [113100/467875]\n",
            "loss: 0.519212  [116000/467875]\n",
            "loss: 0.482610  [118900/467875]\n",
            "loss: 0.472471  [121800/467875]\n",
            "loss: 0.444296  [124700/467875]\n",
            "loss: 0.468366  [127600/467875]\n",
            "loss: 0.639568  [130500/467875]\n",
            "loss: 0.641547  [133400/467875]\n",
            "loss: 0.478060  [136300/467875]\n",
            "loss: 0.375669  [139200/467875]\n",
            "loss: 0.323125  [142100/467875]\n",
            "loss: 0.391039  [145000/467875]\n",
            "loss: 0.427820  [147900/467875]\n",
            "loss: 0.495611  [150800/467875]\n",
            "loss: 0.563011  [153700/467875]\n",
            "loss: 0.563260  [156600/467875]\n",
            "loss: 0.491552  [159500/467875]\n",
            "loss: 0.246631  [162400/467875]\n",
            "loss: 0.417857  [165300/467875]\n",
            "loss: 0.590548  [168200/467875]\n",
            "loss: 0.355174  [171100/467875]\n",
            "loss: 0.426844  [174000/467875]\n",
            "loss: 0.444262  [176900/467875]\n",
            "loss: 0.388860  [179800/467875]\n",
            "loss: 0.364431  [182700/467875]\n",
            "loss: 0.399622  [185600/467875]\n",
            "loss: 0.411096  [188500/467875]\n",
            "loss: 0.600605  [191400/467875]\n",
            "loss: 0.448891  [194300/467875]\n",
            "loss: 0.600497  [197200/467875]\n",
            "loss: 0.635204  [200100/467875]\n",
            "loss: 0.573105  [203000/467875]\n",
            "loss: 0.416718  [205900/467875]\n",
            "loss: 0.601628  [208800/467875]\n",
            "loss: 0.538765  [211700/467875]\n",
            "loss: 0.392902  [214600/467875]\n",
            "loss: 0.527125  [217500/467875]\n",
            "loss: 0.518431  [220400/467875]\n",
            "loss: 0.524680  [223300/467875]\n",
            "loss: 0.325674  [226200/467875]\n",
            "loss: 0.443038  [229100/467875]\n",
            "loss: 0.428311  [232000/467875]\n",
            "loss: 0.641571  [234900/467875]\n",
            "loss: 0.388712  [237800/467875]\n",
            "loss: 0.503735  [240700/467875]\n",
            "loss: 0.353173  [243600/467875]\n",
            "loss: 0.496547  [246500/467875]\n",
            "loss: 0.369181  [249400/467875]\n",
            "loss: 0.467639  [252300/467875]\n",
            "loss: 0.273520  [255200/467875]\n",
            "loss: 0.451059  [258100/467875]\n",
            "loss: 0.401111  [261000/467875]\n",
            "loss: 0.586916  [263900/467875]\n",
            "loss: 0.666860  [266800/467875]\n",
            "loss: 0.535402  [269700/467875]\n",
            "loss: 0.310961  [272600/467875]\n",
            "loss: 0.515973  [275500/467875]\n",
            "loss: 0.401785  [278400/467875]\n",
            "loss: 0.347653  [281300/467875]\n",
            "loss: 0.498967  [284200/467875]\n",
            "loss: 0.414801  [287100/467875]\n",
            "loss: 0.541238  [290000/467875]\n",
            "loss: 0.438570  [292900/467875]\n",
            "loss: 0.356459  [295800/467875]\n",
            "loss: 0.569112  [298700/467875]\n",
            "loss: 0.431276  [301600/467875]\n",
            "loss: 0.438549  [304500/467875]\n",
            "loss: 0.309122  [307400/467875]\n",
            "loss: 0.614784  [310300/467875]\n",
            "loss: 0.436541  [313200/467875]\n",
            "loss: 0.553646  [316100/467875]\n",
            "loss: 0.551246  [319000/467875]\n",
            "loss: 0.345925  [321900/467875]\n",
            "loss: 0.300433  [324800/467875]\n",
            "loss: 0.569810  [327700/467875]\n",
            "loss: 0.363141  [330600/467875]\n",
            "loss: 0.470512  [333500/467875]\n",
            "loss: 0.353417  [336400/467875]\n",
            "loss: 0.414620  [339300/467875]\n",
            "loss: 0.667456  [342200/467875]\n",
            "loss: 0.467377  [345100/467875]\n",
            "loss: 0.497985  [348000/467875]\n",
            "loss: 0.385000  [350900/467875]\n",
            "loss: 0.700460  [353800/467875]\n",
            "loss: 0.406153  [356700/467875]\n",
            "loss: 0.609744  [359600/467875]\n",
            "loss: 0.484403  [362500/467875]\n",
            "loss: 0.461443  [365400/467875]\n",
            "loss: 0.437396  [368300/467875]\n",
            "loss: 0.425249  [371200/467875]\n",
            "loss: 0.396401  [374100/467875]\n",
            "loss: 0.391271  [377000/467875]\n",
            "loss: 0.390468  [379900/467875]\n",
            "loss: 0.437403  [382800/467875]\n",
            "loss: 0.269836  [385700/467875]\n",
            "loss: 0.437685  [388600/467875]\n",
            "loss: 0.606347  [391500/467875]\n",
            "loss: 0.464264  [394400/467875]\n",
            "loss: 0.306973  [397300/467875]\n",
            "loss: 0.328231  [400200/467875]\n",
            "loss: 0.431732  [403100/467875]\n",
            "loss: 0.553350  [406000/467875]\n",
            "loss: 0.407288  [408900/467875]\n",
            "loss: 0.566461  [411800/467875]\n",
            "loss: 0.377278  [414700/467875]\n",
            "loss: 0.438357  [417600/467875]\n",
            "loss: 0.602621  [420500/467875]\n",
            "loss: 0.505158  [423400/467875]\n",
            "loss: 0.351230  [426300/467875]\n",
            "loss: 0.491622  [429200/467875]\n",
            "loss: 0.450148  [432100/467875]\n",
            "loss: 0.418066  [435000/467875]\n",
            "loss: 0.440923  [437900/467875]\n",
            "loss: 0.446341  [440800/467875]\n",
            "loss: 0.555686  [443700/467875]\n",
            "loss: 0.464424  [446600/467875]\n",
            "loss: 0.437827  [449500/467875]\n",
            "loss: 0.560127  [452400/467875]\n",
            "loss: 0.379962  [455300/467875]\n",
            "loss: 0.541836  [458200/467875]\n",
            "loss: 0.594990  [461100/467875]\n",
            "loss: 0.733287  [464000/467875]\n",
            "loss: 0.388411  [466900/467875]\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.476805 \n",
            "\n",
            "\n",
            "Time spend to epoch 0 h 1 m 25 s\n",
            "\n",
            "\n",
            "SPEND 2 h 22 m 30 s IN TOTAL.\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "epochs = 100\n",
        "start = time()\n",
        "for t in range(epochs):\n",
        "    begin = time()\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, criterion, optimizer, t)\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    test_loop(val_dataloader, model, criterion, t)\n",
        "    time_1 = time() - begin\n",
        "    print(\"\\nTime spend to epoch\", '%.0f' % (time_1//3600), \"h\", '%.0f' % (time_1%3600//60), \"m\", '%.0f' % (time_1%3600%60), \"s\\n\")\n",
        "\n",
        "time = time() - start\n",
        "print(\"\\nSPEND\", '%.0f' % (time//3600), \"h\", '%.0f' % (time%3600//60), \"m\", '%.0f' % (time%3600%60), \"s\", \"IN TOTAL.\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc= np.array(val_acc)*100\n",
        "train_acc = np.array(train_acc)*100"
      ],
      "metadata": {
        "id": "d6yH3z0TU7NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize= (7,5))\n",
        "plt.plot(train_acc, color = 'k', linestyle = ':', label = 'Точность на обучающей выборке')\n",
        "plt.plot(val_acc, color = 'k', linestyle = '-', label = 'Точность на тестовой выборке')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Точность, %')\n",
        "plt.title('Точность обучения с использованием оптимизатора Adam')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "cB4b8zFJU0vZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "7a3ec7d4-94f9-4365-e3fe-a92300be033b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb84de8eb00>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize= (7,5))\n",
        "plt.plot(train_loss, color = 'k', linestyle = ':', label = 'Потери на обучающей выборке')\n",
        "plt.plot(val_loss, color = 'k', linestyle = '-', label = 'Потери на тестовой выборке')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Потери, %')\n",
        "plt.title('Функция потерь с использованием оптимизатора Adam')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "z71DJ4YlVZeS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "fd61948b-90f5-4cd4-b358-22df7d85ae58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb84d931720>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAHWCAYAAAA2Of5hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB94UlEQVR4nO3deXgN1/8H8PfNvu97ZJNYEiGxJSJVVCrU2tqKVqjiW0sp1aK1VVUVrRatpS2K1lY7Va2iRBr7moiEJCKyr7Iv9/z+8GR+xk1ISOTi/XqeeciZz8ycmTP33s89c2auQgghQERERET1SqO+K0BERERETMqIiIiI1AKTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiKjezZkzBwqFor6rUa+YlNWD4cOHw8jIqL6rQUREVOs++ugjKBQKDBo0qL6r8szRqu8KvCgyMjKwadMmHD9+HP/++y8KCwvRrVs3tGzZEgMHDkTLli3ru4pERERPRAiB3377Da6urti7dy/u3r0LY2Pj+q7WM4M9ZU/B5s2b4erqiokTJ+LMmTPQ1taGQqFARkYGlixZglatWmH48OEoKSmp76oSERE9tqNHj+L27dv4+eefUVZWhh07dtR3lZ4pTMrqWGhoKN566y3Y2dkhNDQUsbGxCAoKgp6eHk6fPo07d+5g8ODBWL9+PT744AMA975puLq6ok+fPirrKyoqgqmpKcaMGQPg3gtAoVBg+/btKrFGRkYYPny49Pe6deugUCgQFxcnlV29ehXm5ubo2bMnysrKZHFnzpyRrS89PR0KhQJz5syRlVdWtmjRIigUCnTq1ElWfvPmTQwYMAAODg7Q0NCAQqGAQqGAt7f3ww6jtJ2qJldXV1lsfn4+pkyZAicnJ+jq6qJJkyZYvHgxhBDVWt+DdS8uLsbs2bPh4eEBXV1dODk54aOPPkJxcbFKHcePH49NmzahSZMm0NPTQ+vWrfHvv//K4irGTqSnpz9yvyuTmJiIkSNHwsHBAbq6unBzc8N777330MQ+Li4OCoUC69atk5WPGzcOCoVCdq4AQHZ2Nj744AO4urpCV1cXDRo0wLBhw6Q6V5x7VU0Prq+i7S0sLGBgYIB27dph//79ldZ1+PDhla7z/vOsusMA+vTpA1dXV+jp6cHGxga9e/fG5cuXZTFlZWWYN28e3N3doaurC1dXV8yYMUOlfV1dXaW6aGhowM7ODoMGDcKtW7dkcYsXL0b79u1haWkJfX19tG7dutLXaMX58qCePXvKzunHabtJkyZJ57+HhwcWLlwIpVKpsk6FQoFdu3bJli8qKoK5uTkUCgUWL16sUr8HpaamYuTIkbC1tYWenh58fHywfv16Wcz923vY6+1R59X950FlY5Dy8vJgZ2cHhUKBo0ePSuWdOnWCQqFA3759Veo/ZswYlfehmhzzHTt2wM/PDxYWFtDX10fTpk2xcOFC2ftNfHw8xo4diyZNmkBfXx+WlpYYMGCA7P244r33YdP99fnnn3/QoUMHGBoawszMDH369EFkZKSsvhXH6Nq1axg4cCBMTExgaWmJiRMnoqioSBa7du1avPLKK7CxsYGuri68vLzwww8/qByvh9m0aRO8vLzQuXNnBAUFYdOmTZXGnThxAm3btoWenh7c3d2xatWqSuOqWydXV1f07NkTR48eRZs2baCvr4/mzZtL58COHTvQvHlz6T35/PnzNdqvp4WXL+vYl19+CaVSic2bN6N169Yq862srPDLL78gIiICq1atwuzZs2FjY4O33noLX331FTIzM2FhYSHF7927F7m5uXjrrbeeuG4JCQno1q0bmjZtiq1bt0JLq3ZOh+zsbCxYsEClvLy8HL1790Z8fDwmTZqExo0bQ6FQYP78+dVe96uvvophw4bJypYsWYKsrCzpbyEEevfujSNHjmDkyJHw9fXFn3/+ialTpyIxMRHffPMNAGDDhg3SMsePH8fq1avxzTffwMrKCgBga2sLAFAqlejduzdOnDiB0aNHw9PTE5cvX8Y333yD69evq3ygHTt2DFu2bMH7778PXV1dfP/99+jWrRtOnTpVreTzUe7cuQM/Pz9kZ2dj9OjRaNq0KRITE7F9+3YUFBRAR0en2uuKiYnBmjVrVMrz8vLQoUMHREZG4p133kGrVq2Qnp6OPXv24Pbt29IxAoD3338fbdu2lS3/7rvvyv5OSUlB+/btUVBQgPfffx+WlpZYv349evfuje3bt+P1119XqYOVlZXUVgDw9ttvV3u/HjR69GjY2dnhzp07WL58OYKCghAbGwsDAwOpvuvXr0f//v0xZcoUhIeHY8GCBYiMjMTOnTtl6+rQoQNGjx4NpVKJK1euYOnSpbhz5w6OHz8uxXz77bfo3bs3hg4dipKSEmzevBkDBgzAvn370KNHj8fej/tV1XYFBQXo2LEjEhMTMWbMGDg7O+PkyZOYPn06kpKSsHTpUlm8np4e1q5dK0tWduzYofKBXZXCwkJ06tQJMTExGD9+PNzc3LBt2zYMHz4c2dnZmDhxoix+8ODBeO2112Rl06dPl/7v6ekpe22uXr0akZGRsnOhRYsWVdZnyZIlSElJqXSenp4e9u/fj9TUVNjY2Ej137JlC/T09B65r1Ud89zcXPj7+yMkJATa2to4ePAgpk2bBi0tLUyZMgUAcPr0aZw8eRJvvvkmGjRogLi4OPzwww/o1KkTIiIiYGBggJdfflm27xXvjZ988olU1r59ewDA33//je7du6Nhw4aYM2cOCgsLsWzZMgQGBuLcuXMqX1QHDhwIV1dXLFiwAP/99x++++47ZGVl4ZdffpFifvjhBzRr1gy9e/eGlpYW9u7di7Fjx0KpVGLcuHGPPD7FxcX4/fffpX0ePHgwRowYgeTkZNjZ2Ulxly9fRteuXWFtbY05c+agrKwMs2fPlt5z71eTOsXExGDIkCEYM2YM3nrrLSxevBi9evXCypUrMWPGDIwdOxYAsGDBAgwcOBBRUVHQ0FCzvilBdcrCwkK4uLjIykJCQoShoaGsbObMmQKA2Lt3rxBCiKioKAFA/PDDD7K43r17C1dXV6FUKoUQQhw5ckQAENu2bVPZtqGhoQgJCZH+Xrt2rQAgYmNjRWZmpvDy8hJNmjQR6enpsuUq4k6fPi0rT0tLEwDE7NmzZeUPln300UfCxsZGtG7dWnTs2FEqr9inBQsWyJbv2LGjaNasmUr9HwRAjBs3TqW8R48esmO8a9cuAUB8/vnnsrj+/fsLhUIhYmJiVNZx/7F50IYNG4SGhoY4fvy4rHzlypUCgAgNDZXVEYA4c+aMVBYfHy/09PTE66+/LpXNnj1bABBpaWmP3O8HDRs2TGhoaKi0jxBCOi8qExsbKwCItWvXSmUDBw4U3t7ewsnJSXauzJo1SwAQO3bsqHIbNTn3Jk2aJADIjuHdu3eFm5ubcHV1FeXl5bLlhw4dKtzc3GRlD55nlb2OqmPr1q2yNrpw4YIAIN59911Z3IcffigAiH/++Ucqc3Fxke2XEEIMGTJEGBgYyMoKCgpkf5eUlAhvb2/xyiuvqOxTdc7pmrTdvHnzhKGhobh+/bpsndOmTROampri1q1bsnUOHjxYaGlpieTkZCm2S5cuYsiQIQKAWLRokUr97rd06VIBQGzcuFG2vwEBAcLIyEjk5ubKtlfZ+po1ayZ7r7hfSEiIyntohYrXUYXU1FRhbGwsunfvLgCII0eOSPMq3mdatGghFi9eLJVv2LBBNGjQQHTo0EH2PlSTY14ZLy8v0bNnT+nvB88JIYQICwsTAMQvv/xS6To6duxY5XHx9fUVNjY2IiMjQyq7ePGi0NDQEMOGDZPKKo5R7969ZcuPHTtWABAXL158aB2Dg4NFw4YNK9/JB2zfvl0AENHR0UIIIXJzc4Wenp745ptvZHF9+/YVenp6Ij4+XiqLiIgQmpqasvasSZ1cXFwEAHHy5Emp7M8//xQAhL6+vmxbq1atUjk/1IWapYjPn7t370rfyB6m4htCbm4uAKBx48bw9/eXdf1mZmbijz/+wNChQ1W67O/evYv09HTZVJWioiL07t0baWlpOHjwICwtLR9n1yqVmJiIZcuWYebMmSqXlu7evQsAtbq9yhw4cACampp4//33ZeVTpkyBEAJ//PFHjda3bds2eHp6omnTprLj+8orrwAAjhw5IosPCAiQ9Yo6OzujT58++PPPP1FeXi6LzczMRHp6OvLz86tVF6VSiV27dqFXr15o06aNyvya3E5+9uxZbNu2DQsWLFD5tvj777/Dx8en0h6sx7ll/cCBA/Dz88NLL70klRkZGWH06NGIi4tDRESELL6kpAS6urrVWndFezysZ6egoADp6em4cOEC1qxZA1tbWzRu3FiqGwBMnjxZtkzFt/0HL7EWFxcjPT0dqamp+Ouvv/DPP/+gS5cushh9fX3p/1lZWcjJyUGHDh1w7tw5lboVFRWpvHZLS0sfus8Pa7tt27ahQ4cOMDc3l60zKCgI5eXlKpfSW7VqhWbNmkk9NPHx8Thy5IjKJdGqHDhwAHZ2dhg8eLBUpq2tjffffx95eXk4duxYtdZTG+bNmwdTU1OV1/79RowYgbVr10p/r127FiEhIY/sMXnYMa+Qnp6O27dvY926dYiJicHLL78szbv/nCgtLUVGRgY8PDxgZmZW6XnxMElJSbhw4QKGDx8uu5LSokULvPrqq9I5fb8He5UmTJgAALLY++uYk5OD9PR0dOzYETdv3kROTs4j67Vp0ya0adMGHh4eAABjY2P06NFD9jlWXl6OP//8E3379oWzs7NU7unpieDgYJV11qROXl5eCAgIkP729/cHALzyyiuybVWU37x585H79LQxKatjDg4OuHHjxiPjYmJiAACOjo5S2bBhwxAaGor4+HgA995sS0tLK72M884778Da2lo2VfVBP2LECJw4cQJ3796VxpHVltmzZ8PBwUEa83a/Jk2awNzcHEuWLEFoaCjS0tKq9QFUU/Hx8XBwcFC548fT01OaXxPR0dG4evWqyvGt+FBPTU2VxTdq1EhlHY0bN0ZBQQHS0tJk5U2aNIG1tTWMjIxga2uLTz/9VCVxu19aWhpyc3Nr5TLotGnT0KFDB/Ts2VNl3o0bN2plGxXi4+PRpEkTlfKq2iQ7O7ta48Xy8/Ol9tDX14ezszO+/fZblbjPPvsM1tbWaNmyJeLi4nD06FHp/IiPj4eGhob0QVLBzs4OZmZmKnXbvHkzrK2tYWtri65du8LJyQk//vijLGbfvn1o164d9PT0YGFhAWtra/zwww+VfrD99NNPKufWoUOHHrrfD2u76OhoHDx4UGWdQUFBAFTPV0CeqKxbtw7t27ev9DyuTHx8PBo1aqSSqDzu6+1xxcbGYtWqVZg7d+5DL0UOHToU169fx6lTp6RzoToJ6MOOOXAvuba2toaTkxPeeecdTJ06FVOnTpXmFxYWYtasWdI4PysrK1hbWyM7O7taCc/9Ko5pVa+pyr7oPdie7u7u0NDQkI1pCw0NRVBQkDRGzdraGjNmzACAR9YxOzsbBw4cQMeOHRETEyNNgYGBOHPmDK5fvw7g3ntYYWFhpedXZftTkzrdn3gBgKmpKQDAycmp0vL7h72oC44pq2M9e/bEihUr8NNPP2HkyJGVxqSkpGD9+vWwtrZGu3btpPI333wTH3zwATZt2oQZM2Zg48aNaNOmTaUn7qxZs9ChQwdZWa9evSrd3rlz57B7926MHz8eo0ePxj///PMEe/j/IiMjsW7dOmzcuBHa2toq842MjLBlyxa88847sh4TAGjWrFmt1KEuKJVKNG/eHF9//XWl8x98wdfE77//DhMTExQUFGDnzp2YP38+TExM8NFHHz32Oqvj0KFD+PvvvxEWFlan23lcycnJcHFxeWScnp4e9u7dC+BeT+zPP/+MSZMmwd7eHgMHDpTi3n33XXTp0gW3b9/GN998g379+uHkyZPSmzNQ/R7Arl27Sh+2t2/fxsKFC9G5c2ecOXMG+vr6OH78OHr37o2XX34Z33//Pezt7aGtrY21a9fi119/VVlfnz59VAb7f/rpp0hOTq50+49qO6VSiVdffbXKc6jiy8T93nrrLXz00Uf477//sH79enz66acPPQbq6JNPPkGjRo0QEhIiG9/3IGtra/Tq1Qtr166Fra0tAgMDVRLyB1Xn9aKjo4O//voLBQUFOH78OBYuXAgnJyfpC+qECROwdu1aTJo0CQEBATA1NYVCocCbb74puwHjaXnwfL9x4wa6dOmCpk2b4uuvv4aTkxN0dHRw4MABfPPNN4+s47Zt21BcXIwlS5ZgyZIlKvM3bdqEuXPn1qiONa2TpqZmpeupqlzcdyOGumBSVsc+/fRT7Nq1C++99x6uXbuGIUOGSD0ht27dwuHDhzFr1ixkZWXh119/lV2ysbCwkLp+hw4ditDQUJVBuhWaN28ufROuUNWJ+OOPP6J3797Q1NREz549H5ow1sT06dPh6+v70AcGvvrqq/jqq68wdOhQrFy5Eg0bNsSUKVMe2jtUUy4uLvj7779Vno9z7do1aX5NuLu74+LFi+jSpUu1Prijo6NVyq5fvw4DAwNYW1vLyl9++WVp0Hzv3r0RGhqKgwcPVvmBam1tDRMTE1y5cqVG+3A/IQSmTZuG119/XfYl4H7u7u5PtI0Hubi4ICoqSqW8sjYpLS1FTEwMunXr9sj1ampqys77Hj16wMLCAgcPHpQlZR4eHtIHb1BQEJydnfHrr7/ivffeg4uLC5RKJaKjo6XeHeDel6Xs7GyV88Xe3l62zSZNmqB9+/bYtWsXBg8ejN9//x16enr4888/Za/n+y+Z3a9BgwYqr92lS5dWmpRVt+3y8vJU1vkwlpaW6N27N8aMGYPU1FQMHDiw2ncGu7i44NKlS1AqlbLessd9vT2O8+fPY/Pmzdi1a1eV73v3e+eddzB06FCYmpqq3Dn+oOoccwDQ0NCQjnnv3r2RmZmJWbNmSUnZ9u3bERISIktYioqKkJ2d/egdfEDFMa3qNWVlZQVDQ0NZeXR0NNzc3KS/Y2JioFQqpRsC9u7di+LiYuzZs0fW4/Tg8IyqbNq0Cd7e3pg9e7bKvFWrVuHXX3/F3LlzpV7tyt4nH9yfJ63Ts4iXL+uYnZ0dwsLC0L17d+mZZBs3bkR+fj5cXFzwzjvvQF9fH3v37pWNyajw9ttvIyIiAlOnToWmpibefPPNJ65TRY9ajx498Oabb2Lq1KlV3q1UXWFhYdi9eze+/PLLhyYuCQkJGDt2LN5//32MHj0aQUFBMDc3f6JtP+i1115DeXk5li9fLiv/5ptvoFAo0L179xqtb+DAgUhMTKz0rqvCwkKVywRhYWGyMSIJCQnYvXs3unbt+tAPDCEEhBAPjdHQ0EDfvn2xd+9elUeWVKzjUTZv3oxLly5VeodshX79+uHixYsqdx5WdxsPeu2113Dq1ClZT0N+fj5Wr14NV1dXeHl5SeW7d+9GYWGhNGavJirq9rBjWJFsVDzuouJOwAe/8FT0jD7qbsnCwkLZ+jQ1NaFQKGRfNOLi4lTu0n0c1Wm7gQMHIiwsDH/++afKvOzs7CqHLLzzzju4dOkSBgwYUKNfHHnttdeQnJyMLVu2SGVlZWVYtmwZjIyM0LFjx2qv63FNmzYNgYGB6N27d7Xiu3XrBkNDQ2RmZsqS98pU55hXJj09XfZIFU1NTZXXzrJlyx7rC6m9vT18fX2xfv16WVJ35coVHDp0SOXuVgBYsWKFyrYBSO+HFa+Z++uYk5NT5ZeJ+yUkJODff//FwIED0b9/f5VpxIgRiImJQXh4ODQ1NREcHIxdu3bJHiUTGRmpcs4+SZ2eVewpewqcnJywe/duJCUlITQ0FIsWLcKFCxewcuVK+Pr6wtfXt8pEpkePHrC0tMS2bdvQvXv3at00UBPffvstPD09MWHCBGzdulU2LywsTPZtueImhJiYGJw6dQp+fn7SvEOHDuHVV1996LdzpVKJt99+Gw0aNMCXX35Zq/txv169eqFz58745JNPEBcXBx8fHxw6dAi7d+/GpEmT4O7uXqP1vf3229i6dSv+97//4ciRIwgMDER5eTmuXbuGrVu34s8//5QNuvf29kZwcLDskRgAKu26/+eff2SXL2NiYjBp0qSH1ueLL77AoUOH0LFjR+kRHUlJSdi2bRtOnDgBMzOzhy5/6NAhjBo1qtLL4BWmTp2K7du3Y8CAAXjnnXfQunVrZGZmYs+ePVi5ciV8fHweuo0HTZs2Db/99hu6d++O999/HxYWFli/fj1iY2Px+++/Q0NDAwUFBZg9eza+//57tG/fHl27dn3kesvLy3Hw4EEA9y5frl27Fvn5+dLjHQ4cOIAff/wR7du3h4WFBW7evIk1a9bA0NBQuonBx8cHISEhWL16NbKzs9GxY0ecOnUK69evR9++fdG5c2fZNm/evImNGzcCuHdjy/Lly2FiYiIN9u/Rowe+/vprdOvWDUOGDEFqaipWrFgBDw8PXLp0qUbH7UHVbbs9e/agZ8+eGD58OFq3bo38/HxcvnwZ27dvR1xcnOyRJhW6deuGtLS0Gv8E3OjRo7Fq1SoMHz4cZ8+ehaurK7Zv3y717D+Np7kfOnQIoaGh1Y7X1NREZGQkhBAqPUqVrftRx7xfv37w8PCAu7s7SkpKcPDgQezfv192Wbpnz57YsGEDTE1N4eXlhbCwMPz999+PfePTokWL0L17dwQEBGDkyJHSIzGq6v2LjY1F79690a1bN4SFhWHjxo0YMmSI9Fru2rUrdHR00KtXL4wZMwZ5eXlYs2YNbGxskJSU9NC6/Prrr9KjiCrz2muvQUtLC5s2bYK/vz/mzp2LgwcPokOHDhg7dqyUxDdr1kz2GnmSOj2z6uGOzxdeTW/lr7h1+ddff1WZ97iPxLjf+vXrBQCxZ88eWdzDpvtv0wYgFAqFOHv2rGy9D97O/cUXXwhdXV3ZLdgVcbX5SAwh7j1u4YMPPhAODg5CW1tbNGrUSCxatKjKR0Y87JEYQty7xX/hwoWiWbNmQldXV5ibm4vWrVuLuXPnipycHJU6bty4UTRq1Ejo6uqKli1bqtx6XXGbesWkr68vvLy8VG4dr0p8fLwYNmyYsLa2Frq6uqJhw4Zi3Lhxori4uMplKm7x19fXF4mJibJ5lT3qISMjQ4wfP144OjoKHR0d0aBBAxESEiI9QqUm554QQty4cUP0799fmJmZCT09PeHn5yf27dsnzb99+7ZwcnISkyZNkh3TCqjkkRj3H0MjIyPRqlUrsWHDBinmypUromvXrsLS0lLo6OgIJycn8eabb4pLly7J1l1aWirmzp0r3NzchLa2tnBychLTp08XRUVFKsfp/m1aWVmJrl27irCwMFncTz/9JLV/06ZNxdq1a1Ue31CxTzV5JEZ12+7u3bti+vTpwsPDQ+jo6AgrKyvRvn17sXjxYlFSUiJbZ1WPvHjU/PulpKSIESNGCCsrK6GjoyOaN28ue5TEo9b3pI/E6NOnj6y84tys7JEYVXlwfk2O+Zw5c0STJk2Evr6+MDExEb6+vuLbb78VpaWlUkxWVpZ0jIyMjERwcLC4du1ape13f52qOi5CCPH333+LwMBAabu9evUSERERspiKYxQRESH69+8vjI2Nhbm5uRg/frwoLCyUxe7Zs0e0aNFC6OnpCVdXV7Fw4ULx888/P/S9UQghmjdvLpydnaucL4QQnTp1EjY2NtIxOXbsmGjdurXQ0dERDRs2FCtXrqz0NVLdOrm4uIgePXqobLey11hNzu2nTSGEGo50I5kPPvgAP/30E5KTk6WHXdanOXPm4OjRo7KnZdM9CoUC48aNU7l0SkRUH+bMmYO5c+ciLS2t0h5SUi8cU6bmioqKsHHjRvTr108tEjIiIiKqGxxTpqZSU1Px999/Y/v27cjIyFD5qZL65OHhgYKCgvquBhER0XOFSZmaioiIwNChQ2FjY4PvvvsOvr6+9V0lSW387iYRERHJcUwZERERkRrgmDIiIiIiNcCkjIiIiEgNvHBjypRKJe7cuQNjY+Nq/9YdERER0eMSQuDu3btwcHCQ/RzZg164pOzOnTtP9APSRERERI8jISEBDRo0qHL+C5eUVfzkR0JCAkxMTOq5NkRERPS8y83NhZOT0yN/duyFS8oqLlmamJgwKSMiIqKn5lHDpjjQn4iIiEgNMCkjIiIiUgNMyoiIiIjUgFokZStWrICrqyv09PTg7++PU6dOVRm7bt06KBQK2aSnp/cUa0tERERU++o9KduyZQsmT56M2bNn49y5c/Dx8UFwcDBSU1OrXMbExARJSUnSFB8f/xRrTERERFT76j0p+/rrrzFq1CiMGDECXl5eWLlyJQwMDPDzzz9XuYxCoYCdnZ002draPsUaExEREdW+ek3KSkpKcPbsWQQFBUllGhoaCAoKQlhYWJXL5eXlwcXFBU5OTujTpw+uXr1aZWxxcTFyc3NlExEREZG6qdekLD09HeXl5So9Xba2tkhOTq50mSZNmuDnn3/G7t27sXHjRiiVSrRv3x63b9+uNH7BggUwNTWVJj7Nn4iIiNRRvV++rKmAgAAMGzYMvr6+6NixI3bs2AFra2usWrWq0vjp06cjJydHmhISEp5yjYmIiIgerV6f6G9lZQVNTU2kpKTIylNSUmBnZ1etdWhra6Nly5aIiYmpdL6uri50dXWfuK5EREREdalee8p0dHTQunVrHD58WCpTKpU4fPgwAgICqrWO8vJyXL58Gfb29nVVTSIiIqI6V++/fTl58mSEhISgTZs28PPzw9KlS5Gfn48RI0YAAIYNGwZHR0csWLAAAPDZZ5+hXbt28PDwQHZ2NhYtWoT4+Hi8++679bkbRERERE+k3pOyQYMGIS0tDbNmzUJycjJ8fX1x8OBBafD/rVu3oKHx/x16WVlZGDVqFJKTk2Fubo7WrVvj5MmT8PLyqq9dUJGTkwMTE5NH/vAoERERUQWFEELUdyWeptzcXJiamkqJU20rLi5GQEAAGjVqhNWrV8PU1LTWt0FERETPjurmHs/c3ZfqLjQ0FJcvX8bWrVvRsmVLhIeH13eViIiI6BnApKyWvfLKKzhx4gRcXV0RGxuLl156CYsWLYJSqZRikpOTsWLFCgQHB6Nbt26YNWsW9u/fj/T09HqsOREREdUnXr6sI9nZ2Rg9ejS2bdsGAAgODkbPnj2xfft2/Pvvv6jqsDds2BCdOnVC37598eqrr/LH1omIiJ5x1c09mJTVISEEfvzxR7z//vsoKiqSzfP398eAAQOgr6+P8PBwhIeHIyoqShZjaGiIbt264fXXX4evry8MDQ1hYGAAQ0ND6Ovro6CgAKmpqUhJSUFqairS0tJgamqKxo0bw8PDA4aGhnW6f0RERPRoTMqq8DSTsgpXr17F6NGjIYRA//790b9/fzg7O6vEZWVlITw8HPv378euXbuq/Omo6mrQoAEaN24MS0tL6SG6FZODgwM8PDzg4eGBhg0bwsDAAABQWlqK1NRUJCcnIzU1FQ4ODvDy8oK2tna1tllcXIzbt28jKSkJ7u7u1Xp+nFKpxJ07dxAbG4vY2FgkJSWhY8eOaNeu3RPt/8MUFBTg2rVriIyMREREBCIjI5GSkoJBgwZh7Nix0NKq9xuTiYjoOcGkrAr1kZQ9DiEEzp07h127dmHv3r1ITExEfn4+CgsLZXH6+vqwtbWFjY0NrK2tkZmZievXryMjI6NG27O3t0dpaWml49p0dHTQvHlztGzZEr6+vtDS0kJmZqZsun37NhISElR+naFJkybo3LkzOnfujA4dOiAnJwdXrlzB5cuXceXKFVy9ehWxsbEoKSlR2e5LL72EDz/8EL169ZI9FuVxKZVK/P3331i5ciX27NmD8vLySuO8vb3x3XffoXPnzk+8TSIiIiZlVXhWkrKqKJVKFBYWIj8/X7qUWdnz0DIyMhAdHY3o6Gjk5OSguLhYmgoLC3H79m3Z/PtpamrC1tYW1tbWiI+PR3Z2do3qqK+vDxsbG9y6davKsXMP0tTUhLOzMxo2bAgjIyMcOHAApaWlAO4ldh988AHc3NxQUFCAgoIC5OfnS/+//+/i4mLY2dnBzc0Nbm5ucHV1haGhITZt2oRVq1bhxo0b0jYtLS3h5eUFLy8veHp6ory8HPPnz0dmZiYAYMCAAVi8eDGcnJxQWFiIrKwsZGZmIj8/H46OjnB0dHyiZFEIgZiYGMTGxqJFixYP/WmxgoICREREID4+Hrdu3ZL+zcnJQfPmzdG2bVu0bdsWHh4eUp2EEMjMzMSdO3eQlpaG4uJilJSUSJO2tjZat26Nhg0b1skz9ZRKJcrKyqCjo1PtZVJSUrBr1y7s2LED2dnZ6NevH9566y04ODjUev2IHqRUKrFs2TIsWbIEbdu2RUhICLp3717tKwVEVWFSVoVnPSmrbRUf3Ddv3oSenh7s7OxgaWkp+2CPi4vDuXPncP78eVy6dAmampowNzeHhYUFLCwsYG5uDkdHRzg5OcHZ2RkWFhZQKBTIysrCv//+iyNHjuCff/7B5cuXYWhoiGbNmsHb2xve3t5o1qwZGjdujAYNGsguGd65cwfLli3DDz/8oJI0PglTU1MMGzYMY8aMQbNmzVTmZ2ZmYubMmVi5ciWUSiW0tLSgoaFRaU+ejo4O3Nzc4O7uDjc3N5iZmcHExATGxsYwNjaGkZGRtLympiY0NDRQUFCAs2fP4tSpUzh16hSysrKk9bm7u6NDhw546aWX4OnpiStXruD06dM4ffo0rly5UmXP3oP717hxY6Snp+POnTsoLi5+5DI2NjYICAhAQEAAGjdujMTERMTGxiIuLg6xsbHIz89H8+bN0bp1a2mytLREdnY24uPjERcXh/j4eCQkJOD27dvSlJiYiLKyMri4uKBx48bS5OTkJB0XDQ0NKBQKREVF4ffff8fx48dVEnkNDQ28+uqrGD58OPr06QN9ff1H7lNdKiwsxOHDh5Gfn482bdrUWVJbHWlpadi7dy+MjY3RvXt3GBkZ1Us9ngc3btzAiBEjcPz4cVm5tbU1hgwZgmHDhqFly5Z8KDg9FiZlVWBSVn/y8/Ohr69fo96lu3fv4qeffsLGjRtRVlYGAwMD2c0O99/8YGBgAG1tbdn4tNjYWBQWFqJt27b43//+h0GDBlXrBoiLFy9iwoQJsjfoimTUwMAAd+7cQVlZ2WMdh/vp6urC2dkZMTExj+xVtLa2hru7O5ydneHi4gJnZ2cYGhri/PnzOH36NM6fP19pEmZlZQVbW1vo6elBR0cH2tra0NHRQW5uLs6fPy/1SNaEgYEBCgoKarxcdbRt2xb9+vWDmZkZNmzYgNDQUGmelpYW3Nzc0KhRI2lycXGRvhxUTFpaWsjNzUVOTg6ys7ORk5ODrKwspKenIz09HWlpaUhPT4eOjo7sS4KtrW2lH7p5eXk4cOAAfv/9d+zfvx/5+fnSPAsLC6mnsmHDhtDW1oa2tja0tLSgra0NAwMDmJubw8zMDObm5jA1NUVRUZGUvFYkszo6OnB3d5cmY2PjSo9PQUEB9u7di40bN+LgwYPSeaivr48ePXpg4MCBeO2112BoaAilUom0tDQkJibizp070NLSgpWVlTRV9LSXlpYiPz9f6nE2MjKCjY0NNDU1Zdu+e/cuzpw5g/DwcJw7dw76+vpo2LAh3N3d0bBhQ7i6uqKsrAypqanSlJ6ejiZNmqBLly7S2NUHpaSk4Pjx49DX14ezszOcnZ3r5MHb5eXlKvukVCqxcuVKTJ06FQUFBTA0NMTcuXNx584dbNq0STYkw83NDT179kTPnj3RsWNH6OrqSvNKS0tx+/ZtxMbG4saNG9IUExOD9PR0dOvWDe+99x5atWr10DoKIVBaWorCwkIUFBRAqVTCwcGhVpJBIQRycnKkL4xPoqSkBKdOnUJcXBw8PT3h7e0tOx7VqUtZWVmNeyELCgpw7tw55Obmol27drCwsKhp1esFk7IqMCl7sQghkJeXV+UH3KOWvXnzJrS0tGBhYQEjIyPpjbGsrAwJCQm4efMmbty4gfj4eOTk5ODu3bvSlJeXh/LyciiVSulfTU1NtGjRAv7+/vDz80OLFi2gra2N7OxshIWF4cSJEzh+/Diio6Ph5eUFPz8/+Pn5oW3btnB0dHzoG3NpaSmuXLmC2NhY2NrawtHREfb29g99oywqKsK5c+cQFhaGsLAwxMfHw8nJSbr06+bmBj09PVy4cAFnzpzB2bNnERMTIy1vZWUFFxcXKUl0cnJCgwYNpElHRwcxMTG4fv06oqKicP36dSQnJ0OpVEqTEALm5ubo1asX3njjDZWbYGJiYvDLL79g/fr1uHXrVo3bsSasrKzQoEED2XEWQuDatWuyO6idnJxgb2+PCxcuVNqLWhusra1hY2Mju0FHS0sL4eHhuHv3rhTXsmVL5Obmyi7NGxgYwNLSEklJSQ/98lBxabmyfagYxuDg4ABbW1vExcUhIiKi2kMSHqSnp4dXXnkFPXr0QNeuXREfH49Dhw7hzz//xMWLF1XiTU1N4eTkJJ2/QghpKisrQ0lJCUpLS1FaWoqysjJYWlrC3t4eDg4OcHBwgLW1NdLS0qTe3Li4OCQlJcHAwEAah2tra4vU1FT8999/AIBOnTrh559/hpubG4B7r/NDhw5h/fr12L17t+xLj5GREV5++WXk5+cjLi4Ot2/frlZvtr+/P8aOHYuBAwciNTVVuvs+PDwcly9flt437mdtbY327dsjMDAQL730Elq1aiW9t+Xl5UnJdHl5OYQQ0msrLy8PkZGR0vjdiIgI5OXlSfWv+BJjYWEhO3YODg6wsbGBvr6+7PzLzs6WrnwcP35c9uVES0sLnp6eaNmyJby8vFSuHJSWliIiIgJXrlyR6pOXlwdvb2/pPc7Pzw+NGjVCfn6+9KUqNzcXcXFxsmNUcXwUCgV8fHzQqVMndO7cGX5+frCyslJJOEtLSxETEyPd2JWUlCSdN/f/+91339XZUAkmZVVgUkb0ZLKzs5GSkoIGDRo81ceuCCFkYyErpsTERGRlZSErKwvZ2dmypEFfXx+mpqYwNTWFmZkZrK2tYW1tLfUU5efnSx8SMTExsoc8P8jd3R39+/dHv3790KZNGygUCpSUlODSpUs4ffo0Tp06hZSUFJU3+vz8fKlu93+IGRkZwcnJCU5OTnB0dERxcbHUu/KoB0m7urpi6NChGDp0KDw9PSGEwPnz57F161Zs3boVsbGxUqxCoZCSK6VSKfUUVtarqqmpCQMDA+Tn51d5LJydneHv74+2bduirKxM+mJy8+ZNJCQkQEtLCzY2NtJkYmKC8PBwxMfHP3SffHx8oKGhgfj4eGlc59Oir6+PhQsXYty4cVX25Ofn5+Pw4cPYu3cv9u3bh+TkZJUYXV1duLi4yHo8PTw8oKOjg/Xr12Pbtm1Sz7S2tvYje6krLu9XJ9mrD1ZWVmjatCkiIiKeapvZ29vDxMRE5TFSFYyNjaVks7i4GNHR0dW6snH16tU6+x1tJmVVYFJG9PxSKpXIyclBWVkZTE1Na3STQWFhISIjI5GWlqYyz8HBAd7e3k98CamkpATZ2dnQ1dV96OW5nJwc3LhxA9nZ2bKbdIqLi+Hu7o6AgIAqkwchBC5duoTi4mI4ODjAzs5OpedACIGCggKkp6dDQ0MDhoaGMDQ0hI6ODhQKhXQJ8s6dO7hz5w6SkpJgZ2cHf3//h96QUlZWBk1NTZXjJITA1atXsX//fuzbtw8nT56Era0tunbtiuDgYAQFBcHa2lqKz8vLQ0JCAhISElBaWiqtT6FQQKFQQEtLS7oUr62tDU1NTaSnpyMpKUmqb2pqKqysrODq6gpXV1e4uLigQYMGyM/Plz3fMS8vD3369IG7u/sj26+CUqnE+fPncfLkSVhaWkq9yra2tg8dnpGSkoKff/4Zq1atQnx8PLS0tKSec39/f7Ru3RoWFhYwMDCAvr4+dHR0UFJSgnPnziE0NBQnTpxAaGioLGnX09OThm9UjF2tmHR1ddG4cWNp/G6zZs3g5uYm+6KQlZWFjIwM6dhVTKmpqSgqKpJuECouLoaWlhYCAwPRpUsXvPLKK/D29oaGhob0hen8+fO4cOECYmJikJubK7tyIISQLnNWTCYmJtIY24rxsxVjiI2NjWFiYgITExPY2tqibdu20nGquGqQnJyMY8eO4ciRIzhy5AiuX79e5bE3NDSUbuxydnaWzp+KYQZaWloYNGgQLC0tq30e1ASTsiowKSMiql8Vd/++qIPmy8vLER0dDRcXlxrfuCKEQFpaGnR1dWFoaPhcPVOx4pKrkZHRY93ZXlZWJiWamZmZyMrKgoaGBjw9PVWGJTxtTMqqwKSMiIiInqbq5h78QXIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNaAWSdmKFSvg6uoKPT09+Pv749SpU9VabvPmzVAoFOjbt2/dVpCIiIiojtV7UrZlyxZMnjwZs2fPxrlz5+Dj44Pg4GCkpqY+dLm4uDh8+OGH6NChw1OqKREREVHdqfek7Ouvv8aoUaMwYsQIeHl5YeXKlTAwMMDPP/9c5TLl5eUYOnQo5s6di4YNGz7F2hIRERHVjXpNykpKSnD27FkEBQVJZRoaGggKCkJYWFiVy3322WewsbHByJEjH7mN4uJi5ObmyiYiIiIidVOvSVl6ejrKy8tha2srK7e1tUVycnKly5w4cQI//fQT1qxZU61tLFiwAKamptLk5OT0xPUmIiIiqm31fvmyJu7evYu3334ba9asgZWVVbWWmT59OnJycqQpISGhjmtJREREVHNa9blxKysraGpqIiUlRVaekpICOzs7lfgbN24gLi4OvXr1ksqUSiUAQEtLC1FRUXB3d5cto6urC11d3TqoPREREVHtqdeeMh0dHbRu3RqHDx+WypRKJQ4fPoyAgACV+KZNm+Ly5cu4cOGCNPXu3RudO3fGhQsXeGmSiIiInln12lMGAJMnT0ZISAjatGkDPz8/LF26FPn5+RgxYgQAYNiwYXB0dMSCBQugp6cHb29v2fJmZmYAoFJORERE9Cyp96Rs0KBBSEtLw6xZs5CcnAxfX18cPHhQGvx/69YtaGg8U0PfiIiIiGpMIYQQ9V2Jpyk3NxempqbIycmBiYlJfVeHiIiInnPVzT3YBUVERESkBpiUEREREakBJmVEREREaoBJGREREZEaYFJGREREpAaYlBERERGpASZlRERERGqASRkRERGRGmBSRkRERKQGmJQRERERqQEmZURERERqgEkZERERkRpgUkZERESkBpiUEREREakBJmVEREREaoBJGREREZEaYFJGREREpAaYlBERERGpASZlRERERGqASRkRERGRGmBSRkRERKQGmJQRERERqQEmZURERERqgEkZERERkRpgUkZERESkBpiUEREREakBJmVEREREaoBJGREREZEaYFJGREREpAaYlBERERGpASZlRERERGqASRkRERGRGmBSRkRERKQGmJQRERERqQEmZURERERqgEkZERERkRpgUkZERESkBpiUEREREakBJmVEREREaoBJGREREZEaYFJGREREpAaYlBERERGpASZlRERERGqASRkRERGRGmBSRkRERKQGmJQRERERqQEmZURERERqgEkZERERkRpgUkZERESkBpiUEREREakBJmVEREREaoBJGREREZEaYFJGREREpAaYlBERERGpASZlRERERGqASRkRERGRGmBSRkRERKQGmJQRERERqQEmZURERERqgEkZERERkRpgUkZERESkBpiUEREREakBJmVEREREakAtkrIVK1bA1dUVenp68Pf3x6lTp6qM3bFjB9q0aQMzMzMYGhrC19cXGzZseIq1JSIiIqp99Z6UbdmyBZMnT8bs2bNx7tw5+Pj4IDg4GKmpqZXGW1hY4JNPPkFYWBguXbqEESNGYMSIEfjzzz+fcs2JiIiIao9CCCEed+ErV67g2LFjKC8vR2BgIFq3bl3jdfj7+6Nt27ZYvnw5AECpVMLJyQkTJkzAtGnTqrWOVq1aoUePHpg3b94jY3Nzc2FqaoqcnByYmJjUuL5ERERENVHd3OOxe8pWrFiBLl264NixYzhy5AheeeUVzJ8/v0brKCkpwdmzZxEUFPT/FdLQQFBQEMLCwh65vBAChw8fRlRUFF5++eVKY4qLi5GbmyubiIiIiNSNVnUDExIS4OTkJP29fPlyXL16FVZWVgCAsLAw9O7dG5988km1N56eno7y8nLY2trKym1tbXHt2rUql8vJyYGjoyOKi4uhqamJ77//Hq+++mqlsQsWLMDcuXOrXSciIiKi+lDtnrKgoCB8++23qLjaaWlpiYMHD6K4uBh3797F33//DWtr6zqr6P2MjY1x4cIFnD59GvPnz8fkyZNx9OjRSmOnT5+OnJwcaUpISHgqdSQiIiKqiWr3lJ0+fRrTpk2Dv78/Vq9ejdWrV+Ptt9/GsGHDoFAo4OnpifXr19do41ZWVtDU1ERKSoqsPCUlBXZ2dlUup6GhAQ8PDwCAr68vIiMjsWDBAnTq1EklVldXF7q6ujWqFxEREdHTVu2kzMTEBN9//z1OnjyJ4cOH45VXXsHx48dRXl6O8vJymJmZ1XjjOjo6aN26NQ4fPoy+ffsCuDfQ//Dhwxg/fny116NUKlFcXFzj7RMRERGpixoP9G/fvj3OnDkDc3NztGzZEv/+++9jJWQVJk+ejDVr1mD9+vWIjIzEe++9h/z8fIwYMQIAMGzYMEyfPl2KX7BgAf766y/cvHkTkZGRWLJkCTZs2IC33nrrsetAREREVN+q3VNWVlaG1atXIzIyEj4+PpgxYwYGDRqE//3vf1i3bh2WL1+uMmC/OgYNGoS0tDTMmjULycnJ8PX1xcGDB6V13bp1Cxoa/5875ufnY+zYsbh9+zb09fXRtGlTbNy4EYMGDarxtomIiIjURbWfUxYSEoLTp0+jd+/eOH78OFq3bo3vvvsOAPDTTz9h/vz5mDp1Kt577706rfCT4nPKiIiI6Gmqbu5R7aTMzMwMYWFh8PT0REFBAZo3b44bN25I81NTUzFp0iT8+uuvT177OsSkjIiIiJ6mWn94rK2tLQ4dOoSSkhL8888/sLS0lM23sbFR+4SMiIiISF1Ve0zZ8uXLMXToUEyePBn29vbYunVrXdaLiIiI6IVS7aTs1VdfRUpKCtLT05/aQ2KJiOpCeXk5SktL67saRPSc0NbWhqam5hOvp9pJGQAoFAomZET0zBJCIDk5GdnZ2fVdFSJ6zpiZmcHOzg4KheKx11GjpIyI6FlWkZDZ2NjAwMDgid48iYiAe1/2CgoKkJqaCgCwt7d/7HUxKSOiF0J5ebmUkD14oxIR0ZPQ19cHcO9JFDY2No99KbPGT/QnInoWVYwhMzAwqOeaENHzqOK95UnGqzIpI6IXCi9ZElFdqI33llpNyn755RfZA2WJiIieRbw7V328SG1Rq0nZ8OHD4eXlhQkTJtTmaomIiOpMWVkZvv76awQGBsLR0RF6enqYOXNmfVerXly+fBndu3dHSUkJLl26BD8/v6dehwsXLiAkJASNGzeGubk5TExMkJOT89TrUR9qdaC/UqlEbGws/vjjj9pcLRHRC2v48OFYv359lfOzsrJgZmb29Cr0nBFCoFevXkhMTMTcuXPRrFkzaGhowNHRsb6rVi+8vb2hra0NQ0NDaGpqYt26dU91+0ePHkXPnj0xbtw4bN68GSYmJtDX14epqelTrUd9qfW7L93c3DB27NjaXi0R0QurW7duWLt2razs5MmT6NevXz3V6PmxceNGxMXF4fTp0zAyMqrv6tQ7hUKBPXv2IDU1FUZGRk/1xhghBEaNGoWlS5fi3XfffWrbVSc1vnyZm5v70ImIiGqXrq4u7OzsZJOFhYVK3O+//45mzZpBV1cXrq6uWLJkiTSvU6dOUCgUlU5z5swBABQXF+PDDz+Eo6MjDA0N4e/vj6NHj0rrWLduHczMzLBr1y40atQIenp6CA4ORkJCghQzZ84c+Pr6Sn+XlJTAw8MDCoXioQ/tVSgU2LVrl6ysU6dOmDRpkvT3hg0b0KZNGxgbG8POzg5DhgyRng1VlaysLAwbNgzm5uYwMDBA9+7dER0dLc3ft28fvLy80KNHDxgbG8PW1hYffPABSkpKANwbK21paYni4mLZevv27Yu333670nrGxcVBoVDgwoULAO49jmXkyJFwc3ODvr4+mjRpgm+//Va2vuHDh6Nv377S3+vWrUOzZs2gr68PDw8PrFmz5qHHa9KkSejUqVOVdQCA7OxsvPvuu7C2toaJiQleeeUVXLx4UZp/f9vZ2NhAS0ur2m1XMZmYmODVV1+VjS93dXXF0qVLK122b9++GD58OADg2rVriI+PR0xMDFxcXKCnp4d27drhxIkTsmWOHTsGPz8/6Orqwt7eHtOmTUNZWZk0v1OnThg/fjzGjx8PU1NTWFlZYebMmRBCVFmnTz/9FA0aNEBcXJxUduLECXTo0AH6+vpwcnLC+++/j/z8/CqPQ22ocVJmbm5e6WRmZgZzc/O6qCMRUZ3Kz89Hfn6+7E27pKQE+fn5Kh/GFbFKpVIqKy0tRX5+PoqKih4ZW1fOnj2LgQMH4s0338Tly5cxZ84czJw5U7r8tGPHDiQlJSEpKQkBAQGYMmWK9PeHH34IABg/fjzCwsKwefNmXLp0CQMGDEC3bt1kSUxBQQHmz5+PX375BaGhocjOzsabb75ZZb2WL1+OlJSUWtnH0tJSzJs3DxcvXsSuXbsQFxcnfaBXZfjw4Thz5gz27NmDsLAwCCHw2muvSYPH09LSsGPHDjRr1gynTp3Czz//jM2bN2P69OkAgAEDBqC8vBx79uyR1pmamor9+/fjnXfeqVa9lUolGjRogG3btiEiIgKzZs3CjBkzqvwN6c2bN2PkyJEYOXIkLl68iClTpmDcuHHYu3dvtbZXlQEDBiA1NRV//PEHzp49i1atWqFLly7IzMysNL4mbbd27VokJSXh33//RWpqKmbMmFHj+qWlpaG0tBQbNmzADz/8gPPnz8PX1xfdunVDUlISACAxMRGvvfYa2rZti4sXL+KHH37ATz/9hM8//1y2rvXr10NLSwunTp3Ct99+i6+//ho//vhjpdtdsmQJVq1ahb/++guurq4AgBs3bqBbt27o168fLl26hC1btuDEiRMYP358jferRkQNubu7C2NjYzF//nxx9OhRlUnd5eTkCAAiJyenvqtCRE9RYWGhiIiIEIWFhSrzAAgAIjU1VSr7/PPPBQDx7rvvymINDAwEABEbGyuVffPNNwKAGDJkiCzWyspKABBXrlx57HqHhISIPn36qJQfOXJEABBZWVlCCCGGDBkiXn31VVnM1KlThZeXl8qyHTt2FLNnz5aVxcfHC01NTZGYmCgr79Kli5g+fboQQoi1a9cKAOK///6T5kdGRgoAIjw8XAghxOzZs4WPj48QQoiMjAxhbm4u5s2bJ6trZQCInTt3qtRz4sSJVS5z+vRpAUDcvXu30vnXr18XAERoaKhUlp6eLvT19cXWrVulbTRp0kQolUopZsOGDUJHR0fk5+cLIYR47733RPfu3aX5S5YsEQ0bNpSW6datmxg9erQ0PzY2VgAQ58+fr7Lu48aNE/369ZP+vr+d/fz8xIABA2Txo0aNEoGBgdLfDx6viRMnio4dO1ZZh+PHjwsTExNRVFQkW6+7u7tYtWqVEKJ22i47O1sEBgaKUaNGSfNdXFzEN998U+myffr0ESEhIUKI/z+nN23aJM0vLy8XjRo1Ep988okQQogZM2aotNeKFSuEkZGRKC8vF0Lca1NPT09ZzMcffyw8PT1V6rRmzRphYmIizpw5I6vXyJEjZW0qxL1jqKGhUel7iBAPf4+pbu5R456yyMhIzJkzB0uWLMHy5cvh7OyMjh07ShMRET19kZGRCAwMlJUFBgYiOjoa5eXlj1z+8uXLKC8vR+PGjWFkZCRNx44dk12K0tLSQtu2baW/mzZtCjMzM0RGRqqs87PPPkPnzp3x0ksvVWsfBg8eLNv28ePHZfPPnj2LXr16wdnZGcbGxtJnzq1btypdX2RkJLS0tODv7y+VWVpaokmTJrL6BgQEyJ4x9dJLL6GkpAQxMTEAgFGjRuHQoUNITEwEcO/S4vDhw6VlvL298ddffyEtLa3KfVuxYgVat24Na2trGBkZYfXq1Sr13rdvH4yMjHDq1KlK2zIiIqLK9T/KxYsXkZeXB0tLS9kxjo2NrfRRVo/bdubm5rh79y4WLFggm//xxx/DyMgINjY26NSpE0JDQ6tc1/37rqGhgfbt20v7HhkZqdJegYGByMvLw+3bt6Wydu3ayWICAgJUXgu7d+/GmDFj4ODgAG9vb1kdLl68iHXr1smOVXBwsHRDY12pcVKmra2NyZMnIzo6Go6OjmjRogWmTJnCH/glomdWXl4e8vLyYGVlJZVNnToVeXl5WL58uSw2NTUVeXl5cHZ2lsrGjRuHvLw8/PTTT7LYuLg45OXlwdPTs253oBbk5eVBU1MTZ8+exYULF6QpMjJSZfxTdURHR+PHH3/EwoULq73MN998I9t2mzZtpHn5+fkIDg6GiYkJNm3ahNOnT2Pnzp0AII3/ehwPG3ZT8aHesmVL+Pj44JdffsHZs2dx9epV2WXTDz/8EKamprCzs4ORkRGaNWsmW8/mzZvx4YcfYuTIkTh06BAuXLiAESNGqNS7c+fOuHDhQpV3fj7Jw0nz8vJgb28vO74XLlxAVFQUpk6dKot9krY7deoU7OzsVC4rT506FRcuXMBff/2FBg0aoFevXir7X522qE2hoaHYsmWLbFxlhby8PIwZM0Z2rC5evIjo6Gi4u7vXel0qPPbdlxYWFli6dCnGjx+Pjz/+GB4eHvj0009lgx2JiJ4FhoaGKmU6OjrQ0dGpVqy2tja0tbWrFVtXPD09VXofQkND0bhx42r9Dl/Lli1RXl6O1NRUdOjQocq4srIynDlzRnp+VVRUFLKzs1USz48//hjvvvsuPDw8ZD0YD2NnZwcPDw/p74rfEwTuDQLPyMjAl19+CScnJwDAmTNnHro+T09PlJWVITw8HO3btwcAZGRkICoqCl5eXgDu9fTt3LkTQgjpg//EiRPQ0dGRffi+++67WLp0KRITExEUFCTVAQBsbW1x/vx5JCYmorCwEImJibJB96GhoWjfvr3syQSV9U4ZGhrCw8MDXl5eCA0NxcSJE2XrqKjz42jVqhWSk5OhpaUljZuqypO23YQJE9C7d2+UlpZKrwsrKytp/vTp07Fp0yaVnkJ3d3doaWkhNDQULi4uAO6Nxzt58iQGDRoE4F6b/v7777L2Cg0NhbGxMRo0aCCtKzw8XLbu//77D40aNZK9FqZNm4b+/fvD2dkZL7/8Mt544w2pF7hVq1aIiIiQnY9PQ417ylq2bIlWrVpJ08CBA3Hz5k0UFxdjypQpdVFHIiJ6hClTpuDw4cOYN28erl+/jvXr12P58uXSIP5Hady4MYYOHYphw4Zhx44diI2NxalTp7BgwQLs379fitPW1saECRMQHh6Os2fPYvjw4WjXrp3sIaMxMTE4evQoZs2aVWv75+zsDB0dHSxbtgw3b97Enj17MG/evIcu06hRI/Tp0wejRo3CiRMncPHiRbz11ltwdHREnz59AADvvfce4uLiMG7cOERGRuLAgQOYOnUqxo8fL3scxJAhQ3D79m2sWbOmygH+jo6O8PDwkBKK++tx5swZ/Pnnn7h+/TpmzpyJ06dPV1nvyZMn4/fff8fXX3+N6Oho/PDDD1i3bh0++ugjWVxpaSmKiopQVFSE8vJyKJVK6e+KG1RKSkoghEBQUBACAgLQt29fHDp0CHFxcTh58iQ++eQTWXL7uG2XnZ2N5ORkREVF4aeffkLDhg1lX1TKyspQVFSEjIwM/PzzzzA1NZUltgBgZGSEUaNGYerUqThw4AAiIyMxduxY3LlzR0pox44di4SEBEyYMAHXrl3D7t27MXv2bEyePBkaGv+f0ty6dQuTJ09GVFQUfvvtNyxbtkyW5AKQ7mD28/PDpEmTZL2XH3/8MU6ePInx48fjwoULiI6Oxu7du+t8oH+Ne8ruv2WXiIjUQ6tWrbB161bMmjUL8+bNg729PT777LNH3p14v7Vr1+Lzzz/HlClTkJiYCCsrK7Rr1w49e/aUYgwMDPDxxx9jyJAhSExMRIcOHVQu2+bn52Pu3LmVPrbjcVlbW2PdunWYMWMGvvvuO7Rq1QqLFy9G7969H7lPEydORM+ePVFSUoKXX34ZBw4ckBIGZ2dn7Nu3D9OmTYOPjw/Mzc0xdOhQlTFRpqam6NevH/bv31/jz8ExY8bg/PnzGDRoEBQKBQYPHoyxY8dW+aD1bt26YdWqVfjyyy8xbdo0uLi4YMWKFejVq5csbuDAgSrL3t+7CAD+/v6IjY2Fq6srDhw4gE8++QQjRoxAWloa7Ozs8PLLL8PW1laKf9y2GzFiBADA2NgYrVq1wvbt22Xzp06diqlTp0JfXx/e3t7YuXMndHV1VdazePFiKBQKhISEIDc3F61atcKff/4Je3t7APcS34rE2cfHBxYWFhg5ciQ+/fRT2XqGDRuGwsJC+Pn5QVNTExMnTsTo0aOrrP/cuXOxZ88ezJkzB1988QVatGiBY8eO4ZNPPkGHDh0ghIC7u7vUY1dXFELcdw/4CyA3NxempqbIycmBiYlJfVeHiJ6SoqIixMbGws3NDXp6evVdnWfSunXrMGnSpBd2DHGXLl3QrFkzfPfdd/VdlWrz9fXFrl27HnnJ8nnSqVMn+Pr6VvlstLrysPeY6uYejz2m7MyZM9LdK15eXmjduvXjroqIiEhtZWVl4ejRozh69Ci+//77+q5Ojejq6tbJIHmqGzVOym7fvo3BgwcjNDRU+r217OxstG/fHps3b5YNtCMiInrWtWzZEllZWVi4cCGaNGlS39WpkQcHvJN6q/Hly27duiE7Oxvr16+XTs6oqCiMGDECJiYmOHjwYJ1UtLbw8iXRi4mXL4moLtXL5ctjx47h5MmTsm8LTZo0wbJlyx56GzURERERVa3Gj8RwcnKSfjPsfuXl5XBwcKiVShERERG9aGqclC1atAgTJkyQPdfkzJkzmDhxIhYvXlyrlSMiIiJ6UdR4TJm5uTkKCgpQVlYGLa17Vz8r/v/g06ur+uX5+sQxZUQvJo4pI6K6VC9jyp72cz+IiIiIXgQ1TspCQkLqoh5ERET0HLn/ty+pemo8pgy490Oqn376KQYPHozU1FQAwB9//IGrV6/WauWIiIjo0fbu3YtRo0ZBqVRi//796N+//1Ovw9GjR9G/f3+4u7vD1NQULi4ueMF+NOiJ1TgpO3bsGJo3b47w8HDs2LEDeXl5AICLFy9i9uzZtV5BIqIX2fDhw6FQKKqcXtSfPCK5oKAgXLhwAbq6uhgyZAgmTJjwVLe/adMm9OrVC23atMHOnTtx9uxZnD9/nr8mUEM1vnw5bdo0fP7555g8eTKMjY2l8ldeeQXLly+v1coREdG9h3avXbtWVnby5En069evnmpE6kZfXx+nTp1CcnIyLCwsKv2x77qSl5eH8ePHY8eOHXj11Vef2nafRzXuKbt8+TJef/11lXIbGxukp6fXSqWIiOj/6erqws7OTjZZWFioxP3+++9o1qwZdHV14erqiiVLlkjzOnXqVGVv25w5cwAAxcXF+PDDD+Ho6AhDQ0P4+/vj6NGj0jrWrVsHMzMz7Nq1C40aNYKenh6Cg4ORkJAgxcyZMwe+vr7S3yUlJfDw8Hhkr55CocCuXbtkZZ06dcKkSZOkvzds2IA2bdrA2NgYdnZ2GDJkiDSEpjK1sc8AEBoaik6dOsHAwADm5uYIDg5GVlbWQ3sxhw8fLq3//fffh42NDfT09PDSSy/h9OnT0rqPHj0qLaOhoQEbGxuMHDkSRUVFUszly5fxyiuvQF9fH5aWlhg9erR0lQq415vat29fKBQK2NvbIy8vD+bm5tJPIVYmLi5OVl8LCwu88cYbyMjIeGibVPD19ZWO4fHjx6Gvr4+dO3fCwcEBBgYGCAoKUhnS9LDzEwBcXV0xb948DB48GIaGhnB0dMSKFStkMffXSQiBYcOGoUWLFsjKypJidu/ejVatWkFPTw8NGzbE3LlzUVZWVuWxUCc1TsrMzMyQlJSkUn7+/Hk4OjrWSqWIiJ4GIQTy8/Of+lQX42zOnj2LgQMH4s0338Tly5cxZ84czJw5E+vWrQMA7NixA0lJSUhKSkJAQACmTJki/f3hhx8CAMaPH4+wsDBs3rwZly5dwoABA9CtWzdER0dL2ykoKMD8+fPxyy+/IDQ0FNnZ2XjzzTerrNfy5cuRkpJSK/tYWlqKefPm4eLFi9i1axfi4uKk5KcytbHPFy5cQJcuXeDl5YWwsDCcOHECvXr1Qnl5Ob799ltpfQMHDsTAgQOlv7/99lsAwEcffYTff/8d69evx7lz5+Dh4YHg4GCVR0ZFRUUhMTERGzduxJYtW6Se0fz8fAQHB8Pc3BynT5/Gtm3b8Pfff2P8+PFV7ndNkpC///4bSUlJ2L9/P06dOoWvvvqqWsvdLy0tDUlJSTh69Cg2b96M8PBwGBsbo1u3bigsLATw6POzwqJFi+Dj44Pz589j2rRpmDhxIv76669Kt/v+++/j5MmTOHToEMzNzQHcSxCHDRuGiRMnIiIiAqtWrcK6deswf/78Gu9XvRA1NGXKFPHSSy+JpKQkYWxsLKKjo8WJEydEw4YNxZw5c2q6uqcuJydHABA5OTn1XRUieooKCwtFRESEKCwslMry8vIEgKc+5eXlVbveISEhok+fPirlR44cEQBEVlaWEEKIIUOGiFdffVUWM3XqVOHl5aWybMeOHcXs2bNlZfHx8UJTU1MkJibKyrt06SKmT58uhBBi7dq1AoD477//pPmRkZECgAgPDxdCCDF79mzh4+MjhBAiIyNDmJubi3nz5snqWhkAYufOnSr1nDhxYpXLnD59WgAQd+/erTLm/nU9zj4PHjxYBAYGPnL9ISEhIiQkRFaWl5cntLW1xaZNm6SykpIS4eDgIL766ishhGo7RkdHC3Nzc2mZ1atXC3Nzc9k5s3//fqGhoSGSk5OlbVecI1FRUcLQ0FDMnDlTmJqaVlnf2NhYAUCcP39eCCFEUlKS8PDwEPPnz5diKmuTCj4+PtLxrDgvQkNDpfm5ubnCzMxMrFmzRghRvfPTxcVFdOvWTRYzaNAg0b17d5U6ffLJJ8LR0VHExsbK4rt06SK++OILWdmGDRuEvb19lceitlT2HlOhurlHjXvKvvjiCzRt2hROTk7Iy8uDl5cXXn75ZbRv3x6ffvppTVdHRES1IDIyEoGBgbKywMBAREdHo7y8/JHLX758GeXl5WjcuDGMjIyk6dixY7hx44YUp6WlhbZt20p/N23aFGZmZoiMjFRZ52effYbOnTvjpZdeqtY+DB48WLbt48ePy+afPXsWvXr1grOzM4yNjdGxY0cAwK1bt6q1/gdVZ58resoex40bN1BaWiprF21tbfj5+akcrwYNGsDQ0BCNGjXCa6+9hsGDBwO4164+Pj6yh7MHBgZCqVQiKipKZZsfffQRxowZg4YNG1arju3bt4eRkRHs7e3h5OSEKVOmyOZXtIm9vT169OiBiIiIStejpaUFf39/6W9jY2P4+PhI8dU9PwMCAmQxAQEBKsdq+fLlmD9/Ppo0aQJXV1fZvIsXL+Kzzz6TteeoUaOQlJSEgoKCah2T+lTjgf46OjpYs2YNZs2ahcuXLyMvLw8tW7ZEo0aN6qJ+RER1xsDAQDY252luV93k5eVBU1MTZ8+ehaampmyekZFRjdcXHR2NH3/8ERcuXMDt27ertcw333yDoKAg6e+hQ4dK/6+4jBccHIxNmzbB2toat27dQnBwMEpKSmpcP6B6+6yvr/9Y666p48ePw9jYGLGxsRg9ejS+/vprlQTpUY4dO4bjx49j7dq12L17d7WW2bJlCzw9PZGcnIyJEyfiww8/xLJly6T5FW2SnZ2NGTNmYODAgbhy5YpsHRWXDitTF3dfnjp1CgcOHMDw4cOxatUqjBkzRpqXl5eHuXPn4o033lBZ7ln4JY8aJ2WfffYZPvzwQzg5OcHJyaku6kRE9FQoFAqVn4d7Vnl6eiI0NFRWFhoaisaNG6skHJVp2bIlysvLkZqaig4dOlQZV1ZWhjNnzsDPzw/AvbFQ2dnZ8PT0lMV9/PHHePfdd+Hh4VHtpMzOzg4eHh7S3/cnRNeuXUNGRga+/PJL6bPn/t9gfhzV2ecWLVrg8OHDmDt3bo3X7+7uDh0dHYSGhsLFxQXAvXFxp0+flt3AAABubm4wMzODh4cH+vXrh507d2LKlCnw9PTEunXrkJ+fL52roaGh0NDQQJMmTaTlhRCYMmUKZs6c+dAk6UFOTk7w8PCAh4cHRowYgS+//FKWlN3fJhMnTkSvXr1QWloqW0fTpk1RVlaG8PBwtG/fHgBw9+5dXLx4EW+//TaA6p+f//33nyzmv//+Uzm3li5diu7du+P777/HiBEj0L17dzg7OwMAWrVqhaioKNl59Cyp8eXLuXPn1ss3SyIiqtqUKVNw+PBhzJs3D9evX8f69euxfPlyaUD7ozRu3BhDhw7FsGHDsGPHDsTGxuLUqVNYsGAB9u/fL8Vpa2tjwoQJCA8Px9mzZzF8+HC0a9dOStIAICYmBkePHsWsWbNqbf+cnZ2ho6ODZcuW4ebNm9izZw/mzZv3ROuszj5Pnz4dp0+fxtixY3Hp0iVcu3YNP/zwQ7WeNmBoaIj33nsPU6dOxcGDBxEREYFRo0ahoKAAI0eOlMWmpqYiOTkZ4eHh2Lt3L5o2bQrgXm+hnp4eQkJCcOXKFRw5cgQTJkzA22+/DVtbW2n5w4cPIycnB+PGjavRMcjIyEBycjIuXbqE3377TdpuhdLSUhQVFSE5ORkbN25E48aNVZ7S36RJE3Tv3h3vvvsujh8/jsuXL2PYsGEwMjLCkCFDAFT//AwNDcVXX32F69evY8WKFdi2bRsmTpwoi6m487hfv3547bXX8O6770rzZs2ahV9++QVz587F1atXERkZic2bNz87w6tqOpBNoVCIlJSUmi6mNjjQn+jF9LBBuOqsugP9hRBi+/btwsvLS2hrawtnZ2exaNGiStdZ2aB3Ie4NQp81a5ZwdXUV2trawt7eXrz++uvi0qVLQoh7A7pNTU3F77//Lho2bCh0dXVFUFCQiI+Pl9Yxe/ZsAUAsXrz4oXV9EKox0P/XX38Vrq6uQldXVwQEBIg9e/bIBqs/zOPusxBCHD16VLRv317o6uoKMzMzERwcrLIvlQ30F+LeeTdhwgRhZWUldHV1RWBgoDh16pQ0v+LYVExWVlZiyJAhIiMjQ4q5dOmS6Ny5s9DT0xMWFhZi1KhRspsbQkJCBACxfft2qayirapSMdC/YjIzMxM9e/aUDZy/f76xsbHo2LGjuHDhghBCPtBfCCHS0tLEkCFDhKmpqdDX1xdBQUHi6tWrsm0+6vx0cXERc+fOFQMGDBAGBgbCzs5OfPvtt7KYB8+TtLQ0YWNjI1atWiWVHTx4ULRv317o6+sLExMT4efnJ1avXl3lsagttTHQXyFEze7N1tDQwIcffljlGIPa/GZUF6r7S+1E9HwpKipCbGws3NzcnomxJepo3bp1mDRpEn9FgOqEq6srJk2apHJp91nxsPeY6uYeNR5TBtzrXtTR0VEpVygUap+UEREREamjx0rKdu7cCRsbm9quCxEREdEL67GSMiIievEMHz78oU/QJ3oScXFx9V2Felfjuy87duxY6aVLIiIiInp8Ne4pO3LkiPT/insE6uLhcEREREQvkhr3lAHAL7/8gubNm0NfXx/6+vpo0aIFNmzYUNt1IyKqdUqlsr6rQETPodp4b6lxT9nXX3+NmTNnYvz48dLvWJ04cQL/+9//kJ6ejg8++OCJK0VEVNt0dHSgoaGBO3fuwNraGjo6OuzlJ6InJoRASUkJ0tLSoKGh8URDvGr8nDI3NzfMnTsXw4YNk5WvX78ec+bMQWxs7GNX5mngc8qIXlwlJSXPzA8TE9GzxcDAAPb29pUmZXX2nLKkpCTpt63u1759eyQlJdV0dURET42Ojg6cnZ1RVlaG8vLy+q4OET0nNDU1oaWl9cS97zVOyjw8PLB161bMmDFDVr5lyxY0atToiSpDRFTXFAoFtLW1VX6/j4iovtU4KZs7dy4GDRqEf//9VxpTFhoaisOHD2Pr1q21XkEiIiKiF0GN777s168fwsPDYWVlhV27dmHXrl2wsrLCqVOn8Prrr9dFHYmIiIieezUe6P+s40B/IiIieppqfaB/bm5uteKY6BARERHVXLWTMjMzs4feVSCEgEKh4B1NRERERI+hRgP9t2/fDgsLi7qqCxEREdELq0ZJWWBgIGxsbOqqLkREREQvrMf67UsiIiIiql1MyoiIiIjUQLWTMoVCwR/vJSIiIqoj1R5TJoTA8OHDoaur+9C4HTt21LgSK1aswKJFi5CcnAwfHx8sW7YMfn5+lcauWbMGv/zyC65cuQIAaN26Nb744osq44mIiIieBdXuKQsJCYGNjQ1MTU0fOtXUli1bMHnyZMyePRvnzp2Dj48PgoODkZqaWmn80aNHMXjwYBw5cgRhYWFwcnJC165dkZiYWONtExEREamLen+iv7+/P9q2bYvly5cDAJRKJZycnDBhwgRMmzbtkcuXl5fD3Nwcy5cvx7Bhwx4Zzyf6ExER0dNU3dyjXgf6l5SU4OzZswgKCpLKNDQ0EBQUhLCwsGqto6CgAKWlpVU+P624uBi5ubmyiYiIiEjd1GtSlp6ejvLyctja2srKbW1tkZycXK11fPzxx3BwcJAldvdbsGCB7PKqk5PTE9ebiIiIqLY904/E+PLLL7F582bs3LkTenp6lcZMnz4dOTk50pSQkPCUa0lERET0aDV6on9ts7KygqamJlJSUmTlKSkpsLOze+iyixcvxpdffom///4bLVq0qDJOV1f3kXeMEhEREdW3eu0p09HRQevWrXH48GGpTKlU4vDhwwgICKhyua+++grz5s3DwYMH0aZNm6dRVSIiIqI6Va89ZQAwefJkhISEoE2bNvDz88PSpUuRn5+PESNGAACGDRsGR0dHLFiwAACwcOFCzJo1C7/++itcXV2lsWdGRkYwMjKqt/0gIiIiehL1npQNGjQIaWlpmDVrFpKTk+Hr64uDBw9Kg/9v3boFDY3/79D74YcfUFJSgv79+8vWM3v2bMyZM+dpVp2IiIio1tT7c8qeNj6njIiIiJ6mZ+I5ZURERER0D5MyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDdR7UrZixQq4urpCT08P/v7+OHXqVJWxV69eRb9+/eDq6gqFQoGlS5c+vYoSERER1aF6Tcq2bNmCyZMnY/bs2Th37hx8fHwQHByM1NTUSuMLCgrQsGFDfPnll7Czs3vKtSUiIiKqOwohhKivjfv7+6Nt27ZYvnw5AECpVMLJyQkTJkzAtGnTHrqsq6srJk2ahEmTJj00rri4GMXFxdLfubm5cHJyQk5ODkxMTJ54H4iIiIgeJjc3F6ampo/MPeqtp6ykpARnz55FUFDQ/1dGQwNBQUEICwurte0sWLAApqam0uTk5FRr6yYiIiKqLfWWlKWnp6O8vBy2traycltbWyQnJ9fadqZPn46cnBxpSkhIqLV1ExEREdUWrfquQF3T1dWFrq5ufVeDiIiI6KHqrafMysoKmpqaSElJkZWnpKRwED8RERG9cOotKdPR0UHr1q1x+PBhqUypVOLw4cMICAior2oRERER1Yt6vXw5efJkhISEoE2bNvDz88PSpUuRn5+PESNGAACGDRsGR0dHLFiwAMC9mwMiIiKk/ycmJuLChQswMjKCh4dHve0HERER0ZOq16Rs0KBBSEtLw6xZs5CcnAxfX18cPHhQGvx/69YtaGj8f2fenTt30LJlS+nvxYsXY/HixejYsSOOHj36tKtPREREVGvq9Tll9aG6zwohIiIiqg1q/5wyIiIiIvp/TMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyIiIiIjXApIyIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSMiIiISA1o1XcFnkfl5eX47LPPYGZmhvfeew96enoAgPj4eCQlJcHBwQHOzs4AAKVSiUuXLkGhUMDb2xuampoAgLS0NGRkZMDc3By2trbSum/cuAEAcHFxgZbWvebLyclBdnY2jIyMYGlpKcXeuXMHAGBjYyPFFhQU4O7du9DT04OpqakUm56eDiEEzM3Npdji4mIUFBRAW1sbRkZGUmxubi6USiWMjIyk2NLSUhQVFUFTUxMGBgZSbEFBAYQQ0NPTk/atvLwcJSUl0NDQgK6urhRbUlICIQS0tbWhoaEhHZ/y8nIoFAppWxXlAKBQKKBQKGrUPkRERGpJvGBycnIEAJGTk1Nn28jMzBQABABRVFQklU+ePFkAEB9//LFUVlBQIMXeX6dPP/1UABDjx4+XrVuhUAgAIikpSSr74osvBADxzjvvyGKNjIwEAHHjxg2p7NtvvxUAxKBBg2Sxtra2AoC4dOmSVPbjjz8KAKJnz56yWDc3NwFAhIWFSWW//fabACBeeeUVWay3t7cAIA4fPiyV7d69WwAQ/v7+slh/f38BQOzZs0cq+/vvvwUA0bx5c1ls586dBQDx22+/SWVhYWECgHBzc5PF9urVSygUCvHTTz9JZZcuXRKamprC0dFRFjtkyBChra0tli9fLpXduHFD6OjoCEtLS1nsmDFjhJ6enli0aJFUlpycLAwMDISRkZEsdsqUKcLIyEjMmzdPKsvNzRXGxsbC2NhYFBYWSuWzZs0SJiYmYsaMGVJZWVmZMDU1FaampiIjI0MqX7hwoTAzMxMffPCBbHt2dnbC3NxcJCQkSGXLly8XFhYW4n//+58s1sPDQ1haWorr169LZWvXrhVWVlZi+PDhslgfHx9hbW0tLly4IJVt2bJF2NjYiDfffFMWGxgYKGxtbWXnyb59+4SdnZ3o06ePLLZr167Czs5O/PPPP1LZP//8I+zt7UXXrl1lsX379hUODg5i3759UllYWJhwcHAQHTp0kMUOHTpUODo6iu3bt0tlly5dEg0aNBB+fn6y2FGjRokGDRqIX375RSqLjo4WTk5OKuffxIkThbOzs1i9erVUlpiYKFxcXETjxo1lsTNmzBCurq7iu+++k8oyMzOFq6urcHV1FWVlZVL5559/LlxdXcWXX34plRUVFUmxd+/elcoXL14sXF1dxezZs2Xbc3d3F25ubiI1NVUq+/7774Wbm5v46KOPZLEtWrQQ7u7uIi4uTipbt26daNiwoXj//fdlsf7+/sLd3V1cu3ZNKtu6davw8PAQo0aNksV27txZeHh4iPPnz0tle/fuFR4eHuLtt9+Wxfbo0UM0atRIhIaGSmWHDx8WjRo1Ev3795fF9u/fXzRq1Ej2fnLy5EnRuHFjlfepYcOGiUaNGsnOk/Pnz4vGjRuLLl26yGLHjBkjmjRpIrZt2yaVXbt2TTRp0kQEBgbKYj/44APRtGlTsWHDBqksPj5eNG3aVLRq1UoWO2PGDNG0aVPZeZKamiqaNm0qvLy8ZLGff/658PT0FN9++61UlpeXJzw9PYWnp6fss2TJkiXC09NTLFy4UCorKysTXl5ewsvLS2RlZUnl33//vfDy8hJz5syRbc/X11d4eXmJO3fuSGVr164VXl5eYtq0abLY9u3bi2bNmsk+SzZv3iyaNWsmJk6cKIvt0qWLaNasmbhy5YpUtmfPHuHt7S3GjBkji+3Zs6do1qyZOHPmjFR26NAh4e3tLUJCQmSxAwYMEN7e3uL48eNS2fHjx0Xz5s3FwIEDZbEhISGiefPm4tChQ1LZmTNnRPPmzUWvXr1ksf/73/9E8+bNZcehLlQ392BPWR1QKBR47733kJ+fL+sJsrCwgJubGywsLGTx9vb2EEJIvUMAoK+vD3NzcxgaGspijYyMVGK1tLSgp6cHbW1tWayWlpbUO1UX1LWH6sF6CSEghFApKy8vR1lZmay8vLwcpaWlUk9cRWxJSQlKSkpksSUlJSgqKpKtQwiBgoICWfsA93od8/LyZOsQQuDu3bsq9S8uLkZubi6Kiopk5Tk5OSqxRUVFyM7ORkFBgaw8KysLxcXFsv0uKipCZmYm8vPzZbEZGRnIyspCeXm5LDY9PR25ubkqsWlpabJ9LioqQmpqKrKysmSx6enpSElJQWlpqSw2OTkZGRkZKrHJyckoLi6WHYekpCTY29urxN65c0d2fEpKSnDnzh0YGxurxCYmJsqOT0lJCW7fvq1ynmRkZOD27duy41NWVoaEhIRKj9mtW7dk7VdWVob4+HjZa74iNi4uDtnZ2VKZUqlEXFwcAPn5mpmZibi4ONmxFEJIsfe3Z3Z2NuLi4pCZmSnbXmxsrNTDXCEnJwexsbFIT0+Xxd68eRN5eXmy9szNzcXNmzeRkpIii42Li0NKSorsHM7NzUVMTAyaNm0qi42Pj8fNmzdlbZSXl4eYmBjpKsH9sdHR0SgsLJTK8vPzER0drfJemZCQgOjoaFl7FBQU4Pr169DR0ZHF3r59G9HR0bI2KioqwvXr11Ve94mJiYiKipK9xoqLixEVFSVrN+DeFYhr167J2qi0tBTXrl2TXVEAgKSkJFy7dk12vpeXl+PatWsq7xHJycmIjIxEWlqaVKZUKhEZGQlA3vapqamIjIxUaaOIiAhpGxXS09MRERGBpKQkWWxkZCSKi4tlxyIjIwMRERFo3bq1LPbatWvIzMyUtX1WVhauXr2KJk2ayGKjoqJw+/ZtWdtnZ2fjypUrcHR0lMVGR0cjKipK9vrMzc3FlStXYG5uLouNiYnBlStXkJeXJ5Xl5eXh8uXLKp99N27cwOXLl2XvXwUFBbh8+bLK+3hsbCwuX74se5+qTwrx4KfVcy43NxempqbIycmBiYlJfVdHbdx/GlR8SCiVSiiVSigUCllyV3HyampqSm8sFcmMQqGQfSgVFhZCCAFdXV1pHWVlZSgsLISGhoYs6czLy0N5eTkMDAykF1lpaSny8vKgoaEhu9yak5OD0tJSGBsbS9srLS1FVlYWNDU1ZZdxKxIUExMT6dJqaWkp0tPToVAoYGdnJ8VmZmaisLAQpqam0htsWVkZkpKSoFAo0KBBAyk2PT0d+fn5MDMzk+pWXl6OhIQECCHg5uYmi83NzYWZmZn0QaNUKhEbGwsAcHNzk45leno6srKyYGZmBmtra6l9YmJiAAANGzaUjmV6ejoyMjJgamoq24+oqCgIIeDu7i4dy4yMDKSmpsLU1BQODg5S7LVr16BUKuHu7i4dy8zMTCQlJcHY2Fj2IRoVFYWysjI0bNgQ+vr60vFNTEyEoaGhbJ+joqJQXFwMd3d3qZ1zcnIQHx8PAwMDeHh4SLEVH8qurq7S6zI3NxexsbHQ09OTvfFXfCi7urrCzMwMwP9/4Ovq6sLT01OKjYmJwd27d+Hi4iId94KCAly7dg3a2tpo3ry5FHvjxg1kZ2fD2dlZOu5FRUW4evUqNDQ00LJlSyn25s2byMzMRIMGDaTjXlxcLA1FaNOmjRQbGxuLtLQ0ODo6Sh9KpaWlOH/+PACgbdu20msuPj4eycnJsLe3lw1xOHPmDACgdevWUtsnJCTgzp07sLW1haurq7S9//77DwDQqlUrKVFJTExEQkICrK2t4e7uLsWGh4dDqVSiZcuW0jCLpKQkxMXFwdLSEo0bN5Ziz5w5g9LSUrRo0UJqz5SUFNy4cQPm5uay43727FkUFxejefPmUqKclpaG69evw9TUFN7e3lLs+fPnUVBQgGbNmkntmZmZicjISBgZGcHHx0eKvXjxIu7evQtPT0/pNV7xgW9gYIBWrVpJsZcvX0ZOTg6aNGkitWdubi4uXrwIPT09tG3bVoq9evUqMjMz0ahRI6k98/Pzce7cOWhra6Ndu3ZSbEXi5O7uLrVnYWEhTp8+DU1NTQQGBkqxUVFRSE5Ohpubm9SeJSUlUhu9/PLLUmx0dDTu3LkDZ2dn6XVUVlaGEydOQKFQoEOHDtJ7xM2bN5GQkIAGDRpI7SmEwLFjxwAAgYGB0us+Li4OcXFxcHBwkLXnsWPHIIRAu3btpLa/desWbty4AVtbW3h5eUmxx48fR1lZGfz8/KS2T0xMxPXr12FtbS1rz9DQUBQXF6NNmzbSazk5ORkRERGwsLCAr6+vFBseHo6CggK0bNlSavvU1FRcuXIFpqamsuTw1KlTyMvLg4+Pj9T2GRkZuHDhAoyNjeHn5yfFnjlzBjk5OWjevDlsbGyk8+Ts2bMwMDBAQECAFHv+/HlkZmYiMDBQOg51obq5B5MyIiIiojpU3dyDd18SERERqQEmZURERERqgEkZERERkRpgUkZERESkBpiUEREREakBtUjKVqxYAVdXV+jp6cHf3x+nTp16aPy2bdvQtGlT6OnpoXnz5jhw4MBTqikRERFR3aj3pGzLli2YPHkyZs+ejXPnzsHHxwfBwcFITU2tNP7kyZMYPHgwRo4cifPnz6Nv377o27cvrly58pRrTkRERFR76v05Zf7+/mjbti2WL18O4N6DEp2cnDBhwgRMmzZNJX7QoEHIz8/Hvn37pLJ27drB19cXK1eufOT2+JwyIiIiepqeieeUlZSU4OzZswgKCpLKNDQ0EBQUhLCwsEqXCQsLk8UDQHBwcJXxFT9Zc/9EREREpG7qNSlLT09HeXk5bG1tZeW2trZITk6udJnk5OQaxS9YsACmpqbS5OTkVDuVJyIiIqpF9T6mrK5Nnz4dOTk50pSQkFDfVSIiIiJSoVWfG7eysoKmpqbKL92npKTIflz5fnZ2djWK19XVlf1ANhEREZE6qteeMh0dHbRu3RqHDx+WypRKJQ4fPiz7Fff7BQQEyOIB4K+//qoynoiIiOhZUK89ZQAwefJkhISEoE2bNvDz88PSpUuRn5+PESNGAACGDRsGR0dHLFiwAAAwceJEdOzYEUuWLEGPHj2wefNmnDlzBqtXr67P3SAiIiJ6IvWelA0aNAhpaWmYNWsWkpOT4evri4MHD0qD+W/dugUNjf/v0Gvfvj1+/fVXfPrpp5gxYwYaNWqEXbt2wdvbu1rbq3gCCO/CJCIioqehIud41FPI6v05ZU/b7du3eQcmERERPXUJCQlo0KBBlfNfuKRMqVTizp07MDY2hkKhqJNt5ObmwsnJCQkJCXxArRpge6gPtoV6YXuoD7aFeqnt9hBC4O7du3BwcJBd/XtQvV++fNo0NDQemqXWJhMTE7641AjbQ32wLdQL20N9sC3US222h6mp6SNjnvvnlBERERE9C5iUEREREakBJmV1QFdXF7Nnz+ZDa9UE20N9sC3UC9tDfbAt1Et9tccLN9CfiIiISB2xp4yIiIhIDTApIyIiIlIDTMqIiIiI1ACTMiIiIiI1wKSslq1YsQKurq7Q09ODv78/Tp06Vd9VeiEsWLAAbdu2hbGxMWxsbNC3b19ERUXJYoqKijBu3DhYWlrCyMgI/fr1Q0pKSj3V+MXx5ZdfQqFQYNKkSVIZ2+LpSkxMxFtvvQVLS0vo6+ujefPmOHPmjDRfCIFZs2bB3t4e+vr6CAoKQnR0dD3W+PlVXl6OmTNnws3NDfr6+nB3d8e8efNkv4nI9qgb//77L3r16gUHBwcoFArs2rVLNr86xz0zMxNDhw6FiYkJzMzMMHLkSOTl5dVaHZmU1aItW7Zg8uTJmD17Ns6dOwcfHx8EBwcjNTW1vqv23Dt27BjGjRuH//77D3/99RdKS0vRtWtX5OfnSzEffPAB9u7di23btuHYsWO4c+cO3njjjXqs9fPv9OnTWLVqFVq0aCErZ1s8PVlZWQgMDIS2tjb++OMPREREYMmSJTA3N5divvrqK3z33XdYuXIlwsPDYWhoiODgYBQVFdVjzZ9PCxcuxA8//IDly5cjMjISCxcuxFdffYVly5ZJMWyPupGfnw8fHx+sWLGi0vnVOe5Dhw7F1atX8ddff2Hfvn34999/MXr06NqrpKBa4+fnJ8aNGyf9XV5eLhwcHMSCBQvqsVYvptTUVAFAHDt2TAghRHZ2ttDW1hbbtm2TYiIjIwUAERYWVl/VfK7dvXtXNGrUSPz111+iY8eOYuLEiUIItsXT9vHHH4uXXnqpyvlKpVLY2dmJRYsWSWXZ2dlCV1dX/Pbbb0+jii+UHj16iHfeeUdW9sYbb4ihQ4cKIdgeTwsAsXPnTunv6hz3iIgIAUCcPn1aivnjjz+EQqEQiYmJtVIv9pTVkpKSEpw9exZBQUFSmYaGBoKCghAWFlaPNXsx5eTkAAAsLCwAAGfPnkVpaamsfZo2bQpnZ2e2Tx0ZN24cevToITvmANviaduzZw/atGmDAQMGwMbGBi1btsSaNWuk+bGxsUhOTpa1h6mpKfz9/dkedaB9+/Y4fPgwrl+/DgC4ePEiTpw4ge7duwNge9SX6hz3sLAwmJmZoU2bNlJMUFAQNDQ0EB4eXiv1eOF+kLyupKeno7y8HLa2trJyW1tbXLt2rZ5q9WJSKpWYNGkSAgMD4e3tDQBITk6Gjo4OzMzMZLG2trZITk6uh1o+3zZv3oxz587h9OnTKvPYFk/XzZs38cMPP2Dy5MmYMWMGTp8+jffffx86OjoICQmRjnll711sj9o3bdo05ObmomnTptDU1ER5eTnmz5+PoUOHAgDbo55U57gnJyfDxsZGNl9LSwsWFha11jZMyui5M27cOFy5cgUnTpyo76q8kBISEjBx4kT89ddf0NPTq+/qvPCUSiXatGmDL774AgDQsmVLXLlyBStXrkRISEg91+7Fs3XrVmzatAm//vormjVrhgsXLmDSpElwcHBgexAH+tcWKysraGpqqtxBlpKSAjs7u3qq1Ytn/Pjx2LdvH44cOYIGDRpI5XZ2digpKUF2drYsnu1T+86ePYvU1FS0atUKWlpa0NLSwrFjx/Ddd99BS0sLtra2bIunyN7eHl5eXrIyT09P3Lp1CwCkY873rqdj6tSpmDZtGt588000b94cb7/9Nj744AMsWLAAANujvlTnuNvZ2ancuFdWVobMzMxaaxsmZbVER0cHrVu3xuHDh6UypVKJw4cPIyAgoB5r9mIQQmD8+PHYuXMn/vnnH7i5ucnmt27dGtra2rL2iYqKwq1bt9g+taxLly64fPkyLly4IE1t2rTB0KFDpf+zLZ6ewMBAlcfDXL9+HS4uLgAANzc32NnZydojNzcX4eHhbI86UFBQAA0N+UevpqYmlEolALZHfanOcQ8ICEB2djbOnj0rxfzzzz9QKpXw9/evnYrUyu0CJIQQYvPmzUJXV1esW7dOREREiNGjRwszMzORnJxc31V77r333nvC1NRUHD16VCQlJUlTQUGBFPO///1PODs7i3/++UecOXNGBAQEiICAgHqs9Yvj/rsvhWBbPE2nTp0SWlpaYv78+SI6Olps2rRJGBgYiI0bN0oxX375pTAzMxO7d+8Wly5dEn369BFubm6isLCwHmv+fAoJCRGOjo5i3759IjY2VuzYsUNYWVmJjz76SIphe9SNu3fvivPnz4vz588LAOLrr78W58+fF/Hx8UKI6h33bt26iZYtW4rw8HBx4sQJ0ahRIzF48OBaqyOTslq2bNky4ezsLHR0dISfn5/477//6rtKLwQAlU5r166VYgoLC8XYsWOFubm5MDAwEK+//rpISkqqv0q/QB5MytgWT9fevXuFt7e30NXVFU2bNhWrV6+WzVcqlWLmzJnC1tZW6Orqii5duoioqKh6qu3zLTc3V0ycOFE4OzsLPT090bBhQ/HJJ5+I4uJiKYbtUTeOHDlS6edESEiIEKJ6xz0jI0MMHjxYGBkZCRMTEzFixAhx9+7dWqujQoj7HiNMRERERPWCY8qIiIiI1ACTMiIiIiI1wKSMiIiISA0wKSMiIiJSA0zKiIiIiNQAkzIiIiIiNcCkjIiIiEgNMCkjIiIiUgNMyoiIiIjUAJMyInoulZaWYt26dXjppZdgbW0NfX19tGjRAgsXLkRJSUl9V4+ISAV/ZomInksXLlzAlClTMHbsWLRs2RJFRUW4fPky5syZA3t7e/z555/Q1tau72oSEUnYU0ZEzyVvb28cPnwY/fr1Q8OGDeHl5YVBgwbh33//xZUrV7B06VIAgEKhqHSaNGmStK6srCwMGzYM5ubmMDAwQPfu3REdHS3Nf+edd9CiRQsUFxcDAEpKStCyZUsMGzZMivn444/RuHFjGBgYoGHDhpg5cyZKS0ufyrEgomcDkzIiei5paWlVWm5tbY033ngDmzZtksrWrl2LpKQkaQoICJAtM3z4cJw5cwZ79uxBWFgYhBB47bXXpKTqu+++Q35+PqZNmwYA+OSTT5CdnY3ly5dL6zA2Nsa6desQERGBb7/9FmvWrME333xT27tNRM+wyt+1iIieE82aNUN8fLysrLS0FJqamtLfZmZmsLOzk/7W0dGR/h8dHY09e/YgNDQU7du3BwBs2rQJTk5O2LVrFwYMGAAjIyNs3LgRHTt2hLGxMZYuXYojR47AxMREWs+nn34q/d/V1RUffvghNm/ejI8++qjW95mInk1MyojouXbgwAGVy4RfffUVNm7cWK3lIyMjoaWlBX9/f6nM0tISTZo0QWRkpFQWEBCADz/8EPPmzcPHH3+Ml156SbaeLVu24LvvvsONGzeQl5eHsrIyWdJGRMSkjIieay4uLiplN27cQOPGjWt1O0qlEqGhodDU1ERMTIxsXlhYGIYOHYq5c+ciODgYpqam2Lx5M5YsWVKrdSCiZxvHlBHRcykzMxN3795VKT9z5gyOHDmCIUOGVGs9np6eKCsrQ3h4uFSWkZGBqKgoeHl5SWWLFi3CtWvXcOzYMRw8eBBr166V5p08eRIuLi745JNP0KZNGzRq1EjlkioREZMyInou3bp1C76+vvjpp58QExODmzdvYsOGDejTpw86dOggu7vyYRo1aoQ+ffpg1KhROHHiBC5evIi33noLjo6O6NOnDwDg/PnzmDVrFn788UcEBgbi66+/xsSJE3Hz5k1pHbdu3cLmzZtx48YNfPfdd9i5c2dd7ToRPaOYlBHRc8nb2xuzZ8/GunXr0K5dOzRr1gxfffUVxo8fj0OHDskG8z/K2rVr0bp1a/Ts2RMBAQEQQuDAgQPQ1tZGUVER3nrrLQwfPhy9evUCAIwePRqdO3fG22+/jfLycvTu3RsffPABxo8fD19fX5w8eRIzZ86sq10nomcUHx5LREREpAbYU0ZERESkBpiUEREREakBJmVEREREaoBJGREREZEaYFJGREREpAaYlBERERGpASZlRERERGqASRkRERGRGmBSRkRERKQGmJQRERERqQEmZURERERq4P8AdwhEm/nLlZYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}